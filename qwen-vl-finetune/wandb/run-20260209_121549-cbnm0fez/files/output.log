  0%|                                             | 0/1473 [00:00<?, ?it/s]/opt/conda/lib/python3.11/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
                                                                           
{'loss': 0.9307, 'grad_norm': 0.7721452713012695, 'learning_rate': 0.0, 'epoch': 0.0}
{'loss': 0.9791, 'grad_norm': 1.4133292436599731, 'learning_rate': 4.444444444444445e-06, 'epoch': 0.0}
{'loss': 0.9094, 'grad_norm': 1.0886770486831665, 'learning_rate': 8.88888888888889e-06, 'epoch': 0.0}
{'loss': 0.8215, 'grad_norm': 1.0224614143371582, 'learning_rate': 1.3333333333333333e-05, 'epoch': 0.0}
{'loss': 0.7413, 'grad_norm': 0.7345637679100037, 'learning_rate': 1.777777777777778e-05, 'epoch': 0.0}
{'loss': 1.0487, 'grad_norm': 1.720858097076416, 'learning_rate': 2.2222222222222223e-05, 'epoch': 0.0}
{'loss': 1.03, 'grad_norm': 0.9008910655975342, 'learning_rate': 2.6666666666666667e-05, 'epoch': 0.0}
{'loss': 0.8502, 'grad_norm': 1.114410400390625, 'learning_rate': 3.111111111111111e-05, 'epoch': 0.01}
{'loss': 0.7529, 'grad_norm': 0.5373455882072449, 'learning_rate': 3.555555555555556e-05, 'epoch': 0.01}
{'loss': 0.7808, 'grad_norm': 0.5137835144996643, 'learning_rate': 4e-05, 'epoch': 0.01}
{'loss': 0.6818, 'grad_norm': 0.5595455765724182, 'learning_rate': 4.4444444444444447e-05, 'epoch': 0.01}
{'loss': 0.6323, 'grad_norm': 0.44398051500320435, 'learning_rate': 4.888888888888889e-05, 'epoch': 0.01}
{'loss': 0.7512, 'grad_norm': 0.7728102207183838, 'learning_rate': 5.333333333333333e-05, 'epoch': 0.01}
{'loss': 0.717, 'grad_norm': 0.578015923500061, 'learning_rate': 5.7777777777777776e-05, 'epoch': 0.01}
{'loss': 0.8061, 'grad_norm': 0.7537094354629517, 'learning_rate': 6.222222222222222e-05, 'epoch': 0.01}
{'loss': 0.8248, 'grad_norm': 0.4538315534591675, 'learning_rate': 6.666666666666667e-05, 'epoch': 0.01}
[2026-02-09 12:29:51,534] [WARNING] [stage3.py:2274:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.693, 'grad_norm': 0.5001166462898254, 'learning_rate': 7.111111111111112e-05, 'epoch': 0.01}
{'loss': 0.6838, 'grad_norm': 0.26403889060020447, 'learning_rate': 7.555555555555556e-05, 'epoch': 0.01}
{'loss': 0.7947, 'grad_norm': 0.4532715678215027, 'learning_rate': 8e-05, 'epoch': 0.01}
{'loss': 0.7981, 'grad_norm': 0.5158246159553528, 'learning_rate': 8.444444444444444e-05, 'epoch': 0.01}
{'loss': 0.6908, 'grad_norm': 0.336169570684433, 'learning_rate': 8.888888888888889e-05, 'epoch': 0.01}
{'loss': 0.8004, 'grad_norm': 0.24252039194107056, 'learning_rate': 9.333333333333334e-05, 'epoch': 0.01}
{'loss': 0.7011, 'grad_norm': 0.3057715892791748, 'learning_rate': 9.777777777777778e-05, 'epoch': 0.02}
{'loss': 0.6708, 'grad_norm': 0.2460460215806961, 'learning_rate': 0.00010222222222222222, 'epoch': 0.02}
{'loss': 0.6405, 'grad_norm': 0.4781881868839264, 'learning_rate': 0.00010666666666666667, 'epoch': 0.02}
{'loss': 0.6789, 'grad_norm': 0.4022640287876129, 'learning_rate': 0.00011111111111111112, 'epoch': 0.02}
{'loss': 0.7325, 'grad_norm': 0.4965517520904541, 'learning_rate': 0.00011555555555555555, 'epoch': 0.02}
{'loss': 0.7616, 'grad_norm': 0.3912856876850128, 'learning_rate': 0.00012, 'epoch': 0.02}
{'loss': 0.6705, 'grad_norm': 0.32064682245254517, 'learning_rate': 0.00012444444444444444, 'epoch': 0.02}
{'loss': 0.525, 'grad_norm': 0.2303573191165924, 'learning_rate': 0.00012888888888888892, 'epoch': 0.02}
{'loss': 0.7584, 'grad_norm': 0.302426815032959, 'learning_rate': 0.00013333333333333334, 'epoch': 0.02}
{'loss': 0.6952, 'grad_norm': 0.24876773357391357, 'learning_rate': 0.0001377777777777778, 'epoch': 0.02}
{'loss': 0.7808, 'grad_norm': 0.3837733268737793, 'learning_rate': 0.00014222222222222224, 'epoch': 0.02}
{'loss': 0.6436, 'grad_norm': 0.2988187372684479, 'learning_rate': 0.00014666666666666666, 'epoch': 0.02}
{'loss': 0.645, 'grad_norm': 0.49687251448631287, 'learning_rate': 0.0001511111111111111, 'epoch': 0.02}
{'loss': 0.7773, 'grad_norm': 0.34753552079200745, 'learning_rate': 0.00015555555555555556, 'epoch': 0.02}
{'loss': 0.7016, 'grad_norm': 0.37498965859413147, 'learning_rate': 0.00016, 'epoch': 0.03}
{'loss': 0.7119, 'grad_norm': 0.604378879070282, 'learning_rate': 0.00016444444444444444, 'epoch': 0.03}
{'loss': 0.6809, 'grad_norm': 0.4447372257709503, 'learning_rate': 0.00016888888888888889, 'epoch': 0.03}
{'loss': 0.7395, 'grad_norm': 0.4474585950374603, 'learning_rate': 0.00017333333333333334, 'epoch': 0.03}
{'loss': 0.682, 'grad_norm': 0.35147470235824585, 'learning_rate': 0.00017777777777777779, 'epoch': 0.03}
{'loss': 0.7535, 'grad_norm': 0.43196210265159607, 'learning_rate': 0.00018222222222222224, 'epoch': 0.03}
{'loss': 0.7147, 'grad_norm': 0.41327857971191406, 'learning_rate': 0.0001866666666666667, 'epoch': 0.03}
{'loss': 0.778, 'grad_norm': 0.4588475823402405, 'learning_rate': 0.00019111111111111114, 'epoch': 0.03}
{'loss': 0.6731, 'grad_norm': 0.262443482875824, 'learning_rate': 0.00019555555555555556, 'epoch': 0.03}
{'loss': 0.5626, 'grad_norm': 0.30674949288368225, 'learning_rate': 0.0002, 'epoch': 0.03}
{'loss': 0.7472, 'grad_norm': 0.3342629075050354, 'learning_rate': 0.0001999997580012294, 'epoch': 0.03}
{'loss': 0.8051, 'grad_norm': 0.294944703578949, 'learning_rate': 0.00019999903200608888, 'epoch': 0.03}
{'loss': 0.6659, 'grad_norm': 0.457041472196579, 'learning_rate': 0.00019999782201809226, 'epoch': 0.03}
{'loss': 0.6176, 'grad_norm': 0.35484299063682556, 'learning_rate': 0.0001999961280430958, 'epoch': 0.03}
{'loss': 0.5977, 'grad_norm': 0.3901214003562927, 'learning_rate': 0.0001999939500892983, 'epoch': 0.03}
{'loss': 0.5484, 'grad_norm': 0.3483926057815552, 'learning_rate': 0.00019999128816724108, 'epoch': 0.04}
{'loss': 0.5294, 'grad_norm': 0.3269480764865875, 'learning_rate': 0.0001999881422898077, 'epoch': 0.04}
{'loss': 0.6148, 'grad_norm': 0.3547949492931366, 'learning_rate': 0.00019998451247222416, 'epoch': 0.04}
{'loss': 0.5974, 'grad_norm': 0.4458550810813904, 'learning_rate': 0.00019998039873205868, 'epoch': 0.04}
{'loss': 0.5862, 'grad_norm': 0.27033257484436035, 'learning_rate': 0.00019997580108922165, 'epoch': 0.04}
{'loss': 0.6041, 'grad_norm': 0.4231521487236023, 'learning_rate': 0.0001999707195659656, 'epoch': 0.04}
{'loss': 0.7081, 'grad_norm': 0.38844120502471924, 'learning_rate': 0.0001999651541868849, 'epoch': 0.04}
{'loss': 0.7396, 'grad_norm': 0.2943965196609497, 'learning_rate': 0.00019995910497891587, 'epoch': 0.04}
{'loss': 0.6729, 'grad_norm': 0.3480381667613983, 'learning_rate': 0.0001999525719713366, 'epoch': 0.04}
{'loss': 0.7022, 'grad_norm': 0.29424139857292175, 'learning_rate': 0.00019994555519576662, 'epoch': 0.04}
{'loss': 0.5187, 'grad_norm': 0.38627228140830994, 'learning_rate': 0.00019993805468616693, 'epoch': 0.04}
{'loss': 0.6049, 'grad_norm': 0.3601972758769989, 'learning_rate': 0.00019993007047883988, 'epoch': 0.04}
{'loss': 0.5901, 'grad_norm': 0.23841455578804016, 'learning_rate': 0.00019992160261242877, 'epoch': 0.04}
{'loss': 0.6376, 'grad_norm': 0.45510950684547424, 'learning_rate': 0.0001999126511279179, 'epoch': 0.04}
{'loss': 0.7513, 'grad_norm': 2.668412923812866, 'learning_rate': 0.00019990321606863225, 'epoch': 0.04}
{'loss': 0.7365, 'grad_norm': 0.5159078240394592, 'learning_rate': 0.00019989329748023725, 'epoch': 0.05}
{'loss': 0.7145, 'grad_norm': 0.7914908528327942, 'learning_rate': 0.00019988289541073863, 'epoch': 0.05}
{'loss': 0.7729, 'grad_norm': 0.4484212100505829, 'learning_rate': 0.00019987200991048216, 'epoch': 0.05}
{'loss': 0.7984, 'grad_norm': 0.3238886296749115, 'learning_rate': 0.00019986064103215339, 'epoch': 0.05}
{'loss': 0.8193, 'grad_norm': 0.3814014494419098, 'learning_rate': 0.00019984878883077737, 'epoch': 0.05}
{'loss': 0.6199, 'grad_norm': 0.278530091047287, 'learning_rate': 0.00019983645336371853, 'epoch': 0.05}
{'loss': 0.6239, 'grad_norm': 0.3045260012149811, 'learning_rate': 0.0001998236346906802, 'epoch': 0.05}
{'loss': 0.6885, 'grad_norm': 0.3890630304813385, 'learning_rate': 0.00019981033287370443, 'epoch': 0.05}
{'loss': 0.664, 'grad_norm': 0.33995506167411804, 'learning_rate': 0.0001997965479771717, 'epoch': 0.05}
{'loss': 0.5824, 'grad_norm': 0.3852987587451935, 'learning_rate': 0.00019978228006780054, 'epoch': 0.05}
{'loss': 0.5617, 'grad_norm': 0.22332647442817688, 'learning_rate': 0.00019976752921464734, 'epoch': 0.05}
{'loss': 0.6889, 'grad_norm': 0.29107826948165894, 'learning_rate': 0.00019975229548910582, 'epoch': 0.05}
[2026-02-09 13:19:47,512] [WARNING] [stage3.py:2274:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.6739, 'grad_norm': 0.28281038999557495, 'learning_rate': 0.00019973657896490686, 'epoch': 0.05}
{'loss': 0.7605, 'grad_norm': 0.4619443714618683, 'learning_rate': 0.00019972037971811802, 'epoch': 0.05}
{'loss': 0.7583, 'grad_norm': 0.32929515838623047, 'learning_rate': 0.0001997036978271433, 'epoch': 0.06}
{'loss': 0.6431, 'grad_norm': 0.3647229075431824, 'learning_rate': 0.00019968653337272261, 'epoch': 0.06}
{'loss': 0.5674, 'grad_norm': 0.1778690367937088, 'learning_rate': 0.0001996688864379315, 'epoch': 0.06}
{'loss': 0.6843, 'grad_norm': 0.38676050305366516, 'learning_rate': 0.0001996507571081807, 'epoch': 0.06}
{'loss': 0.6174, 'grad_norm': 0.27765336632728577, 'learning_rate': 0.0001996321454712157, 'epoch': 0.06}
{'loss': 0.7057, 'grad_norm': 0.23891420662403107, 'learning_rate': 0.0001996130516171164, 'epoch': 0.06}
{'loss': 0.6915, 'grad_norm': 0.22092895209789276, 'learning_rate': 0.00019959347563829657, 'epoch': 0.06}
{'loss': 0.9158, 'grad_norm': 0.5218536853790283, 'learning_rate': 0.00019957341762950344, 'epoch': 0.06}
{'loss': 0.6227, 'grad_norm': 0.4208160638809204, 'learning_rate': 0.00019955287768781734, 'epoch': 0.06}
{'loss': 0.7487, 'grad_norm': 0.27539241313934326, 'learning_rate': 0.00019953185591265103, 'epoch': 0.06}
{'loss': 0.6692, 'grad_norm': 0.255460649728775, 'learning_rate': 0.0001995103524057494, 'epoch': 0.06}
{'loss': 0.5905, 'grad_norm': 0.186614990234375, 'learning_rate': 0.00019948836727118888, 'epoch': 0.06}
{'loss': 0.8715, 'grad_norm': 0.41005799174308777, 'learning_rate': 0.00019946590061537702, 'epoch': 0.06}
{'loss': 0.6797, 'grad_norm': 0.24903006851673126, 'learning_rate': 0.00019944295254705185, 'epoch': 0.06}
{'loss': 0.6728, 'grad_norm': 0.2737872004508972, 'learning_rate': 0.00019941952317728147, 'epoch': 0.06}
{'loss': 0.6924, 'grad_norm': 0.20008252561092377, 'learning_rate': 0.00019939561261946343, 'epoch': 0.07}
{'loss': 0.5707, 'grad_norm': 0.33250826597213745, 'learning_rate': 0.00019937122098932428, 'epoch': 0.07}
{'loss': 0.6646, 'grad_norm': 0.30801835656166077, 'learning_rate': 0.00019934634840491886, 'epoch': 0.07}
{'loss': 0.6479, 'grad_norm': 0.22076693177223206, 'learning_rate': 0.00019932099498662993, 'epoch': 0.07}
{'loss': 0.6928, 'grad_norm': 0.22490255534648895, 'learning_rate': 0.00019929516085716734, 'epoch': 0.07}
{'loss': 0.763, 'grad_norm': 0.28896990418434143, 'learning_rate': 0.0001992688461415677, 'epoch': 0.07}
{'loss': 0.6074, 'grad_norm': 0.2958163917064667, 'learning_rate': 0.0001992420509671936, 'epoch': 0.07}
{'loss': 0.6163, 'grad_norm': 0.2614251971244812, 'learning_rate': 0.00019921477546373296, 'epoch': 0.07}
{'loss': 0.6826, 'grad_norm': 0.40020307898521423, 'learning_rate': 0.00019918701976319857, 'epoch': 0.07}
{'loss': 0.657, 'grad_norm': 0.21724754571914673, 'learning_rate': 0.00019915878399992732, 'epoch': 0.07}
{'loss': 0.728, 'grad_norm': 0.7807772755622864, 'learning_rate': 0.00019913006831057969, 'epoch': 0.07}
{'loss': 0.6145, 'grad_norm': 0.3475845456123352, 'learning_rate': 0.00019910087283413883, 'epoch': 0.07}
{'loss': 0.6174, 'grad_norm': 0.3115443289279938, 'learning_rate': 0.00019907119771191015, 'epoch': 0.07}
{'loss': 0.7757, 'grad_norm': 0.30406075716018677, 'learning_rate': 0.0001990410430875205, 'epoch': 0.07}
{'loss': 0.6354, 'grad_norm': 0.33567845821380615, 'learning_rate': 0.0001990104091069176, 'epoch': 0.07}
{'loss': 0.6076, 'grad_norm': 0.36685243248939514, 'learning_rate': 0.00019897929591836903, 'epoch': 0.08}
{'loss': 0.689, 'grad_norm': 0.2964310348033905, 'learning_rate': 0.00019894770367246195, 'epoch': 0.08}
{'loss': 0.6248, 'grad_norm': 0.24347899854183197, 'learning_rate': 0.00019891563252210203, 'epoch': 0.08}
{'loss': 0.7818, 'grad_norm': 0.32335373759269714, 'learning_rate': 0.00019888308262251285, 'epoch': 0.08}
{'loss': 0.6813, 'grad_norm': 0.34644341468811035, 'learning_rate': 0.00019885005413123515, 'epoch': 0.08}
{'loss': 0.7014, 'grad_norm': 0.2968464493751526, 'learning_rate': 0.00019881654720812594, 'epoch': 0.08}
{'loss': 0.5592, 'grad_norm': 0.34396281838417053, 'learning_rate': 0.000198782562015358, 'epoch': 0.08}
{'loss': 0.7304, 'grad_norm': 0.3220386505126953, 'learning_rate': 0.00019874809871741876, 'epoch': 0.08}
{'loss': 0.9473, 'grad_norm': 0.4411284923553467, 'learning_rate': 0.00019871315748110972, 'epoch': 0.08}
{'loss': 0.6429, 'grad_norm': 0.37330445647239685, 'learning_rate': 0.00019867773847554568, 'epoch': 0.08}
{'loss': 0.6655, 'grad_norm': 0.6025868058204651, 'learning_rate': 0.00019864184187215372, 'epoch': 0.08}
{'loss': 0.807, 'grad_norm': 0.31094279885292053, 'learning_rate': 0.00019860546784467248, 'epoch': 0.08}
{'loss': 0.6961, 'grad_norm': 0.2421727180480957, 'learning_rate': 0.00019856861656915143, 'epoch': 0.08}
{'loss': 0.7129, 'grad_norm': 0.48664671182632446, 'learning_rate': 0.00019853128822394975, 'epoch': 0.08}
{'loss': 0.8569, 'grad_norm': 0.448537141084671, 'learning_rate': 0.0001984934829897358, 'epoch': 0.08}
{'loss': 0.6377, 'grad_norm': 0.3660299777984619, 'learning_rate': 0.00019845520104948592, 'epoch': 0.09}
{'loss': 0.4993, 'grad_norm': 0.28821486234664917, 'learning_rate': 0.0001984164425884838, 'epoch': 0.09}
{'loss': 0.6961, 'grad_norm': 0.295521080493927, 'learning_rate': 0.00019837720779431942, 'epoch': 0.09}
{'loss': 0.6963, 'grad_norm': 0.3012920022010803, 'learning_rate': 0.00019833749685688824, 'epoch': 0.09}
{'loss': 0.7392, 'grad_norm': 0.2832539677619934, 'learning_rate': 0.0001982973099683902, 'epoch': 0.09}
{'loss': 0.6214, 'grad_norm': 0.283075749874115, 'learning_rate': 0.00019825664732332884, 'epoch': 0.09}
{'loss': 0.8831, 'grad_norm': 0.3253920376300812, 'learning_rate': 0.00019821550911851036, 'epoch': 0.09}
{'loss': 0.5999, 'grad_norm': 0.2684498131275177, 'learning_rate': 0.00019817389555304272, 'epoch': 0.09}
{'loss': 0.7959, 'grad_norm': 0.32547011971473694, 'learning_rate': 0.00019813180682833447, 'epoch': 0.09}
{'loss': 0.6471, 'grad_norm': 0.9520447850227356, 'learning_rate': 0.0001980892431480941, 'epoch': 0.09}
{'loss': 0.7589, 'grad_norm': 0.2773679494857788, 'learning_rate': 0.0001980462047183287, 'epoch': 0.09}
{'loss': 0.635, 'grad_norm': 0.3456280529499054, 'learning_rate': 0.0001980026917473432, 'epoch': 0.09}
{'loss': 0.6846, 'grad_norm': 0.3176789879798889, 'learning_rate': 0.00019795870444573935, 'epoch': 0.09}
{'loss': 0.7147, 'grad_norm': 0.26898178458213806, 'learning_rate': 0.0001979142430264146, 'epoch': 0.09}
{'loss': 0.6259, 'grad_norm': 0.21845899522304535, 'learning_rate': 0.00019786930770456117, 'epoch': 0.1}
{'loss': 0.6074, 'grad_norm': 0.21532024443149567, 'learning_rate': 0.0001978238986976648, 'epoch': 0.1}
{'loss': 0.6923, 'grad_norm': 0.337561696767807, 'learning_rate': 0.00019777801622550408, 'epoch': 0.1}
{'loss': 0.8509, 'grad_norm': 0.4516255855560303, 'learning_rate': 0.000197731660510149, 'epoch': 0.1}
{'loss': 0.515, 'grad_norm': 0.20457284152507782, 'learning_rate': 0.0001976848317759601, 'epoch': 0.1}
{'loss': 0.7112, 'grad_norm': 0.2367071509361267, 'learning_rate': 0.00019763753024958723, 'epoch': 0.1}
{'loss': 0.6735, 'grad_norm': 0.3667047917842865, 'learning_rate': 0.00019758975615996873, 'epoch': 0.1}
{'loss': 0.6985, 'grad_norm': 0.1888953447341919, 'learning_rate': 0.0001975415097383299, 'epoch': 0.1}
{'loss': 0.6496, 'grad_norm': 0.3019585609436035, 'learning_rate': 0.00019749279121818235, 'epoch': 0.1}
{'loss': 0.6576, 'grad_norm': 0.359977126121521, 'learning_rate': 0.00019744360083532247, 'epoch': 0.1}
{'loss': 0.6391, 'grad_norm': 0.30430302023887634, 'learning_rate': 0.00019739393882783047, 'epoch': 0.1}
{'loss': 0.5852, 'grad_norm': 0.6355593800544739, 'learning_rate': 0.0001973438054360693, 'epoch': 0.1}
{'loss': 0.7252, 'grad_norm': 0.46917179226875305, 'learning_rate': 0.0001972932009026833, 'epoch': 0.1}
{'loss': 0.5134, 'grad_norm': 0.35627225041389465, 'learning_rate': 0.00019724212547259716, 'epoch': 0.1}
{'loss': 0.6523, 'grad_norm': 0.28060147166252136, 'learning_rate': 0.00019719057939301477, 'epoch': 0.1}
{'loss': 0.7056, 'grad_norm': 0.2612786889076233, 'learning_rate': 0.00019713856291341783, 'epoch': 0.11}
{'loss': 0.6245, 'grad_norm': 0.4776436388492584, 'learning_rate': 0.00019708607628556485, 'epoch': 0.11}
{'loss': 0.8484, 'grad_norm': 0.33752140402793884, 'learning_rate': 0.0001970331197634898, 'epoch': 0.11}
{'loss': 0.6392, 'grad_norm': 0.21057254076004028, 'learning_rate': 0.00019697969360350098, 'epoch': 0.11}
{'loss': 0.5557, 'grad_norm': 0.2354736328125, 'learning_rate': 0.0001969257980641796, 'epoch': 0.11}
{'loss': 0.72, 'grad_norm': 0.29445597529411316, 'learning_rate': 0.00019687143340637887, 'epoch': 0.11}
{'loss': 0.6835, 'grad_norm': 0.46066299080848694, 'learning_rate': 0.00019681659989322233, 'epoch': 0.11}
{'loss': 0.7082, 'grad_norm': 0.21752385795116425, 'learning_rate': 0.00019676129779010282, 'epoch': 0.11}
{'loss': 0.7632, 'grad_norm': 0.36513325572013855, 'learning_rate': 0.00019670552736468118, 'epoch': 0.11}
{'loss': 0.6705, 'grad_norm': 0.3339005410671234, 'learning_rate': 0.0001966492888868849, 'epoch': 0.11}
{'loss': 0.6732, 'grad_norm': 1.400420904159546, 'learning_rate': 0.00019659258262890683, 'epoch': 0.11}
{'loss': 0.6708, 'grad_norm': 0.1976778507232666, 'learning_rate': 0.00019653540886520386, 'epoch': 0.11}
{'loss': 0.7338, 'grad_norm': 0.3080761730670929, 'learning_rate': 0.00019647776787249565, 'epoch': 0.11}
{'loss': 0.6962, 'grad_norm': 0.33376383781433105, 'learning_rate': 0.00019641965992976308, 'epoch': 0.11}
{'loss': 0.6388, 'grad_norm': 0.33336353302001953, 'learning_rate': 0.00019636108531824724, 'epoch': 0.11}
{'loss': 0.6553, 'grad_norm': 0.25374484062194824, 'learning_rate': 0.0001963020443214478, 'epoch': 0.12}
{'loss': 0.6466, 'grad_norm': 0.25729644298553467, 'learning_rate': 0.00019624253722512174, 'epoch': 0.12}
{'loss': 0.8141, 'grad_norm': 0.36423733830451965, 'learning_rate': 0.00019618256431728194, 'epoch': 0.12}
{'loss': 0.5883, 'grad_norm': 0.36814892292022705, 'learning_rate': 0.00019612212588819575, 'epoch': 0.12}
{'loss': 0.6705, 'grad_norm': 0.2531428039073944, 'learning_rate': 0.00019606122223038376, 'epoch': 0.12}
{'loss': 0.6918, 'grad_norm': 0.3764787018299103, 'learning_rate': 0.0001959998536386181, 'epoch': 0.12}
{'loss': 0.5657, 'grad_norm': 0.3239559233188629, 'learning_rate': 0.00019593802040992128, 'epoch': 0.12}
{'loss': 0.7344, 'grad_norm': 0.4174410104751587, 'learning_rate': 0.00019587572284356463, 'epoch': 0.12}
{'loss': 0.667, 'grad_norm': 0.25066056847572327, 'learning_rate': 0.0001958129612410668, 'epoch': 0.12}
{'loss': 0.6778, 'grad_norm': 0.29407399892807007, 'learning_rate': 0.00019574973590619243, 'epoch': 0.12}
{'loss': 0.7369, 'grad_norm': 0.3110421895980835, 'learning_rate': 0.0001956860471449506, 'epoch': 0.12}
{'loss': 0.6833, 'grad_norm': 0.27289125323295593, 'learning_rate': 0.0001956218952655933, 'epoch': 0.12}
{'loss': 0.6221, 'grad_norm': 0.26952090859413147, 'learning_rate': 0.0001955572805786141, 'epoch': 0.12}
{'loss': 0.7086, 'grad_norm': 0.40269094705581665, 'learning_rate': 0.00019549220339674643, 'epoch': 0.12}
{'loss': 0.6128, 'grad_norm': 0.28405749797821045, 'learning_rate': 0.00019542666403496233, 'epoch': 0.12}
{'loss': 0.6239, 'grad_norm': 0.3097859025001526, 'learning_rate': 0.00019536066281047063, 'epoch': 0.13}
{'loss': 0.6933, 'grad_norm': 0.26954177021980286, 'learning_rate': 0.00019529420004271567, 'epoch': 0.13}
{'loss': 0.8206, 'grad_norm': 0.29938554763793945, 'learning_rate': 0.0001952272760533756, 'epoch': 0.13}
{'loss': 0.6654, 'grad_norm': 0.3695489466190338, 'learning_rate': 0.0001951598911663609, 'epoch': 0.13}
{'loss': 0.564, 'grad_norm': 0.3855414092540741, 'learning_rate': 0.00019509204570781273, 'epoch': 0.13}
{'loss': 0.5648, 'grad_norm': 0.3281165361404419, 'learning_rate': 0.00019502374000610151, 'epoch': 0.13}
{'loss': 0.6751, 'grad_norm': 0.37447023391723633, 'learning_rate': 0.0001949549743918251, 'epoch': 0.13}
[2026-02-09 14:50:16,506] [WARNING] [stage3.py:2274:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.693, 'grad_norm': 0.37475594878196716, 'learning_rate': 0.00019488574919780736, 'epoch': 0.13}
{'loss': 0.7555, 'grad_norm': 0.3804723024368286, 'learning_rate': 0.0001948160647590966, 'epoch': 0.13}
{'loss': 0.884, 'grad_norm': 0.6937606334686279, 'learning_rate': 0.00019474592141296372, 'epoch': 0.13}
{'loss': 0.7347, 'grad_norm': 0.24868975579738617, 'learning_rate': 0.00019467531949890086, 'epoch': 0.13}
{'loss': 0.7044, 'grad_norm': 0.4600081145763397, 'learning_rate': 0.00019460425935861948, 'epoch': 0.13}
{'loss': 0.4787, 'grad_norm': 0.192743182182312, 'learning_rate': 0.00019453274133604895, 'epoch': 0.13}
{'loss': 0.675, 'grad_norm': 0.2432420253753662, 'learning_rate': 0.00019446076577733475, 'epoch': 0.13}
{'loss': 0.5696, 'grad_norm': 0.2504326105117798, 'learning_rate': 0.00019438833303083678, 'epoch': 0.14}
{'loss': 0.781, 'grad_norm': 0.31496483087539673, 'learning_rate': 0.00019431544344712776, 'epoch': 0.14}
{'loss': 0.6663, 'grad_norm': 0.258262038230896, 'learning_rate': 0.0001942420973789915, 'epoch': 0.14}
{'loss': 0.7473, 'grad_norm': 0.3408324122428894, 'learning_rate': 0.00019416829518142118, 'epoch': 0.14}
{'loss': 0.5533, 'grad_norm': 0.2566181719303131, 'learning_rate': 0.00019409403721161756, 'epoch': 0.14}
{'loss': 0.725, 'grad_norm': 0.30262088775634766, 'learning_rate': 0.00019401932382898744, 'epoch': 0.14}
{'loss': 0.6726, 'grad_norm': 0.40016788244247437, 'learning_rate': 0.00019394415539514178, 'epoch': 0.14}
{'loss': 0.6228, 'grad_norm': 0.20785687863826752, 'learning_rate': 0.0001938685322738939, 'epoch': 0.14}
{'loss': 0.6585, 'grad_norm': 0.2052999585866928, 'learning_rate': 0.00019379245483125784, 'epoch': 0.14}
{'loss': 0.644, 'grad_norm': 0.2643490135669708, 'learning_rate': 0.00019371592343544656, 'epoch': 0.14}
{'loss': 0.7732, 'grad_norm': 0.25740131735801697, 'learning_rate': 0.00019363893845687015, 'epoch': 0.14}
{'loss': 0.8472, 'grad_norm': 0.349398136138916, 'learning_rate': 0.00019356150026813405, 'epoch': 0.14}
{'loss': 0.7143, 'grad_norm': 0.24970148503780365, 'learning_rate': 0.00019348360924403713, 'epoch': 0.14}
{'loss': 0.6261, 'grad_norm': 0.27337130904197693, 'learning_rate': 0.00019340526576157004, 'epoch': 0.14}
{'loss': 0.7982, 'grad_norm': 0.3350779712200165, 'learning_rate': 0.00019332647019991333, 'epoch': 0.14}
{'loss': 0.6544, 'grad_norm': 0.29827845096588135, 'learning_rate': 0.00019324722294043558, 'epoch': 0.15}
{'loss': 0.7796, 'grad_norm': 0.39642274379730225, 'learning_rate': 0.0001931675243666916, 'epoch': 0.15}
{'loss': 0.7041, 'grad_norm': 0.23298484086990356, 'learning_rate': 0.00019308737486442045, 'epoch': 0.15}
{'loss': 0.5733, 'grad_norm': 0.2601325809955597, 'learning_rate': 0.0001930067748215438, 'epoch': 0.15}
{'loss': 0.6476, 'grad_norm': 0.2284848839044571, 'learning_rate': 0.00019292572462816388, 'epoch': 0.15}
{'loss': 0.7782, 'grad_norm': 0.3058721721172333, 'learning_rate': 0.0001928442246765616, 'epoch': 0.15}
{'loss': 0.6268, 'grad_norm': 0.5302316546440125, 'learning_rate': 0.0001927622753611948, 'epoch': 0.15}
[2026-02-09 15:13:48,287] [WARNING] [stage3.py:2274:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.5391, 'grad_norm': 0.31134098768234253, 'learning_rate': 0.00019267987707869606, 'epoch': 0.15}
{'loss': 0.7927, 'grad_norm': 0.3563663065433502, 'learning_rate': 0.0001925970302278711, 'epoch': 0.15}
{'loss': 0.8079, 'grad_norm': 0.32126685976982117, 'learning_rate': 0.0001925137352096966, 'epoch': 0.15}
{'loss': 0.7656, 'grad_norm': 0.3600529730319977, 'learning_rate': 0.00019242999242731842, 'epoch': 0.15}
{'loss': 0.707, 'grad_norm': 0.2184348702430725, 'learning_rate': 0.0001923458022860496, 'epoch': 0.15}
{'loss': 0.7625, 'grad_norm': 0.48579686880111694, 'learning_rate': 0.0001922611651933683, 'epoch': 0.15}
{'loss': 0.6423, 'grad_norm': 0.28114646673202515, 'learning_rate': 0.00019217608155891596, 'epoch': 0.15}
{'loss': 0.5891, 'grad_norm': 0.3819039463996887, 'learning_rate': 0.0001920905517944954, 'epoch': 0.15}
{'loss': 0.5194, 'grad_norm': 0.2860521674156189, 'learning_rate': 0.0001920045763140684, 'epoch': 0.16}
[2026-02-09 15:21:02,164] [WARNING] [stage3.py:2274:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.6909, 'grad_norm': 0.29931873083114624, 'learning_rate': 0.00019191815553375427, 'epoch': 0.16}
{'loss': 0.6843, 'grad_norm': 0.36333781480789185, 'learning_rate': 0.00019183128987182747, 'epoch': 0.16}
{'loss': 0.6956, 'grad_norm': 0.3245343863964081, 'learning_rate': 0.00019174397974871564, 'epoch': 0.16}
{'loss': 0.696, 'grad_norm': 0.310453861951828, 'learning_rate': 0.00019165622558699763, 'epoch': 0.16}
{'loss': 0.6979, 'grad_norm': 0.41585713624954224, 'learning_rate': 0.0001915680278114014, 'epoch': 0.16}
{'loss': 0.7236, 'grad_norm': 0.293470561504364, 'learning_rate': 0.0001914793868488021, 'epoch': 0.16}
{'loss': 0.6715, 'grad_norm': 0.19597208499908447, 'learning_rate': 0.00019139030312821973, 'epoch': 0.16}
{'loss': 0.7165, 'grad_norm': 0.25631850957870483, 'learning_rate': 0.00019130077708081732, 'epoch': 0.16}
{'loss': 0.6849, 'grad_norm': 0.32682883739471436, 'learning_rate': 0.0001912108091398988, 'epoch': 0.16}
{'loss': 0.6799, 'grad_norm': 0.31246304512023926, 'learning_rate': 0.0001911203997409067, 'epoch': 0.16}
{'loss': 0.7679, 'grad_norm': 0.4540252387523651, 'learning_rate': 0.00019102954932142033, 'epoch': 0.16}
{'loss': 0.6186, 'grad_norm': 0.32997927069664, 'learning_rate': 0.0001909382583211535, 'epoch': 0.16}
{'loss': 0.6768, 'grad_norm': 0.248642697930336, 'learning_rate': 0.00019084652718195238, 'epoch': 0.16}
{'loss': 0.5374, 'grad_norm': 0.1981206089258194, 'learning_rate': 0.00019075435634779343, 'epoch': 0.17}
{'loss': 0.6944, 'grad_norm': 0.3143192529678345, 'learning_rate': 0.0001906617462647813, 'epoch': 0.17}
{'loss': 0.6307, 'grad_norm': 0.30426353216171265, 'learning_rate': 0.00019056869738114638, 'epoch': 0.17}
{'loss': 0.6828, 'grad_norm': 0.32039186358451843, 'learning_rate': 0.00019047521014724304, 'epoch': 0.17}
{'loss': 0.6559, 'grad_norm': 0.4933789372444153, 'learning_rate': 0.0001903812850155472, 'epoch': 0.17}
{'loss': 0.813, 'grad_norm': 0.4030376970767975, 'learning_rate': 0.00019028692244065426, 'epoch': 0.17}
{'loss': 0.6973, 'grad_norm': 0.413373202085495, 'learning_rate': 0.00019019212287927663, 'epoch': 0.17}
{'loss': 0.7043, 'grad_norm': 0.4516341984272003, 'learning_rate': 0.0001900968867902419, 'epoch': 0.17}
{'loss': 0.8505, 'grad_norm': 0.43799400329589844, 'learning_rate': 0.00019000121463449046, 'epoch': 0.17}
{'loss': 0.7029, 'grad_norm': 0.3061172664165497, 'learning_rate': 0.00018990510687507314, 'epoch': 0.17}
{'loss': 0.6284, 'grad_norm': 0.24583423137664795, 'learning_rate': 0.00018980856397714913, 'epoch': 0.17}
{'loss': 0.6835, 'grad_norm': 0.42177391052246094, 'learning_rate': 0.00018971158640798368, 'epoch': 0.17}
{'loss': 0.6425, 'grad_norm': 0.21154797077178955, 'learning_rate': 0.00018961417463694586, 'epoch': 0.17}
{'loss': 0.5893, 'grad_norm': 0.3568713068962097, 'learning_rate': 0.00018951632913550626, 'epoch': 0.17}
{'loss': 0.69, 'grad_norm': 0.2521984577178955, 'learning_rate': 0.00018941805037723465, 'epoch': 0.17}
{'loss': 0.5796, 'grad_norm': 0.37919512391090393, 'learning_rate': 0.00018931933883779785, 'epoch': 0.18}
{'loss': 0.7706, 'grad_norm': 0.4469758868217468, 'learning_rate': 0.00018922019499495725, 'epoch': 0.18}
{'loss': 0.7121, 'grad_norm': 0.2831338346004486, 'learning_rate': 0.00018912061932856663, 'epoch': 0.18}
{'loss': 0.6374, 'grad_norm': 0.3067805767059326, 'learning_rate': 0.00018902061232056977, 'epoch': 0.18}
{'loss': 0.9046, 'grad_norm': 0.33004987239837646, 'learning_rate': 0.0001889201744549981, 'epoch': 0.18}
{'loss': 0.63, 'grad_norm': 0.33634963631629944, 'learning_rate': 0.00018881930621796847, 'epoch': 0.18}
{'loss': 0.7739, 'grad_norm': 0.29159605503082275, 'learning_rate': 0.00018871800809768067, 'epoch': 0.18}
{'loss': 0.7432, 'grad_norm': 0.5204683542251587, 'learning_rate': 0.00018861628058441506, 'epoch': 0.18}
{'loss': 0.711, 'grad_norm': 0.3369591236114502, 'learning_rate': 0.0001885141241705303, 'epoch': 0.18}
{'loss': 0.6477, 'grad_norm': 0.24116843938827515, 'learning_rate': 0.00018841153935046098, 'epoch': 0.18}
{'loss': 0.7311, 'grad_norm': 0.3246218264102936, 'learning_rate': 0.00018830852662071507, 'epoch': 0.18}
{'loss': 0.5762, 'grad_norm': 0.33268919587135315, 'learning_rate': 0.00018820508647987165, 'epoch': 0.18}
{'loss': 0.5959, 'grad_norm': 0.275950163602829, 'learning_rate': 0.00018810121942857845, 'epoch': 0.18}
{'loss': 0.6637, 'grad_norm': 0.28176257014274597, 'learning_rate': 0.00018799692596954947, 'epoch': 0.18}
{'loss': 0.6291, 'grad_norm': 0.24563008546829224, 'learning_rate': 0.0001878922066075625, 'epoch': 0.18}
{'loss': 0.6438, 'grad_norm': 0.3212602734565735, 'learning_rate': 0.0001877870618494566, 'epoch': 0.19}
{'loss': 0.5623, 'grad_norm': 0.27678462862968445, 'learning_rate': 0.0001876814922041299, 'epoch': 0.19}
{'loss': 0.5844, 'grad_norm': 1.1903395652770996, 'learning_rate': 0.0001875754981825368, 'epoch': 0.19}
{'loss': 0.6628, 'grad_norm': 0.23486004769802094, 'learning_rate': 0.00018746908029768585, 'epoch': 0.19}
{'loss': 0.7562, 'grad_norm': 0.3867153227329254, 'learning_rate': 0.00018736223906463696, 'epoch': 0.19}
{'loss': 0.6296, 'grad_norm': 0.45675042271614075, 'learning_rate': 0.00018725497500049907, 'epoch': 0.19}
{'loss': 0.7079, 'grad_norm': 0.352875292301178, 'learning_rate': 0.0001871472886244276, 'epoch': 0.19}
{'loss': 0.6459, 'grad_norm': 0.20844756066799164, 'learning_rate': 0.00018703918045762197, 'epoch': 0.19}
{'loss': 0.7003, 'grad_norm': 0.3730771839618683, 'learning_rate': 0.00018693065102332306, 'epoch': 0.19}
{'loss': 0.7811, 'grad_norm': 0.2960215210914612, 'learning_rate': 0.00018682170084681065, 'epoch': 0.19}
{'loss': 0.6121, 'grad_norm': 0.42206594347953796, 'learning_rate': 0.0001867123304554009, 'epoch': 0.19}
{'loss': 0.751, 'grad_norm': 0.2380007952451706, 'learning_rate': 0.00018660254037844388, 'epoch': 0.19}
{'loss': 0.7835, 'grad_norm': 0.3398238718509674, 'learning_rate': 0.0001864923311473208, 'epoch': 0.19}
{'loss': 0.6877, 'grad_norm': 0.3327701687812805, 'learning_rate': 0.00018638170329544164, 'epoch': 0.19}
[2026-02-09 16:06:49,645] [WARNING] [stage3.py:2274:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.6573, 'grad_norm': 0.22236277163028717, 'learning_rate': 0.00018627065735824248, 'epoch': 0.19}
{'loss': 0.5586, 'grad_norm': 0.4558206796646118, 'learning_rate': 0.00018615919387318297, 'epoch': 0.2}
{'loss': 0.6061, 'grad_norm': 0.6891971230506897, 'learning_rate': 0.00018604731337974357, 'epoch': 0.2}
{'loss': 0.5755, 'grad_norm': 0.3391786515712738, 'learning_rate': 0.00018593501641942317, 'epoch': 0.2}
{'loss': 0.8143, 'grad_norm': 0.35616105794906616, 'learning_rate': 0.00018582230353573627, 'epoch': 0.2}
{'loss': 0.6768, 'grad_norm': 0.4190550744533539, 'learning_rate': 0.00018570917527421048, 'epoch': 0.2}
{'loss': 0.7164, 'grad_norm': 0.4677138924598694, 'learning_rate': 0.0001855956321823838, 'epoch': 0.2}
{'loss': 0.7891, 'grad_norm': 0.4049092233181, 'learning_rate': 0.00018548167480980193, 'epoch': 0.2}
{'loss': 0.6516, 'grad_norm': 0.3923403322696686, 'learning_rate': 0.00018536730370801585, 'epoch': 0.2}
{'loss': 0.7776, 'grad_norm': 0.48025521636009216, 'learning_rate': 0.00018525251943057885, 'epoch': 0.2}
{'loss': 0.6502, 'grad_norm': 0.19217224419116974, 'learning_rate': 0.000185137322533044, 'epoch': 0.2}
{'loss': 0.5873, 'grad_norm': 0.4703029990196228, 'learning_rate': 0.00018502171357296144, 'epoch': 0.2}
{'loss': 0.5796, 'grad_norm': 0.17957712709903717, 'learning_rate': 0.0001849056931098757, 'epoch': 0.2}
{'loss': 0.6226, 'grad_norm': 0.27054521441459656, 'learning_rate': 0.00018478926170532298, 'epoch': 0.2}
{'loss': 0.7133, 'grad_norm': 0.2543017566204071, 'learning_rate': 0.00018467241992282843, 'epoch': 0.2}
{'loss': 0.6271, 'grad_norm': 0.3049336373806, 'learning_rate': 0.00018455516832790338, 'epoch': 0.21}
{'loss': 0.6239, 'grad_norm': 0.22885379195213318, 'learning_rate': 0.00018443750748804266, 'epoch': 0.21}
{'loss': 0.7174, 'grad_norm': 0.3209044337272644, 'learning_rate': 0.00018431943797272187, 'epoch': 0.21}
{'loss': 0.6624, 'grad_norm': 0.34147128462791443, 'learning_rate': 0.00018420096035339452, 'epoch': 0.21}
{'loss': 0.6214, 'grad_norm': 0.4098781943321228, 'learning_rate': 0.00018408207520348942, 'epoch': 0.21}
{'loss': 0.601, 'grad_norm': 0.30834442377090454, 'learning_rate': 0.00018396278309840779, 'epoch': 0.21}
{'loss': 0.7537, 'grad_norm': 0.2302454710006714, 'learning_rate': 0.0001838430846155204, 'epoch': 0.21}
{'loss': 0.6248, 'grad_norm': 0.22440339624881744, 'learning_rate': 0.000183722980334165, 'epoch': 0.21}
{'loss': 0.5951, 'grad_norm': 0.3133467733860016, 'learning_rate': 0.00018360247083564342, 'epoch': 0.21}
{'loss': 0.649, 'grad_norm': 0.8769293427467346, 'learning_rate': 0.00018348155670321859, 'epoch': 0.21}
{'loss': 0.7366, 'grad_norm': 0.3543587923049927, 'learning_rate': 0.00018336023852211195, 'epoch': 0.21}
{'loss': 0.628, 'grad_norm': 0.25727930665016174, 'learning_rate': 0.00018323851687950055, 'epoch': 0.21}
{'loss': 0.7626, 'grad_norm': 0.29535582661628723, 'learning_rate': 0.00018311639236451416, 'epoch': 0.21}
{'loss': 0.7195, 'grad_norm': 0.3797985315322876, 'learning_rate': 0.00018299386556823235, 'epoch': 0.21}
{'loss': 0.6995, 'grad_norm': 0.298480361700058, 'learning_rate': 0.00018287093708368188, 'epoch': 0.21}
{'loss': 0.542, 'grad_norm': 0.3973228633403778, 'learning_rate': 0.00018274760750583354, 'epoch': 0.22}
{'loss': 0.664, 'grad_norm': 0.25924772024154663, 'learning_rate': 0.0001826238774315995, 'epoch': 0.22}
{'loss': 0.778, 'grad_norm': 0.4097082018852234, 'learning_rate': 0.00018249974745983023, 'epoch': 0.22}
{'loss': 0.7065, 'grad_norm': 0.219573974609375, 'learning_rate': 0.00018237521819131178, 'epoch': 0.22}
{'loss': 0.6328, 'grad_norm': 0.32812604308128357, 'learning_rate': 0.00018225029022876275, 'epoch': 0.22}
{'loss': 0.7616, 'grad_norm': 0.27709004282951355, 'learning_rate': 0.00018212496417683137, 'epoch': 0.22}
{'loss': 0.6477, 'grad_norm': 0.4376414716243744, 'learning_rate': 0.0001819992406420927, 'epoch': 0.22}
{'loss': 0.6014, 'grad_norm': 0.26566800475120544, 'learning_rate': 0.00018187312023304548, 'epoch': 0.22}
{'loss': 0.6038, 'grad_norm': 0.23940224945545197, 'learning_rate': 0.00018174660356010943, 'epoch': 0.22}
{'loss': 0.5145, 'grad_norm': 0.2245665192604065, 'learning_rate': 0.0001816196912356222, 'epoch': 0.22}
{'loss': 0.6908, 'grad_norm': 0.2947406470775604, 'learning_rate': 0.00018149238387383623, 'epoch': 0.22}
{'loss': 0.6263, 'grad_norm': 0.22368647158145905, 'learning_rate': 0.00018136468209091602, 'epoch': 0.22}
{'loss': 0.8243, 'grad_norm': 0.26957714557647705, 'learning_rate': 0.00018123658650493512, 'epoch': 0.22}
{'loss': 0.6127, 'grad_norm': 0.33009234070777893, 'learning_rate': 0.000181108097735873, 'epoch': 0.22}
[2026-02-09 16:42:54,474] [WARNING] [stage3.py:2274:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.6958, 'grad_norm': 0.29640743136405945, 'learning_rate': 0.0001809792164056121, 'epoch': 0.22}
{'loss': 0.7921, 'grad_norm': 0.3693712651729584, 'learning_rate': 0.00018084994313793495, 'epoch': 0.23}
{'loss': 0.7172, 'grad_norm': 0.3695891201496124, 'learning_rate': 0.00018072027855852097, 'epoch': 0.23}
{'loss': 0.581, 'grad_norm': 0.5736150741577148, 'learning_rate': 0.0001805902232949435, 'epoch': 0.23}
{'loss': 0.5291, 'grad_norm': 0.3997284770011902, 'learning_rate': 0.00018045977797666684, 'epoch': 0.23}
{'loss': 0.689, 'grad_norm': 0.30268996953964233, 'learning_rate': 0.00018032894323504316, 'epoch': 0.23}
{'loss': 0.7879, 'grad_norm': 0.5826317667961121, 'learning_rate': 0.0001801977197033093, 'epoch': 0.23}
{'loss': 0.6851, 'grad_norm': 0.2917678654193878, 'learning_rate': 0.000180066108016584, 'epoch': 0.23}
{'loss': 0.6213, 'grad_norm': 0.2630440890789032, 'learning_rate': 0.00017993410881186458, 'epoch': 0.23}
{'loss': 0.634, 'grad_norm': 0.39070940017700195, 'learning_rate': 0.000179801722728024, 'epoch': 0.23}
{'loss': 0.6994, 'grad_norm': 0.3409932255744934, 'learning_rate': 0.00017966895040580748, 'epoch': 0.23}
{'loss': 0.7679, 'grad_norm': 0.2813071012496948, 'learning_rate': 0.00017953579248782995, 'epoch': 0.23}
{'loss': 0.7794, 'grad_norm': 0.4361826181411743, 'learning_rate': 0.00017940224961857242, 'epoch': 0.23}
{'loss': 0.6614, 'grad_norm': 0.3749982416629791, 'learning_rate': 0.0001792683224443791, 'epoch': 0.23}
{'loss': 0.7303, 'grad_norm': 0.3109644055366516, 'learning_rate': 0.00017913401161345416, 'epoch': 0.23}
{'loss': 0.6087, 'grad_norm': 0.3130338490009308, 'learning_rate': 0.00017899931777585882, 'epoch': 0.24}
[2026-02-09 16:55:37,023] [WARNING] [stage3.py:2274:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.8131, 'grad_norm': 0.24370111525058746, 'learning_rate': 0.00017886424158350782, 'epoch': 0.24}
{'loss': 0.6922, 'grad_norm': 0.37422502040863037, 'learning_rate': 0.00017872878369016673, 'epoch': 0.24}
{'loss': 0.616, 'grad_norm': 0.35913366079330444, 'learning_rate': 0.00017859294475144837, 'epoch': 0.24}
{'loss': 0.6969, 'grad_norm': 0.3807152807712555, 'learning_rate': 0.00017845672542480984, 'epoch': 0.24}
{'loss': 0.6971, 'grad_norm': 0.3598615825176239, 'learning_rate': 0.00017832012636954936, 'epoch': 0.24}
{'loss': 0.7315, 'grad_norm': 0.3351404666900635, 'learning_rate': 0.000178183148246803, 'epoch': 0.24}
{'loss': 0.6449, 'grad_norm': 0.4063749313354492, 'learning_rate': 0.00017804579171954148, 'epoch': 0.24}
{'loss': 0.7472, 'grad_norm': 0.2802487015724182, 'learning_rate': 0.00017790805745256704, 'epoch': 0.24}
{'loss': 0.699, 'grad_norm': 0.3253813087940216, 'learning_rate': 0.00017776994611251015, 'epoch': 0.24}
{'loss': 0.5749, 'grad_norm': 0.24342389404773712, 'learning_rate': 0.0001776314583678263, 'epoch': 0.24}
{'loss': 0.5789, 'grad_norm': 0.3157629668712616, 'learning_rate': 0.00017749259488879275, 'epoch': 0.24}
{'loss': 0.7003, 'grad_norm': 0.2902268171310425, 'learning_rate': 0.00017735335634750532, 'epoch': 0.24}
{'loss': 0.6821, 'grad_norm': 0.2998955249786377, 'learning_rate': 0.00017721374341787515, 'epoch': 0.24}
{'loss': 0.5776, 'grad_norm': 0.2994632422924042, 'learning_rate': 0.00017707375677562538, 'epoch': 0.24}
{'loss': 0.8526, 'grad_norm': 0.36241984367370605, 'learning_rate': 0.00017693339709828792, 'epoch': 0.25}
{'loss': 0.5381, 'grad_norm': 0.3422166109085083, 'learning_rate': 0.00017679266506520012, 'epoch': 0.25}
{'loss': 0.6426, 'grad_norm': 0.637984037399292, 'learning_rate': 0.0001766515613575016, 'epoch': 0.25}
{'loss': 0.6617, 'grad_norm': 0.40606921911239624, 'learning_rate': 0.00017651008665813081, 'epoch': 0.25}
{'loss': 0.7166, 'grad_norm': 0.25457867980003357, 'learning_rate': 0.0001763682416518219, 'epoch': 0.25}
{'loss': 0.7109, 'grad_norm': 0.28800398111343384, 'learning_rate': 0.00017622602702510105, 'epoch': 0.25}
{'loss': 0.6004, 'grad_norm': 0.15203681588172913, 'learning_rate': 0.0001760834434662837, 'epoch': 0.25}
{'loss': 0.7329, 'grad_norm': 0.2509802579879761, 'learning_rate': 0.00017594049166547073, 'epoch': 0.25}
{'loss': 0.7357, 'grad_norm': 0.27568793296813965, 'learning_rate': 0.0001757971723145453, 'epoch': 0.25}
{'loss': 0.6815, 'grad_norm': 0.3516845107078552, 'learning_rate': 0.0001756534861071696, 'epoch': 0.25}
{'loss': 0.56, 'grad_norm': 0.3651936948299408, 'learning_rate': 0.0001755094337387813, 'epoch': 0.25}
{'loss': 0.6576, 'grad_norm': 0.3027029037475586, 'learning_rate': 0.00017536501590659036, 'epoch': 0.25}
{'loss': 0.6132, 'grad_norm': 0.3851512372493744, 'learning_rate': 0.00017522023330957548, 'epoch': 0.25}
{'loss': 0.7181, 'grad_norm': 1.6797003746032715, 'learning_rate': 0.00017507508664848094, 'epoch': 0.25}
{'loss': 0.633, 'grad_norm': 0.4067816436290741, 'learning_rate': 0.00017492957662581295, 'epoch': 0.25}
{'loss': 0.567, 'grad_norm': 0.3606634736061096, 'learning_rate': 0.00017478370394583646, 'epoch': 0.26}
{'loss': 0.6939, 'grad_norm': 0.3381116986274719, 'learning_rate': 0.0001746374693145717, 'epoch': 0.26}
{'loss': 0.5537, 'grad_norm': 0.3717197775840759, 'learning_rate': 0.0001744908734397906, 'epoch': 0.26}
{'loss': 0.6846, 'grad_norm': 0.3757854700088501, 'learning_rate': 0.00017434391703101363, 'epoch': 0.26}
{'loss': 0.5391, 'grad_norm': 0.2874312996864319, 'learning_rate': 0.00017419660079950623, 'epoch': 0.26}
{'loss': 0.69, 'grad_norm': 0.37904903292655945, 'learning_rate': 0.00017404892545827533, 'epoch': 0.26}
{'loss': 0.6403, 'grad_norm': 0.47223326563835144, 'learning_rate': 0.00017390089172206592, 'epoch': 0.26}
{'loss': 0.7258, 'grad_norm': 0.3958050012588501, 'learning_rate': 0.00017375250030735766, 'epoch': 0.26}
{'loss': 0.6824, 'grad_norm': 0.29584982991218567, 'learning_rate': 0.00017360375193236133, 'epoch': 0.26}
{'loss': 0.5934, 'grad_norm': 0.19679825007915497, 'learning_rate': 0.00017345464731701547, 'epoch': 0.26}
{'loss': 0.704, 'grad_norm': 0.23514480888843536, 'learning_rate': 0.00017330518718298264, 'epoch': 0.26}
{'loss': 0.5399, 'grad_norm': 0.254682719707489, 'learning_rate': 0.00017315537225364632, 'epoch': 0.26}
{'loss': 0.6723, 'grad_norm': 0.2857709527015686, 'learning_rate': 0.00017300520325410701, 'epoch': 0.26}
{'loss': 0.5472, 'grad_norm': 0.33910030126571655, 'learning_rate': 0.00017285468091117904, 'epoch': 0.26}
{'loss': 0.7458, 'grad_norm': 0.35374701023101807, 'learning_rate': 0.0001727038059533868, 'epoch': 0.26}
{'loss': 0.7227, 'grad_norm': 0.2668200731277466, 'learning_rate': 0.0001725525791109614, 'epoch': 0.27}
{'loss': 0.6836, 'grad_norm': 0.30253109335899353, 'learning_rate': 0.00017240100111583703, 'epoch': 0.27}
{'loss': 0.7435, 'grad_norm': 0.25735145807266235, 'learning_rate': 0.00017224907270164746, 'epoch': 0.27}
{'loss': 0.6184, 'grad_norm': 0.29632696509361267, 'learning_rate': 0.0001720967946037225, 'epoch': 0.27}
{'loss': 0.7013, 'grad_norm': 0.3164880573749542, 'learning_rate': 0.00017194416755908436, 'epoch': 0.27}
{'loss': 0.5783, 'grad_norm': 0.30495554208755493, 'learning_rate': 0.0001717911923064442, 'epoch': 0.27}
{'loss': 0.6776, 'grad_norm': 0.38389357924461365, 'learning_rate': 0.0001716378695861985, 'epoch': 0.27}
{'loss': 0.5956, 'grad_norm': 0.2884669601917267, 'learning_rate': 0.0001714842001404254, 'epoch': 0.27}
{'loss': 0.7226, 'grad_norm': 0.6012521386146545, 'learning_rate': 0.00017133018471288133, 'epoch': 0.27}
{'loss': 0.7286, 'grad_norm': 0.3109017312526703, 'learning_rate': 0.00017117582404899712, 'epoch': 0.27}
{'loss': 0.571, 'grad_norm': 0.2768697440624237, 'learning_rate': 0.00017102111889587458, 'epoch': 0.27}
{'loss': 0.6747, 'grad_norm': 0.5094801783561707, 'learning_rate': 0.00017086607000228282, 'epoch': 0.27}
{'loss': 0.6658, 'grad_norm': 0.3090399503707886, 'learning_rate': 0.00017071067811865476, 'epoch': 0.27}
{'loss': 0.6519, 'grad_norm': 0.38577428460121155, 'learning_rate': 0.00017055494399708323, 'epoch': 0.27}
{'loss': 0.5634, 'grad_norm': 0.30137231945991516, 'learning_rate': 0.00017039886839131759, 'epoch': 0.28}
{'loss': 0.6682, 'grad_norm': 0.38393691182136536, 'learning_rate': 0.00017024245205675986, 'epoch': 0.28}
{'loss': 0.6926, 'grad_norm': 2.050844192504883, 'learning_rate': 0.00017008569575046134, 'epoch': 0.28}
{'loss': 0.8047, 'grad_norm': 0.3376897871494293, 'learning_rate': 0.00016992860023111862, 'epoch': 0.28}
{'loss': 0.6749, 'grad_norm': 0.35006779432296753, 'learning_rate': 0.00016977116625907024, 'epoch': 0.28}
{'loss': 0.5277, 'grad_norm': 0.297773152589798, 'learning_rate': 0.0001696133945962927, 'epoch': 0.28}
{'loss': 0.7455, 'grad_norm': 0.33842232823371887, 'learning_rate': 0.00016945528600639692, 'epoch': 0.28}
{'loss': 0.5332, 'grad_norm': 0.29835012555122375, 'learning_rate': 0.0001692968412546247, 'epoch': 0.28}
{'loss': 0.7527, 'grad_norm': 0.2978537380695343, 'learning_rate': 0.00016913806110784466, 'epoch': 0.28}
{'loss': 0.6672, 'grad_norm': 0.38468337059020996, 'learning_rate': 0.00016897894633454886, 'epoch': 0.28}
{'loss': 0.6543, 'grad_norm': 0.4082014560699463, 'learning_rate': 0.0001688194977048488, 'epoch': 0.28}
{'loss': 0.641, 'grad_norm': 0.29360276460647583, 'learning_rate': 0.000168659715990472, 'epoch': 0.28}
{'loss': 0.625, 'grad_norm': 0.2874123156070709, 'learning_rate': 0.00016849960196475806, 'epoch': 0.28}
{'loss': 0.6331, 'grad_norm': 0.3162027597427368, 'learning_rate': 0.00016833915640265484, 'epoch': 0.28}
{'loss': 0.5868, 'grad_norm': 0.35428106784820557, 'learning_rate': 0.00016817838008071503, 'epoch': 0.28}
{'loss': 0.6637, 'grad_norm': 0.38510406017303467, 'learning_rate': 0.00016801727377709194, 'epoch': 0.29}
{'loss': 0.634, 'grad_norm': 0.37198755145072937, 'learning_rate': 0.00016785583827153618, 'epoch': 0.29}
{'loss': 0.6331, 'grad_norm': 0.21711747348308563, 'learning_rate': 0.00016769407434539168, 'epoch': 0.29}
{'loss': 0.772, 'grad_norm': 0.31279388070106506, 'learning_rate': 0.00016753198278159182, 'epoch': 0.29}
{'loss': 0.7217, 'grad_norm': 0.26433220505714417, 'learning_rate': 0.00016736956436465573, 'epoch': 0.29}
{'loss': 0.7509, 'grad_norm': 0.32113826274871826, 'learning_rate': 0.00016720681988068462, 'epoch': 0.29}
{'loss': 0.7146, 'grad_norm': 0.3469649851322174, 'learning_rate': 0.0001670437501173578, 'epoch': 0.29}
{'loss': 0.7034, 'grad_norm': 0.7503575682640076, 'learning_rate': 0.00016688035586392885, 'epoch': 0.29}
{'loss': 0.7037, 'grad_norm': 0.4035641849040985, 'learning_rate': 0.000166716637911222, 'epoch': 0.29}
{'loss': 0.6179, 'grad_norm': 0.21321821212768555, 'learning_rate': 0.0001665525970516281, 'epoch': 0.29}
{'loss': 0.6025, 'grad_norm': 0.1941632181406021, 'learning_rate': 0.00016638823407910084, 'epoch': 0.29}
{'loss': 0.6349, 'grad_norm': 0.2898523807525635, 'learning_rate': 0.00016622354978915304, 'epoch': 0.29}
{'loss': 0.6554, 'grad_norm': 0.21141843497753143, 'learning_rate': 0.00016605854497885253, 'epoch': 0.29}
[2026-02-09 18:05:06,408] [WARNING] [stage3.py:2274:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.7658, 'grad_norm': 0.3777136206626892, 'learning_rate': 0.00016589322044681861, 'epoch': 0.29}
{'loss': 0.7356, 'grad_norm': 0.28568458557128906, 'learning_rate': 0.00016572757699321791, 'epoch': 0.29}
{'loss': 0.6979, 'grad_norm': 0.3120613992214203, 'learning_rate': 0.00016556161541976064, 'epoch': 0.3}
{'loss': 0.6999, 'grad_norm': 0.35277992486953735, 'learning_rate': 0.00016539533652969683, 'epoch': 0.3}
{'loss': 0.6995, 'grad_norm': 0.3572380244731903, 'learning_rate': 0.00016522874112781213, 'epoch': 0.3}
{'loss': 0.461, 'grad_norm': 0.7880094051361084, 'learning_rate': 0.0001650618300204242, 'epoch': 0.3}
{'loss': 0.7111, 'grad_norm': 0.47235843539237976, 'learning_rate': 0.00016489460401537874, 'epoch': 0.3}
{'loss': 0.6029, 'grad_norm': 0.32462868094444275, 'learning_rate': 0.0001647270639220455, 'epoch': 0.3}
{'loss': 0.7568, 'grad_norm': 0.36980727314949036, 'learning_rate': 0.0001645592105513144, 'epoch': 0.3}
{'loss': 0.6342, 'grad_norm': 0.27970293164253235, 'learning_rate': 0.00016439104471559156, 'epoch': 0.3}
{'loss': 0.7347, 'grad_norm': 0.3183307945728302, 'learning_rate': 0.0001642225672287956, 'epoch': 0.3}
{'loss': 0.6225, 'grad_norm': 0.3105468153953552, 'learning_rate': 0.00016405377890635334, 'epoch': 0.3}
{'loss': 0.6226, 'grad_norm': 0.3354199528694153, 'learning_rate': 0.00016388468056519612, 'epoch': 0.3}
{'loss': 0.6714, 'grad_norm': 0.43857070803642273, 'learning_rate': 0.0001637152730237558, 'epoch': 0.3}
{'loss': 0.6454, 'grad_norm': 0.35888025164604187, 'learning_rate': 0.00016354555710196067, 'epoch': 0.3}
{'loss': 0.7366, 'grad_norm': 0.2766241729259491, 'learning_rate': 0.00016337553362123165, 'epoch': 0.3}
{'loss': 0.6699, 'grad_norm': 0.37998735904693604, 'learning_rate': 0.00016320520340447816, 'epoch': 0.31}
{'loss': 0.6129, 'grad_norm': 0.22625847160816193, 'learning_rate': 0.0001630345672760943, 'epoch': 0.31}
[2026-02-09 18:19:41,029] [WARNING] [stage3.py:2274:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.7689, 'grad_norm': 0.3486991822719574, 'learning_rate': 0.00016286362606195468, 'epoch': 0.31}
{'loss': 0.7527, 'grad_norm': 0.2917095422744751, 'learning_rate': 0.0001626923805894107, 'epoch': 0.31}
{'loss': 0.7014, 'grad_norm': 0.2892804741859436, 'learning_rate': 0.00016252083168728608, 'epoch': 0.31}
{'loss': 0.7936, 'grad_norm': 0.40794700384140015, 'learning_rate': 0.00016234898018587337, 'epoch': 0.31}
{'loss': 0.6892, 'grad_norm': 0.5323091149330139, 'learning_rate': 0.0001621768269169296, 'epoch': 0.31}
{'loss': 0.6329, 'grad_norm': 0.33134451508522034, 'learning_rate': 0.0001620043727136724, 'epoch': 0.31}
{'loss': 0.6216, 'grad_norm': 0.3435065746307373, 'learning_rate': 0.0001618316184107758, 'epoch': 0.31}
{'loss': 0.6185, 'grad_norm': 0.2605794370174408, 'learning_rate': 0.00016165856484436645, 'epoch': 0.31}
{'loss': 0.6666, 'grad_norm': 0.19965268671512604, 'learning_rate': 0.00016148521285201927, 'epoch': 0.31}
{'loss': 0.5957, 'grad_norm': 0.3119930028915405, 'learning_rate': 0.00016131156327275372, 'epoch': 0.31}
{'loss': 0.6246, 'grad_norm': 0.22395090758800507, 'learning_rate': 0.00016113761694702943, 'epoch': 0.31}
{'loss': 0.707, 'grad_norm': 0.6346316933631897, 'learning_rate': 0.00016096337471674241, 'epoch': 0.31}
{'loss': 0.5465, 'grad_norm': 0.32232776284217834, 'learning_rate': 0.00016078883742522075, 'epoch': 0.31}
{'loss': 0.6892, 'grad_norm': 0.3273630738258362, 'learning_rate': 0.00016061400591722058, 'epoch': 0.32}
{'loss': 0.5846, 'grad_norm': 0.43614256381988525, 'learning_rate': 0.0001604388810389222, 'epoch': 0.32}
{'loss': 0.7235, 'grad_norm': 0.34084901213645935, 'learning_rate': 0.00016026346363792567, 'epoch': 0.32}
{'loss': 0.82, 'grad_norm': 0.42457929253578186, 'learning_rate': 0.00016008775456324687, 'epoch': 0.32}
{'loss': 0.7417, 'grad_norm': 0.37135887145996094, 'learning_rate': 0.00015991175466531342, 'epoch': 0.32}
{'loss': 0.8441, 'grad_norm': 0.2895047962665558, 'learning_rate': 0.00015973546479596052, 'epoch': 0.32}
{'loss': 0.7808, 'grad_norm': 0.3187792897224426, 'learning_rate': 0.0001595588858084268, 'epoch': 0.32}
{'loss': 0.7931, 'grad_norm': 0.37609145045280457, 'learning_rate': 0.00015938201855735014, 'epoch': 0.32}
{'loss': 0.6037, 'grad_norm': 0.18936146795749664, 'learning_rate': 0.00015920486389876383, 'epoch': 0.32}
{'loss': 0.5279, 'grad_norm': 0.6799744367599487, 'learning_rate': 0.00015902742269009197, 'epoch': 0.32}
{'loss': 0.6289, 'grad_norm': 0.5640221238136292, 'learning_rate': 0.00015884969579014566, 'epoch': 0.32}
{'loss': 0.6262, 'grad_norm': 0.26022619009017944, 'learning_rate': 0.0001586716840591187, 'epoch': 0.32}
{'loss': 0.5878, 'grad_norm': 0.28676408529281616, 'learning_rate': 0.00015849338835858355, 'epoch': 0.32}
{'loss': 0.5659, 'grad_norm': 0.2977222800254822, 'learning_rate': 0.00015831480955148697, 'epoch': 0.32}
{'loss': 0.6428, 'grad_norm': 0.3533039689064026, 'learning_rate': 0.000158135948502146, 'epoch': 0.32}
{'loss': 0.6845, 'grad_norm': 0.2849905788898468, 'learning_rate': 0.00015795680607624373, 'epoch': 0.33}
{'loss': 0.7401, 'grad_norm': 0.5460483431816101, 'learning_rate': 0.00015777738314082514, 'epoch': 0.33}
{'loss': 0.6803, 'grad_norm': 0.25369352102279663, 'learning_rate': 0.00015759768056429274, 'epoch': 0.33}
{'loss': 0.6096, 'grad_norm': 0.5086636543273926, 'learning_rate': 0.0001574176992164026, 'epoch': 0.33}
{'loss': 0.6956, 'grad_norm': 0.3229794502258301, 'learning_rate': 0.00015723743996826013, 'epoch': 0.33}
{'loss': 0.6679, 'grad_norm': 0.3236982226371765, 'learning_rate': 0.00015705690369231551, 'epoch': 0.33}
{'loss': 0.6657, 'grad_norm': 0.25225314497947693, 'learning_rate': 0.00015687609126235997, 'epoch': 0.33}
{'loss': 0.7058, 'grad_norm': 0.43072909116744995, 'learning_rate': 0.00015669500355352116, 'epoch': 0.33}
{'loss': 0.6459, 'grad_norm': 0.2888272702693939, 'learning_rate': 0.0001565136414422592, 'epoch': 0.33}
{'loss': 0.636, 'grad_norm': 0.428307443857193, 'learning_rate': 0.0001563320058063622, 'epoch': 0.33}
{'loss': 0.6466, 'grad_norm': 0.3063206672668457, 'learning_rate': 0.00015615009752494222, 'epoch': 0.33}
{'loss': 0.8255, 'grad_norm': 0.43753331899642944, 'learning_rate': 0.0001559679174784308, 'epoch': 0.33}
{'loss': 0.6539, 'grad_norm': 0.46420609951019287, 'learning_rate': 0.000155785466548575, 'epoch': 0.33}
{'loss': 0.6545, 'grad_norm': 0.4856560230255127, 'learning_rate': 0.0001556027456184327, 'epoch': 0.33}
{'loss': 0.6225, 'grad_norm': 0.43134891986846924, 'learning_rate': 0.00015541975557236882, 'epoch': 0.33}
{'loss': 0.7014, 'grad_norm': 0.3549503684043884, 'learning_rate': 0.0001552364972960506, 'epoch': 0.34}
{'loss': 0.6204, 'grad_norm': 0.5171933174133301, 'learning_rate': 0.00015505297167644363, 'epoch': 0.34}
{'loss': 0.5803, 'grad_norm': 0.2576184570789337, 'learning_rate': 0.0001548691796018074, 'epoch': 0.34}
{'loss': 0.6258, 'grad_norm': 0.4063340425491333, 'learning_rate': 0.00015468512196169102, 'epoch': 0.34}
{'loss': 0.754, 'grad_norm': 0.335904061794281, 'learning_rate': 0.00015450079964692896, 'epoch': 0.34}
{'loss': 0.668, 'grad_norm': 0.4032256007194519, 'learning_rate': 0.00015431621354963668, 'epoch': 0.34}
{'loss': 0.649, 'grad_norm': 0.22427567839622498, 'learning_rate': 0.00015413136456320634, 'epoch': 0.34}
{'loss': 0.6603, 'grad_norm': 0.28131017088890076, 'learning_rate': 0.0001539462535823025, 'epoch': 0.34}
{'loss': 0.5494, 'grad_norm': 0.3677847981452942, 'learning_rate': 0.00015376088150285773, 'epoch': 0.34}
{'loss': 0.7713, 'grad_norm': 0.3518703281879425, 'learning_rate': 0.00015357524922206842, 'epoch': 0.34}
{'loss': 0.6449, 'grad_norm': 0.23598028719425201, 'learning_rate': 0.00015338935763839015, 'epoch': 0.34}
{'loss': 0.6055, 'grad_norm': 0.2684611678123474, 'learning_rate': 0.00015320320765153367, 'epoch': 0.34}
{'loss': 0.6965, 'grad_norm': 0.5487588047981262, 'learning_rate': 0.00015301680016246028, 'epoch': 0.34}
[2026-02-09 19:04:52,392] [WARNING] [stage3.py:2274:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.7434, 'grad_norm': 0.3458617925643921, 'learning_rate': 0.00015283013607337776, 'epoch': 0.34}
{'loss': 0.7555, 'grad_norm': 0.3040889501571655, 'learning_rate': 0.0001526432162877356, 'epoch': 0.35}
{'loss': 0.6018, 'grad_norm': 0.27181172370910645, 'learning_rate': 0.00015245604171022098, 'epoch': 0.35}
{'loss': 0.6993, 'grad_norm': 0.259166955947876, 'learning_rate': 0.0001522686132467543, 'epoch': 0.35}
{'loss': 0.6509, 'grad_norm': 0.3065475821495056, 'learning_rate': 0.0001520809318044847, 'epoch': 0.35}
{'loss': 0.6848, 'grad_norm': 0.30581557750701904, 'learning_rate': 0.00015189299829178573, 'epoch': 0.35}
{'loss': 0.683, 'grad_norm': 0.7338224649429321, 'learning_rate': 0.00015170481361825096, 'epoch': 0.35}
{'loss': 0.7218, 'grad_norm': 0.340000182390213, 'learning_rate': 0.0001515163786946896, 'epoch': 0.35}
{'loss': 0.6047, 'grad_norm': 0.37438589334487915, 'learning_rate': 0.00015132769443312207, 'epoch': 0.35}
{'loss': 0.712, 'grad_norm': 0.2877807319164276, 'learning_rate': 0.00015113876174677554, 'epoch': 0.35}
{'loss': 0.6134, 'grad_norm': 0.20358416438102722, 'learning_rate': 0.00015094958155007952, 'epoch': 0.35}
{'loss': 0.7562, 'grad_norm': 0.3878602683544159, 'learning_rate': 0.0001507601547586616, 'epoch': 0.35}
{'loss': 0.5666, 'grad_norm': 0.3245214819908142, 'learning_rate': 0.0001505704822893427, 'epoch': 0.35}
{'loss': 0.8507, 'grad_norm': 0.35430917143821716, 'learning_rate': 0.00015038056506013297, 'epoch': 0.35}
{'loss': 0.684, 'grad_norm': 0.36927327513694763, 'learning_rate': 0.0001501904039902271, 'epoch': 0.35}
{'loss': 0.68, 'grad_norm': 0.439937949180603, 'learning_rate': 0.00015000000000000001, 'epoch': 0.35}
{'loss': 0.719, 'grad_norm': 0.20590032637119293, 'learning_rate': 0.00014980935401100233, 'epoch': 0.36}
{'loss': 0.8128, 'grad_norm': 0.42381227016448975, 'learning_rate': 0.00014961846694595593, 'epoch': 0.36}
{'loss': 0.8032, 'grad_norm': 0.29744336009025574, 'learning_rate': 0.00014942733972874957, 'epoch': 0.36}
{'loss': 0.6408, 'grad_norm': 0.1787409484386444, 'learning_rate': 0.00014923597328443422, 'epoch': 0.36}
{'loss': 0.7701, 'grad_norm': 0.5773813724517822, 'learning_rate': 0.0001490443685392188, 'epoch': 0.36}
{'loss': 0.6838, 'grad_norm': 0.27909690141677856, 'learning_rate': 0.00014885252642046553, 'epoch': 0.36}
{'loss': 0.664, 'grad_norm': 0.373408704996109, 'learning_rate': 0.00014866044785668563, 'epoch': 0.36}
{'loss': 0.6962, 'grad_norm': 0.2571290135383606, 'learning_rate': 0.00014846813377753456, 'epoch': 0.36}
{'loss': 0.6399, 'grad_norm': 0.4390278458595276, 'learning_rate': 0.00014827558511380773, 'epoch': 0.36}
{'loss': 0.745, 'grad_norm': 0.39420264959335327, 'learning_rate': 0.00014808280279743593, 'epoch': 0.36}
{'loss': 0.6363, 'grad_norm': 0.3639530539512634, 'learning_rate': 0.0001478897877614809, 'epoch': 0.36}
{'loss': 0.7167, 'grad_norm': 0.3837060034275055, 'learning_rate': 0.00014769654094013058, 'epoch': 0.36}
{'loss': 0.6239, 'grad_norm': 0.23365724086761475, 'learning_rate': 0.00014750306326869492, 'epoch': 0.36}
{'loss': 0.7079, 'grad_norm': 0.2835024893283844, 'learning_rate': 0.00014730935568360102, 'epoch': 0.36}
{'loss': 0.7119, 'grad_norm': 0.33782249689102173, 'learning_rate': 0.00014711541912238886, 'epoch': 0.36}
{'loss': 0.7115, 'grad_norm': 0.36560893058776855, 'learning_rate': 0.00014692125452370663, 'epoch': 0.37}
{'loss': 0.7372, 'grad_norm': 0.3326360285282135, 'learning_rate': 0.0001467268628273062, 'epoch': 0.37}
{'loss': 0.6389, 'grad_norm': 0.27036792039871216, 'learning_rate': 0.00014653224497403864, 'epoch': 0.37}
{'loss': 0.5886, 'grad_norm': 0.3961980640888214, 'learning_rate': 0.00014633740190584952, 'epoch': 0.37}
{'loss': 0.6249, 'grad_norm': 0.410755455493927, 'learning_rate': 0.00014614233456577454, 'epoch': 0.37}
{'loss': 0.7507, 'grad_norm': 0.2655481994152069, 'learning_rate': 0.00014594704389793477, 'epoch': 0.37}
{'loss': 0.5782, 'grad_norm': 0.1896231472492218, 'learning_rate': 0.00014575153084753233, 'epoch': 0.37}
{'loss': 0.6499, 'grad_norm': 0.2227931171655655, 'learning_rate': 0.0001455557963608455, 'epoch': 0.37}
{'loss': 0.7657, 'grad_norm': 0.3603239059448242, 'learning_rate': 0.00014535984138522442, 'epoch': 0.37}
{'loss': 0.6297, 'grad_norm': 0.3761681914329529, 'learning_rate': 0.00014516366686908637, 'epoch': 0.37}
{'loss': 0.6744, 'grad_norm': 0.27809831500053406, 'learning_rate': 0.00014496727376191114, 'epoch': 0.37}
{'loss': 0.5561, 'grad_norm': 0.31470027565956116, 'learning_rate': 0.00014477066301423657, 'epoch': 0.37}
{'loss': 0.6974, 'grad_norm': 0.1957871913909912, 'learning_rate': 0.00014457383557765386, 'epoch': 0.37}
{'loss': 0.5564, 'grad_norm': 0.5072822570800781, 'learning_rate': 0.0001443767924048029, 'epoch': 0.37}
{'loss': 0.7117, 'grad_norm': 0.2979450523853302, 'learning_rate': 0.00014417953444936785, 'epoch': 0.37}
{'loss': 0.6714, 'grad_norm': 0.30265259742736816, 'learning_rate': 0.00014398206266607236, 'epoch': 0.38}
{'loss': 0.6818, 'grad_norm': 0.9022337794303894, 'learning_rate': 0.000143784378010675, 'epoch': 0.38}
[2026-02-09 19:43:34,276] [WARNING] [stage3.py:2274:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.667, 'grad_norm': 0.30469098687171936, 'learning_rate': 0.00014358648143996463, 'epoch': 0.38}
{'loss': 0.6622, 'grad_norm': 0.2553268373012543, 'learning_rate': 0.00014338837391175582, 'epoch': 0.38}
{'loss': 0.699, 'grad_norm': 0.28377631306648254, 'learning_rate': 0.0001431900563848841, 'epoch': 0.38}
{'loss': 0.5781, 'grad_norm': 0.6229742765426636, 'learning_rate': 0.00014299152981920145, 'epoch': 0.38}
{'loss': 0.6016, 'grad_norm': 0.19279199838638306, 'learning_rate': 0.00014279279517557156, 'epoch': 0.38}
{'loss': 0.6648, 'grad_norm': 0.2321246862411499, 'learning_rate': 0.0001425938534158652, 'epoch': 0.38}
{'loss': 0.6639, 'grad_norm': 0.23356522619724274, 'learning_rate': 0.0001423947055029556, 'epoch': 0.38}
{'loss': 0.6949, 'grad_norm': 0.2682957947254181, 'learning_rate': 0.00014219535240071377, 'epoch': 0.38}
[2026-02-09 19:50:10,889] [WARNING] [stage3.py:2274:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.7679, 'grad_norm': 0.41962355375289917, 'learning_rate': 0.00014199579507400385, 'epoch': 0.38}
{'loss': 0.7415, 'grad_norm': 0.30978021025657654, 'learning_rate': 0.00014179603448867835, 'epoch': 0.38}
{'loss': 0.5895, 'grad_norm': 0.21066397428512573, 'learning_rate': 0.00014159607161157362, 'epoch': 0.38}
{'loss': 0.6496, 'grad_norm': 0.2987475097179413, 'learning_rate': 0.00014139590741050502, 'epoch': 0.38}
{'loss': 0.5461, 'grad_norm': 0.22411514818668365, 'learning_rate': 0.00014119554285426246, 'epoch': 0.39}
{'loss': 0.8199, 'grad_norm': 0.327964723110199, 'learning_rate': 0.00014099497891260538, 'epoch': 0.39}
{'loss': 0.6776, 'grad_norm': 0.27939584851264954, 'learning_rate': 0.00014079421655625833, 'epoch': 0.39}
{'loss': 0.5498, 'grad_norm': 0.374836802482605, 'learning_rate': 0.0001405932567569062, 'epoch': 0.39}
{'loss': 0.6736, 'grad_norm': 0.4198252558708191, 'learning_rate': 0.00014039210048718949, 'epoch': 0.39}
{'loss': 0.6626, 'grad_norm': 0.4982069432735443, 'learning_rate': 0.0001401907487206996, 'epoch': 0.39}
{'loss': 0.6869, 'grad_norm': 0.3170725405216217, 'learning_rate': 0.00013998920243197407, 'epoch': 0.39}
{'loss': 0.6145, 'grad_norm': 0.35250648856163025, 'learning_rate': 0.00013978746259649209, 'epoch': 0.39}
[2026-02-09 19:59:56,394] [WARNING] [stage3.py:2274:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.706, 'grad_norm': 0.2847745418548584, 'learning_rate': 0.00013958553019066943, 'epoch': 0.39}
{'loss': 0.5772, 'grad_norm': 0.2533075511455536, 'learning_rate': 0.00013938340619185395, 'epoch': 0.39}
{'loss': 0.6204, 'grad_norm': 0.4045317769050598, 'learning_rate': 0.00013918109157832088, 'epoch': 0.39}
{'loss': 0.7532, 'grad_norm': 0.30713072419166565, 'learning_rate': 0.00013897858732926793, 'epoch': 0.39}
{'loss': 0.5244, 'grad_norm': 0.7260162830352783, 'learning_rate': 0.00013877589442481077, 'epoch': 0.39}
{'loss': 0.7041, 'grad_norm': 0.2501831650733948, 'learning_rate': 0.00013857301384597796, 'epoch': 0.39}
{'loss': 0.6397, 'grad_norm': 0.45392417907714844, 'learning_rate': 0.00013836994657470661, 'epoch': 0.39}
{'loss': 0.6874, 'grad_norm': 0.29798975586891174, 'learning_rate': 0.00013816669359383726, 'epoch': 0.4}
{'loss': 0.6961, 'grad_norm': 0.4139127731323242, 'learning_rate': 0.0001379632558871094, 'epoch': 0.4}
{'loss': 0.5885, 'grad_norm': 0.33020156621932983, 'learning_rate': 0.00013775963443915645, 'epoch': 0.4}
{'loss': 0.7026, 'grad_norm': 0.33135780692100525, 'learning_rate': 0.00013755583023550126, 'epoch': 0.4}
{'loss': 0.6196, 'grad_norm': 0.2166694700717926, 'learning_rate': 0.00013735184426255117, 'epoch': 0.4}
{'loss': 0.7043, 'grad_norm': 0.343746542930603, 'learning_rate': 0.00013714767750759322, 'epoch': 0.4}
{'loss': 0.7244, 'grad_norm': 0.34506383538246155, 'learning_rate': 0.00013694333095878958, 'epoch': 0.4}
{'loss': 0.7725, 'grad_norm': 0.31194090843200684, 'learning_rate': 0.00013673880560517246, 'epoch': 0.4}
{'loss': 0.7555, 'grad_norm': 0.2829630970954895, 'learning_rate': 0.00013653410243663952, 'epoch': 0.4}
{'loss': 0.8362, 'grad_norm': 0.24609532952308655, 'learning_rate': 0.0001363292224439491, 'epoch': 0.4}
{'loss': 0.7197, 'grad_norm': 0.560031533241272, 'learning_rate': 0.00013612416661871533, 'epoch': 0.4}
{'loss': 0.8129, 'grad_norm': 0.3035900592803955, 'learning_rate': 0.0001359189359534033, 'epoch': 0.4}
{'loss': 0.6439, 'grad_norm': 0.34385544061660767, 'learning_rate': 0.0001357135314413245, 'epoch': 0.4}
[2026-02-09 20:15:57,409] [WARNING] [stage3.py:2274:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.6248, 'grad_norm': 0.4657895863056183, 'learning_rate': 0.00013550795407663157, 'epoch': 0.4}
{'loss': 0.5931, 'grad_norm': 0.4062446057796478, 'learning_rate': 0.00013530220485431405, 'epoch': 0.4}
{'loss': 0.7385, 'grad_norm': 0.2163681983947754, 'learning_rate': 0.00013509628477019304, 'epoch': 0.41}
{'loss': 0.641, 'grad_norm': 0.34891945123672485, 'learning_rate': 0.0001348901948209167, 'epoch': 0.41}
{'loss': 0.6371, 'grad_norm': 0.26390430331230164, 'learning_rate': 0.00013468393600395525, 'epoch': 0.41}
[2026-02-09 20:20:00,966] [WARNING] [stage3.py:2274:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.6095, 'grad_norm': 0.2224055975675583, 'learning_rate': 0.0001344775093175964, 'epoch': 0.41}
{'loss': 0.5714, 'grad_norm': 0.3616146445274353, 'learning_rate': 0.00013427091576094022, 'epoch': 0.41}
{'loss': 0.595, 'grad_norm': 0.29357802867889404, 'learning_rate': 0.00013406415633389438, 'epoch': 0.41}
{'loss': 0.4835, 'grad_norm': 0.25252920389175415, 'learning_rate': 0.00013385723203716943, 'epoch': 0.41}
{'loss': 0.7454, 'grad_norm': 0.4927598834037781, 'learning_rate': 0.00013365014387227393, 'epoch': 0.41}
{'loss': 0.7848, 'grad_norm': 0.5501267313957214, 'learning_rate': 0.0001334428928415095, 'epoch': 0.41}
{'loss': 0.6401, 'grad_norm': 0.31949281692504883, 'learning_rate': 0.00013323547994796597, 'epoch': 0.41}
{'loss': 0.5147, 'grad_norm': 0.42943835258483887, 'learning_rate': 0.00013302790619551674, 'epoch': 0.41}
{'loss': 0.7126, 'grad_norm': 0.2496846616268158, 'learning_rate': 0.0001328201725888136, 'epoch': 0.41}
{'loss': 0.6923, 'grad_norm': 0.21145033836364746, 'learning_rate': 0.00013261228013328207, 'epoch': 0.41}
{'loss': 0.6634, 'grad_norm': 0.3022773563861847, 'learning_rate': 0.0001324042298351166, 'epoch': 0.41}
{'loss': 0.6216, 'grad_norm': 0.2851219177246094, 'learning_rate': 0.0001321960227012755, 'epoch': 0.42}
{'loss': 0.6008, 'grad_norm': 0.22313553094863892, 'learning_rate': 0.00013198765973947615, 'epoch': 0.42}
{'loss': 0.8484, 'grad_norm': 0.23837067186832428, 'learning_rate': 0.00013177914195819016, 'epoch': 0.42}
{'loss': 0.6034, 'grad_norm': 0.2577102780342102, 'learning_rate': 0.00013157047036663853, 'epoch': 0.42}
{'loss': 0.6617, 'grad_norm': 0.3017939329147339, 'learning_rate': 0.00013136164597478652, 'epoch': 0.42}
{'loss': 0.693, 'grad_norm': 0.3419877886772156, 'learning_rate': 0.00013115266979333917, 'epoch': 0.42}
{'loss': 0.7579, 'grad_norm': 0.3551924228668213, 'learning_rate': 0.00013094354283373598, 'epoch': 0.42}
{'loss': 0.6906, 'grad_norm': 0.28900474309921265, 'learning_rate': 0.0001307342661081463, 'epoch': 0.42}
{'loss': 0.5415, 'grad_norm': 0.3493272364139557, 'learning_rate': 0.0001305248406294644, 'epoch': 0.42}
{'loss': 0.6744, 'grad_norm': 0.27901795506477356, 'learning_rate': 0.00013031526741130435, 'epoch': 0.42}
{'loss': 0.8011, 'grad_norm': 0.26902633905410767, 'learning_rate': 0.00013010554746799543, 'epoch': 0.42}
{'loss': 0.5714, 'grad_norm': 0.28321969509124756, 'learning_rate': 0.00012989568181457704, 'epoch': 0.42}
{'loss': 0.649, 'grad_norm': 0.26941242814064026, 'learning_rate': 0.00012968567146679372, 'epoch': 0.42}
{'loss': 0.7593, 'grad_norm': 0.3303925693035126, 'learning_rate': 0.00012947551744109043, 'epoch': 0.42}
{'loss': 0.7692, 'grad_norm': 0.26965996623039246, 'learning_rate': 0.00012926522075460745, 'epoch': 0.42}
{'loss': 0.6805, 'grad_norm': 0.2552265524864197, 'learning_rate': 0.00012905478242517562, 'epoch': 0.43}
{'loss': 0.7245, 'grad_norm': 0.3409542143344879, 'learning_rate': 0.00012884420347131123, 'epoch': 0.43}
{'loss': 0.6333, 'grad_norm': 0.2279922068119049, 'learning_rate': 0.00012863348491221128, 'epoch': 0.43}
{'loss': 0.8082, 'grad_norm': 0.4668239653110504, 'learning_rate': 0.00012842262776774837, 'epoch': 0.43}
{'loss': 0.6265, 'grad_norm': 0.22236639261245728, 'learning_rate': 0.00012821163305846596, 'epoch': 0.43}
{'loss': 0.717, 'grad_norm': 0.4848316013813019, 'learning_rate': 0.00012800050180557322, 'epoch': 0.43}
{'loss': 0.7069, 'grad_norm': 0.2610943019390106, 'learning_rate': 0.00012778923503094016, 'epoch': 0.43}
{'loss': 0.6822, 'grad_norm': 0.39447638392448425, 'learning_rate': 0.00012757783375709288, 'epoch': 0.43}
{'loss': 0.6128, 'grad_norm': 0.2776521146297455, 'learning_rate': 0.0001273662990072083, 'epoch': 0.43}
{'loss': 0.7269, 'grad_norm': 0.32118502259254456, 'learning_rate': 0.00012715463180510943, 'epoch': 0.43}
{'loss': 0.8449, 'grad_norm': 0.37985655665397644, 'learning_rate': 0.00012694283317526026, 'epoch': 0.43}
{'loss': 0.6789, 'grad_norm': 0.43489235639572144, 'learning_rate': 0.00012673090414276101, 'epoch': 0.43}
{'loss': 0.7929, 'grad_norm': 0.4023869037628174, 'learning_rate': 0.00012651884573334297, 'epoch': 0.43}
{'loss': 0.6996, 'grad_norm': 0.3073541522026062, 'learning_rate': 0.00012630665897336364, 'epoch': 0.43}
{'loss': 0.6726, 'grad_norm': 0.23573245108127594, 'learning_rate': 0.00012609434488980168, 'epoch': 0.43}
{'loss': 0.6771, 'grad_norm': 0.28457748889923096, 'learning_rate': 0.00012588190451025207, 'epoch': 0.44}
{'loss': 0.6698, 'grad_norm': 0.28274714946746826, 'learning_rate': 0.00012566933886292106, 'epoch': 0.44}
{'loss': 0.7338, 'grad_norm': 0.33353111147880554, 'learning_rate': 0.00012545664897662109, 'epoch': 0.44}
{'loss': 0.8748, 'grad_norm': 0.3228013515472412, 'learning_rate': 0.00012524383588076598, 'epoch': 0.44}
{'loss': 0.5774, 'grad_norm': 0.328660249710083, 'learning_rate': 0.0001250309006053659, 'epoch': 0.44}
{'loss': 0.6022, 'grad_norm': 0.23804718255996704, 'learning_rate': 0.00012481784418102242, 'epoch': 0.44}
{'loss': 0.5107, 'grad_norm': 0.30447688698768616, 'learning_rate': 0.00012460466763892323, 'epoch': 0.44}
{'loss': 0.633, 'grad_norm': 0.23121590912342072, 'learning_rate': 0.00012439137201083773, 'epoch': 0.44}
{'loss': 0.7274, 'grad_norm': 0.21493977308273315, 'learning_rate': 0.0001241779583291114, 'epoch': 0.44}
{'loss': 0.6303, 'grad_norm': 0.37971851229667664, 'learning_rate': 0.00012396442762666128, 'epoch': 0.44}
{'loss': 0.6798, 'grad_norm': 0.2320977747440338, 'learning_rate': 0.00012375078093697063, 'epoch': 0.44}
{'loss': 0.5682, 'grad_norm': 0.4292546212673187, 'learning_rate': 0.00012353701929408427, 'epoch': 0.44}
{'loss': 0.7232, 'grad_norm': 0.4010060727596283, 'learning_rate': 0.00012332314373260326, 'epoch': 0.44}
{'loss': 0.8487, 'grad_norm': 0.31467652320861816, 'learning_rate': 0.00012310915528768, 'epoch': 0.44}
{'loss': 0.7234, 'grad_norm': 0.29340699315071106, 'learning_rate': 0.0001228950549950134, 'epoch': 0.44}
{'loss': 0.5903, 'grad_norm': 0.2112685590982437, 'learning_rate': 0.0001226808438908436, 'epoch': 0.45}
{'loss': 0.6985, 'grad_norm': 0.4501890540122986, 'learning_rate': 0.00012246652301194701, 'epoch': 0.45}
{'loss': 0.6251, 'grad_norm': 0.3007189631462097, 'learning_rate': 0.00012225209339563145, 'epoch': 0.45}
{'loss': 0.6281, 'grad_norm': 0.261309415102005, 'learning_rate': 0.00012203755607973102, 'epoch': 0.45}
{'loss': 0.618, 'grad_norm': 0.20611752569675446, 'learning_rate': 0.00012182291210260097, 'epoch': 0.45}
{'loss': 0.5824, 'grad_norm': 0.28690990805625916, 'learning_rate': 0.00012160816250311298, 'epoch': 0.45}
{'loss': 0.7051, 'grad_norm': 0.39812007546424866, 'learning_rate': 0.00012139330832064974, 'epoch': 0.45}
{'loss': 0.6711, 'grad_norm': 0.2512768805027008, 'learning_rate': 0.00012117835059510027, 'epoch': 0.45}
{'loss': 0.6848, 'grad_norm': 0.3605792224407196, 'learning_rate': 0.00012096329036685468, 'epoch': 0.45}
{'loss': 0.6408, 'grad_norm': 0.4742375612258911, 'learning_rate': 0.00012074812867679912, 'epoch': 0.45}
{'loss': 0.6292, 'grad_norm': 0.22310319542884827, 'learning_rate': 0.00012053286656631093, 'epoch': 0.45}
{'loss': 0.6681, 'grad_norm': 0.27847594022750854, 'learning_rate': 0.00012031750507725344, 'epoch': 0.45}
{'loss': 0.6877, 'grad_norm': 0.33081069588661194, 'learning_rate': 0.00012010204525197094, 'epoch': 0.45}
{'loss': 0.7065, 'grad_norm': 0.34148868918418884, 'learning_rate': 0.00011988648813328367, 'epoch': 0.45}
{'loss': 0.8484, 'grad_norm': 0.2798655927181244, 'learning_rate': 0.00011967083476448282, 'epoch': 0.46}
{'loss': 0.736, 'grad_norm': 0.3049849271774292, 'learning_rate': 0.00011945508618932537, 'epoch': 0.46}
{'loss': 0.6877, 'grad_norm': 0.2066255360841751, 'learning_rate': 0.00011923924345202915, 'epoch': 0.46}
{'loss': 0.6819, 'grad_norm': 0.3881027400493622, 'learning_rate': 0.00011902330759726765, 'epoch': 0.46}
{'loss': 0.6308, 'grad_norm': 0.4043647348880768, 'learning_rate': 0.00011880727967016514, 'epoch': 0.46}
{'loss': 0.6921, 'grad_norm': 0.33979207277297974, 'learning_rate': 0.00011859116071629149, 'epoch': 0.46}
{'loss': 0.608, 'grad_norm': 0.31589770317077637, 'learning_rate': 0.00011837495178165706, 'epoch': 0.46}
{'loss': 0.7853, 'grad_norm': 0.2992321252822876, 'learning_rate': 0.0001181586539127078, 'epoch': 0.46}
{'loss': 0.4982, 'grad_norm': 0.24061381816864014, 'learning_rate': 0.00011794226815632012, 'epoch': 0.46}
{'loss': 0.6525, 'grad_norm': 0.453396201133728, 'learning_rate': 0.00011772579555979572, 'epoch': 0.46}
{'loss': 0.7409, 'grad_norm': 0.2925848364830017, 'learning_rate': 0.00011750923717085662, 'epoch': 0.46}
{'loss': 0.7818, 'grad_norm': 0.5413249135017395, 'learning_rate': 0.00011729259403764016, 'epoch': 0.46}
{'loss': 0.5489, 'grad_norm': 0.25544077157974243, 'learning_rate': 0.00011707586720869374, 'epoch': 0.46}
{'loss': 0.7065, 'grad_norm': 0.23277658224105835, 'learning_rate': 0.00011685905773296992, 'epoch': 0.46}
{'loss': 0.6187, 'grad_norm': 0.30800268054008484, 'learning_rate': 0.0001166421666598212, 'epoch': 0.46}
{'loss': 0.6908, 'grad_norm': 0.2993369400501251, 'learning_rate': 0.000116425195038995, 'epoch': 0.47}
{'loss': 0.6465, 'grad_norm': 0.40710386633872986, 'learning_rate': 0.00011620814392062873, 'epoch': 0.47}
{'loss': 0.6604, 'grad_norm': 0.45317140221595764, 'learning_rate': 0.0001159910143552444, 'epoch': 0.47}
{'loss': 0.748, 'grad_norm': 0.3278229236602783, 'learning_rate': 0.00011577380739374375, 'epoch': 0.47}
{'loss': 0.893, 'grad_norm': 0.35227128863334656, 'learning_rate': 0.00011555652408740318, 'epoch': 0.47}
{'loss': 0.6531, 'grad_norm': 0.475795716047287, 'learning_rate': 0.00011533916548786857, 'epoch': 0.47}
{'loss': 0.6955, 'grad_norm': 0.2504597008228302, 'learning_rate': 0.00011512173264715011, 'epoch': 0.47}
{'loss': 0.543, 'grad_norm': 0.2705357074737549, 'learning_rate': 0.00011490422661761744, 'epoch': 0.47}
{'loss': 0.7971, 'grad_norm': 0.28326883912086487, 'learning_rate': 0.00011468664845199444, 'epoch': 0.47}
{'loss': 0.7421, 'grad_norm': 0.2575732469558716, 'learning_rate': 0.00011446899920335405, 'epoch': 0.47}
{'loss': 0.752, 'grad_norm': 0.28495433926582336, 'learning_rate': 0.00011425127992511327, 'epoch': 0.47}
{'loss': 0.7284, 'grad_norm': 0.2909505367279053, 'learning_rate': 0.00011403349167102805, 'epoch': 0.47}
{'loss': 0.7469, 'grad_norm': 0.3538399040699005, 'learning_rate': 0.00011381563549518823, 'epoch': 0.47}
{'loss': 0.6063, 'grad_norm': 0.18569082021713257, 'learning_rate': 0.00011359771245201232, 'epoch': 0.47}
{'loss': 0.5952, 'grad_norm': 0.332339346408844, 'learning_rate': 0.00011337972359624243, 'epoch': 0.47}
{'loss': 0.6926, 'grad_norm': 0.2438836246728897, 'learning_rate': 0.00011316166998293935, 'epoch': 0.48}
{'loss': 0.7665, 'grad_norm': 0.5483644008636475, 'learning_rate': 0.00011294355266747718, 'epoch': 0.48}
{'loss': 0.7453, 'grad_norm': 0.35179615020751953, 'learning_rate': 0.00011272537270553836, 'epoch': 0.48}
{'loss': 0.7308, 'grad_norm': 0.37660402059555054, 'learning_rate': 0.00011250713115310851, 'epoch': 0.48}
{'loss': 0.852, 'grad_norm': 0.6145078539848328, 'learning_rate': 0.00011228882906647142, 'epoch': 0.48}
{'loss': 0.6783, 'grad_norm': 0.3697352111339569, 'learning_rate': 0.00011207046750220381, 'epoch': 0.48}
{'loss': 0.804, 'grad_norm': 0.31659212708473206, 'learning_rate': 0.00011185204751717029, 'epoch': 0.48}
{'loss': 0.5826, 'grad_norm': 0.2824128568172455, 'learning_rate': 0.00011163357016851816, 'epoch': 0.48}
{'loss': 0.6719, 'grad_norm': 0.21150517463684082, 'learning_rate': 0.00011141503651367251, 'epoch': 0.48}
{'loss': 0.7853, 'grad_norm': 0.33277273178100586, 'learning_rate': 0.00011119644761033078, 'epoch': 0.48}
{'loss': 0.8273, 'grad_norm': 0.41635337471961975, 'learning_rate': 0.00011097780451645792, 'epoch': 0.48}
{'loss': 0.671, 'grad_norm': 0.28776028752326965, 'learning_rate': 0.00011075910829028115, 'epoch': 0.48}
{'loss': 0.5823, 'grad_norm': 0.24498234689235687, 'learning_rate': 0.00011054035999028478, 'epoch': 0.48}
{'loss': 0.6244, 'grad_norm': 0.41402631998062134, 'learning_rate': 0.00011032156067520525, 'epoch': 0.48}
{'loss': 0.5691, 'grad_norm': 0.47263985872268677, 'learning_rate': 0.00011010271140402579, 'epoch': 0.49}
{'loss': 0.5701, 'grad_norm': 0.34642288088798523, 'learning_rate': 0.00010988381323597157, 'epoch': 0.49}
{'loss': 0.6221, 'grad_norm': 0.31695055961608887, 'learning_rate': 0.00010966486723050431, 'epoch': 0.49}
{'loss': 0.6674, 'grad_norm': 0.4287826716899872, 'learning_rate': 0.00010944587444731731, 'epoch': 0.49}
{'loss': 0.5496, 'grad_norm': 0.19424235820770264, 'learning_rate': 0.00010922683594633021, 'epoch': 0.49}
{'loss': 0.5939, 'grad_norm': 0.21302887797355652, 'learning_rate': 0.00010900775278768403, 'epoch': 0.49}
{'loss': 0.7136, 'grad_norm': 0.41375210881233215, 'learning_rate': 0.00010878862603173583, 'epoch': 0.49}
{'loss': 0.6821, 'grad_norm': 0.285858154296875, 'learning_rate': 0.00010856945673905369, 'epoch': 0.49}
{'loss': 0.7526, 'grad_norm': 0.19794197380542755, 'learning_rate': 0.0001083502459704117, 'epoch': 0.49}
{'loss': 0.4825, 'grad_norm': 0.4118849039077759, 'learning_rate': 0.0001081309947867845, 'epoch': 0.49}
{'loss': 0.8248, 'grad_norm': 0.4009796679019928, 'learning_rate': 0.00010791170424934247, 'epoch': 0.49}
{'loss': 0.5659, 'grad_norm': 0.24627593159675598, 'learning_rate': 0.0001076923754194464, 'epoch': 0.49}
{'loss': 0.6314, 'grad_norm': 0.2576512396335602, 'learning_rate': 0.00010747300935864243, 'epoch': 0.49}
{'loss': 0.6829, 'grad_norm': 0.25862985849380493, 'learning_rate': 0.00010725360712865693, 'epoch': 0.49}
{'loss': 0.6679, 'grad_norm': 0.31728431582450867, 'learning_rate': 0.00010703416979139127, 'epoch': 0.49}
{'loss': 0.6705, 'grad_norm': 0.24799104034900665, 'learning_rate': 0.00010681469840891676, 'epoch': 0.5}
{'loss': 0.7324, 'grad_norm': 0.34399792551994324, 'learning_rate': 0.00010659519404346954, 'epoch': 0.5}
{'loss': 0.7007, 'grad_norm': 0.429658442735672, 'learning_rate': 0.00010637565775744531, 'epoch': 0.5}
{'loss': 0.7592, 'grad_norm': 0.4574047029018402, 'learning_rate': 0.00010615609061339432, 'epoch': 0.5}
{'loss': 0.5723, 'grad_norm': 0.2948324680328369, 'learning_rate': 0.00010593649367401605, 'epoch': 0.5}
{'loss': 0.5374, 'grad_norm': 0.19189554452896118, 'learning_rate': 0.00010571686800215444, 'epoch': 0.5}
{'loss': 0.7128, 'grad_norm': 0.33642417192459106, 'learning_rate': 0.00010549721466079226, 'epoch': 0.5}
{'loss': 0.7958, 'grad_norm': 0.4335666596889496, 'learning_rate': 0.00010527753471304625, 'epoch': 0.5}
{'loss': 0.6046, 'grad_norm': 0.36325255036354065, 'learning_rate': 0.00010505782922216201, 'epoch': 0.5}
{'loss': 0.5872, 'grad_norm': 0.2268161177635193, 'learning_rate': 0.00010483809925150869, 'epoch': 0.5}
{'loss': 0.6362, 'grad_norm': 0.26907992362976074, 'learning_rate': 0.00010461834586457398, 'epoch': 0.5}
{'loss': 0.5931, 'grad_norm': 0.5617568492889404, 'learning_rate': 0.00010439857012495881, 'epoch': 0.5}
{'loss': 0.7919, 'grad_norm': 0.32216548919677734, 'learning_rate': 0.0001041787730963724, 'epoch': 0.5}
{'loss': 0.5955, 'grad_norm': 0.24797560274600983, 'learning_rate': 0.00010395895584262696, 'epoch': 0.5}
{'loss': 0.5794, 'grad_norm': 0.22019153833389282, 'learning_rate': 0.0001037391194276326, 'epoch': 0.5}
{'loss': 0.6637, 'grad_norm': 0.36809948086738586, 'learning_rate': 0.0001035192649153921, 'epoch': 0.51}
{'loss': 0.7296, 'grad_norm': 0.3197386860847473, 'learning_rate': 0.00010329939336999596, 'epoch': 0.51}
{'loss': 0.5544, 'grad_norm': 0.2068960964679718, 'learning_rate': 0.00010307950585561706, 'epoch': 0.51}
{'loss': 0.6919, 'grad_norm': 0.20181645452976227, 'learning_rate': 0.00010285960343650552, 'epoch': 0.51}
{'loss': 0.7378, 'grad_norm': 0.24083268642425537, 'learning_rate': 0.00010263968717698364, 'epoch': 0.51}
{'loss': 0.6371, 'grad_norm': 0.20741520822048187, 'learning_rate': 0.00010241975814144074, 'epoch': 0.51}
{'loss': 0.8122, 'grad_norm': 0.3155517280101776, 'learning_rate': 0.00010219981739432795, 'epoch': 0.51}
{'loss': 0.6792, 'grad_norm': 0.26315128803253174, 'learning_rate': 0.00010197986600015305, 'epoch': 0.51}
{'loss': 0.6455, 'grad_norm': 0.27138468623161316, 'learning_rate': 0.00010175990502347539, 'epoch': 0.51}
{'loss': 0.6904, 'grad_norm': 0.3487459421157837, 'learning_rate': 0.00010153993552890069, 'epoch': 0.51}
{'loss': 0.7607, 'grad_norm': 0.39614012837409973, 'learning_rate': 0.00010131995858107591, 'epoch': 0.51}
{'loss': 0.7559, 'grad_norm': 0.2622326910495758, 'learning_rate': 0.00010109997524468402, 'epoch': 0.51}
{'loss': 0.6943, 'grad_norm': 0.19758540391921997, 'learning_rate': 0.000100879986584439, 'epoch': 0.51}
{'loss': 0.6234, 'grad_norm': 0.30419158935546875, 'learning_rate': 0.00010065999366508057, 'epoch': 0.51}
{'loss': 0.7786, 'grad_norm': 0.19655972719192505, 'learning_rate': 0.00010043999755136904, 'epoch': 0.51}
{'loss': 0.685, 'grad_norm': 0.23453421890735626, 'learning_rate': 0.00010021999930808013, 'epoch': 0.52}
{'loss': 0.6668, 'grad_norm': 0.30062586069107056, 'learning_rate': 0.0001, 'epoch': 0.52}
{'loss': 0.6221, 'grad_norm': 0.49637871980667114, 'learning_rate': 9.978000069191989e-05, 'epoch': 0.52}
{'loss': 0.6504, 'grad_norm': 0.32010242342948914, 'learning_rate': 9.9560002448631e-05, 'epoch': 0.52}
{'loss': 0.6717, 'grad_norm': 0.39869147539138794, 'learning_rate': 9.934000633491944e-05, 'epoch': 0.52}
{'loss': 0.7584, 'grad_norm': 0.5658974051475525, 'learning_rate': 9.9120013415561e-05, 'epoch': 0.52}
{'loss': 0.7084, 'grad_norm': 0.319140762090683, 'learning_rate': 9.890002475531601e-05, 'epoch': 0.52}
{'loss': 0.7649, 'grad_norm': 0.3275013566017151, 'learning_rate': 9.868004141892411e-05, 'epoch': 0.52}
{'loss': 0.5423, 'grad_norm': 0.25009605288505554, 'learning_rate': 9.846006447109933e-05, 'epoch': 0.52}
{'loss': 0.7455, 'grad_norm': 0.3686683773994446, 'learning_rate': 9.824009497652465e-05, 'epoch': 0.52}
{'loss': 0.7102, 'grad_norm': 0.2932814955711365, 'learning_rate': 9.802013399984696e-05, 'epoch': 0.52}
{'loss': 0.6349, 'grad_norm': 0.346384197473526, 'learning_rate': 9.780018260567207e-05, 'epoch': 0.52}
{'loss': 0.676, 'grad_norm': 0.24005945026874542, 'learning_rate': 9.758024185855929e-05, 'epoch': 0.52}
{'loss': 0.6323, 'grad_norm': 0.3538743257522583, 'learning_rate': 9.73603128230164e-05, 'epoch': 0.52}
{'loss': 0.7674, 'grad_norm': 0.30615195631980896, 'learning_rate': 9.714039656349452e-05, 'epoch': 0.53}
{'loss': 0.6571, 'grad_norm': 0.248757466673851, 'learning_rate': 9.692049414438299e-05, 'epoch': 0.53}
{'loss': 0.6084, 'grad_norm': 0.2812042534351349, 'learning_rate': 9.670060663000408e-05, 'epoch': 0.53}
{'loss': 0.6404, 'grad_norm': 0.34023842215538025, 'learning_rate': 9.648073508460794e-05, 'epoch': 0.53}
{'loss': 0.7474, 'grad_norm': 0.35827741026878357, 'learning_rate': 9.626088057236745e-05, 'epoch': 0.53}
{'loss': 0.6392, 'grad_norm': 0.27774521708488464, 'learning_rate': 9.604104415737308e-05, 'epoch': 0.53}
{'loss': 0.7076, 'grad_norm': 0.4018293619155884, 'learning_rate': 9.582122690362763e-05, 'epoch': 0.53}
{'loss': 0.598, 'grad_norm': 0.2625916302204132, 'learning_rate': 9.560142987504121e-05, 'epoch': 0.53}
{'loss': 0.7403, 'grad_norm': 0.22142575681209564, 'learning_rate': 9.538165413542607e-05, 'epoch': 0.53}
{'loss': 0.6163, 'grad_norm': 0.29706424474716187, 'learning_rate': 9.516190074849134e-05, 'epoch': 0.53}
{'loss': 0.7865, 'grad_norm': 0.21580588817596436, 'learning_rate': 9.4942170777838e-05, 'epoch': 0.53}
{'loss': 0.5384, 'grad_norm': 0.35509181022644043, 'learning_rate': 9.472246528695376e-05, 'epoch': 0.53}
{'loss': 0.6846, 'grad_norm': 0.21166691184043884, 'learning_rate': 9.450278533920775e-05, 'epoch': 0.53}
{'loss': 0.7074, 'grad_norm': 0.31970396637916565, 'learning_rate': 9.428313199784556e-05, 'epoch': 0.53}
{'loss': 0.7787, 'grad_norm': 0.270204097032547, 'learning_rate': 9.406350632598393e-05, 'epoch': 0.53}
{'loss': 0.83, 'grad_norm': 0.32513827085494995, 'learning_rate': 9.384390938660572e-05, 'epoch': 0.54}
{'loss': 0.5892, 'grad_norm': 0.21086962521076202, 'learning_rate': 9.362434224255469e-05, 'epoch': 0.54}
{'loss': 0.6842, 'grad_norm': 0.28308677673339844, 'learning_rate': 9.340480595653047e-05, 'epoch': 0.54}
{'loss': 0.7022, 'grad_norm': 0.3386445641517639, 'learning_rate': 9.318530159108323e-05, 'epoch': 0.54}
{'loss': 0.6843, 'grad_norm': 0.254109263420105, 'learning_rate': 9.296583020860876e-05, 'epoch': 0.54}
{'loss': 0.6375, 'grad_norm': 0.1551855504512787, 'learning_rate': 9.274639287134308e-05, 'epoch': 0.54}
{'loss': 0.7515, 'grad_norm': 0.2887408137321472, 'learning_rate': 9.252699064135758e-05, 'epoch': 0.54}
{'loss': 0.5262, 'grad_norm': 0.2230641096830368, 'learning_rate': 9.230762458055363e-05, 'epoch': 0.54}
{'loss': 0.5794, 'grad_norm': 0.481362909078598, 'learning_rate': 9.208829575065754e-05, 'epoch': 0.54}
{'loss': 0.6915, 'grad_norm': 0.2822359502315521, 'learning_rate': 9.18690052132155e-05, 'epoch': 0.54}
{'loss': 0.6741, 'grad_norm': 0.39865875244140625, 'learning_rate': 9.164975402958834e-05, 'epoch': 0.54}
{'loss': 0.7158, 'grad_norm': 0.2900437116622925, 'learning_rate': 9.143054326094632e-05, 'epoch': 0.54}
{'loss': 0.5264, 'grad_norm': 0.24308134615421295, 'learning_rate': 9.12113739682642e-05, 'epoch': 0.54}
{'loss': 0.7054, 'grad_norm': 0.30585405230522156, 'learning_rate': 9.0992247212316e-05, 'epoch': 0.54}
{'loss': 0.6672, 'grad_norm': 0.35129472613334656, 'learning_rate': 9.077316405366981e-05, 'epoch': 0.54}
{'loss': 0.7216, 'grad_norm': 0.3592328131198883, 'learning_rate': 9.055412555268271e-05, 'epoch': 0.55}
{'loss': 0.645, 'grad_norm': 0.2890411913394928, 'learning_rate': 9.03351327694957e-05, 'epoch': 0.55}
{'loss': 0.6678, 'grad_norm': 0.3179076611995697, 'learning_rate': 9.011618676402845e-05, 'epoch': 0.55}
{'loss': 0.7102, 'grad_norm': 0.2389884740114212, 'learning_rate': 8.989728859597424e-05, 'epoch': 0.55}
{'loss': 0.7078, 'grad_norm': 0.33236196637153625, 'learning_rate': 8.967843932479478e-05, 'epoch': 0.55}
{'loss': 0.5289, 'grad_norm': 0.24012920260429382, 'learning_rate': 8.945964000971524e-05, 'epoch': 0.55}
{'loss': 0.6425, 'grad_norm': 0.2564663887023926, 'learning_rate': 8.924089170971887e-05, 'epoch': 0.55}
{'loss': 0.7052, 'grad_norm': 0.38375717401504517, 'learning_rate': 8.902219548354209e-05, 'epoch': 0.55}
{'loss': 0.6429, 'grad_norm': 0.4397491216659546, 'learning_rate': 8.880355238966923e-05, 'epoch': 0.55}
{'loss': 0.6133, 'grad_norm': 0.2766092121601105, 'learning_rate': 8.858496348632751e-05, 'epoch': 0.55}
{'loss': 0.7092, 'grad_norm': 0.3394783139228821, 'learning_rate': 8.836642983148185e-05, 'epoch': 0.55}
{'loss': 0.6345, 'grad_norm': 0.34540629386901855, 'learning_rate': 8.814795248282974e-05, 'epoch': 0.55}
{'loss': 0.6802, 'grad_norm': 0.23958389461040497, 'learning_rate': 8.792953249779621e-05, 'epoch': 0.55}
{'loss': 0.7368, 'grad_norm': 0.3278590440750122, 'learning_rate': 8.77111709335286e-05, 'epoch': 0.55}
{'loss': 0.6681, 'grad_norm': 0.30540984869003296, 'learning_rate': 8.749286884689152e-05, 'epoch': 0.55}
{'loss': 0.6578, 'grad_norm': 0.25419095158576965, 'learning_rate': 8.727462729446167e-05, 'epoch': 0.56}
{'loss': 0.6008, 'grad_norm': 0.4424987733364105, 'learning_rate': 8.705644733252285e-05, 'epoch': 0.56}
{'loss': 0.7122, 'grad_norm': 0.2601652145385742, 'learning_rate': 8.683833001706067e-05, 'epoch': 0.56}
{'loss': 0.7164, 'grad_norm': 0.17683100700378418, 'learning_rate': 8.662027640375758e-05, 'epoch': 0.56}
{'loss': 0.652, 'grad_norm': 0.1902625858783722, 'learning_rate': 8.640228754798773e-05, 'epoch': 0.56}
{'loss': 0.6374, 'grad_norm': 0.23901279270648956, 'learning_rate': 8.61843645048118e-05, 'epoch': 0.56}
{'loss': 0.5349, 'grad_norm': 0.23671913146972656, 'learning_rate': 8.596650832897197e-05, 'epoch': 0.56}
{'loss': 0.621, 'grad_norm': 0.24784904718399048, 'learning_rate': 8.574872007488675e-05, 'epoch': 0.56}
{'loss': 0.8258, 'grad_norm': 0.3474028408527374, 'learning_rate': 8.553100079664598e-05, 'epoch': 0.56}
{'loss': 0.79, 'grad_norm': 0.3329821825027466, 'learning_rate': 8.531335154800555e-05, 'epoch': 0.56}
{'loss': 0.6744, 'grad_norm': 0.32806485891342163, 'learning_rate': 8.509577338238255e-05, 'epoch': 0.56}
{'loss': 0.646, 'grad_norm': 0.32489126920700073, 'learning_rate': 8.487826735284991e-05, 'epoch': 0.56}
{'loss': 0.6129, 'grad_norm': 0.2488836795091629, 'learning_rate': 8.466083451213144e-05, 'epoch': 0.56}
{'loss': 0.6298, 'grad_norm': 0.20749376714229584, 'learning_rate': 8.444347591259681e-05, 'epoch': 0.56}
{'loss': 0.5229, 'grad_norm': 0.29001858830451965, 'learning_rate': 8.422619260625625e-05, 'epoch': 0.57}
{'loss': 0.6282, 'grad_norm': 0.3023492991924286, 'learning_rate': 8.400898564475562e-05, 'epoch': 0.57}
{'loss': 0.7175, 'grad_norm': 0.34363633394241333, 'learning_rate': 8.379185607937126e-05, 'epoch': 0.57}
{'loss': 0.6916, 'grad_norm': 0.4267646074295044, 'learning_rate': 8.357480496100498e-05, 'epoch': 0.57}
{'loss': 0.6014, 'grad_norm': 0.29156962037086487, 'learning_rate': 8.335783334017883e-05, 'epoch': 0.57}
{'loss': 0.7312, 'grad_norm': 0.27594634890556335, 'learning_rate': 8.314094226703007e-05, 'epoch': 0.57}
{'loss': 0.6981, 'grad_norm': 0.3461724817752838, 'learning_rate': 8.292413279130624e-05, 'epoch': 0.57}
{'loss': 0.6784, 'grad_norm': 0.3246540129184723, 'learning_rate': 8.270740596235984e-05, 'epoch': 0.57}
{'loss': 0.7595, 'grad_norm': 0.3145531713962555, 'learning_rate': 8.249076282914339e-05, 'epoch': 0.57}
{'loss': 0.542, 'grad_norm': 0.24094679951667786, 'learning_rate': 8.22742044402043e-05, 'epoch': 0.57}
{'loss': 0.665, 'grad_norm': 0.34454479813575745, 'learning_rate': 8.205773184367991e-05, 'epoch': 0.57}
{'loss': 0.6386, 'grad_norm': 0.30969738960266113, 'learning_rate': 8.184134608729222e-05, 'epoch': 0.57}
{'loss': 0.701, 'grad_norm': 0.3192804455757141, 'learning_rate': 8.162504821834295e-05, 'epoch': 0.57}
{'loss': 0.5082, 'grad_norm': 0.21935394406318665, 'learning_rate': 8.140883928370855e-05, 'epoch': 0.57}
{'loss': 0.8364, 'grad_norm': 0.27411526441574097, 'learning_rate': 8.119272032983487e-05, 'epoch': 0.57}
{'loss': 0.7051, 'grad_norm': 0.34158703684806824, 'learning_rate': 8.097669240273236e-05, 'epoch': 0.58}
{'loss': 0.5511, 'grad_norm': 0.3301103115081787, 'learning_rate': 8.076075654797086e-05, 'epoch': 0.58}
{'loss': 0.7125, 'grad_norm': 0.35366806387901306, 'learning_rate': 8.054491381067464e-05, 'epoch': 0.58}
{'loss': 0.5902, 'grad_norm': 0.28388532996177673, 'learning_rate': 8.03291652355172e-05, 'epoch': 0.58}
{'loss': 0.6873, 'grad_norm': 0.23918963968753815, 'learning_rate': 8.011351186671637e-05, 'epoch': 0.58}
{'loss': 0.7311, 'grad_norm': 0.35109877586364746, 'learning_rate': 7.989795474802909e-05, 'epoch': 0.58}
{'loss': 0.7078, 'grad_norm': 0.2309390902519226, 'learning_rate': 7.96824949227466e-05, 'epoch': 0.58}
{'loss': 0.5603, 'grad_norm': 0.31684410572052, 'learning_rate': 7.94671334336891e-05, 'epoch': 0.58}
{'loss': 0.6973, 'grad_norm': 0.348480224609375, 'learning_rate': 7.92518713232009e-05, 'epoch': 0.58}
{'loss': 0.651, 'grad_norm': 0.248671293258667, 'learning_rate': 7.903670963314536e-05, 'epoch': 0.58}
{'loss': 0.6943, 'grad_norm': 0.3231421411037445, 'learning_rate': 7.882164940489974e-05, 'epoch': 0.58}
{'loss': 0.6691, 'grad_norm': 0.2538028955459595, 'learning_rate': 7.860669167935028e-05, 'epoch': 0.58}
{'loss': 0.6274, 'grad_norm': 0.2598460912704468, 'learning_rate': 7.839183749688704e-05, 'epoch': 0.58}
{'loss': 0.6748, 'grad_norm': 0.2831476628780365, 'learning_rate': 7.817708789739904e-05, 'epoch': 0.58}
{'loss': 0.6987, 'grad_norm': 0.2645479440689087, 'learning_rate': 7.796244392026903e-05, 'epoch': 0.58}
{'loss': 0.7047, 'grad_norm': 0.4873344898223877, 'learning_rate': 7.774790660436858e-05, 'epoch': 0.59}
{'loss': 0.6521, 'grad_norm': 0.2155776470899582, 'learning_rate': 7.753347698805302e-05, 'epoch': 0.59}
{'loss': 0.7263, 'grad_norm': 0.30654630064964294, 'learning_rate': 7.731915610915644e-05, 'epoch': 0.59}
{'loss': 0.6189, 'grad_norm': 0.18495896458625793, 'learning_rate': 7.710494500498662e-05, 'epoch': 0.59}
{'loss': 0.8051, 'grad_norm': 0.3026930093765259, 'learning_rate': 7.689084471232001e-05, 'epoch': 0.59}
{'loss': 0.6412, 'grad_norm': 0.318343847990036, 'learning_rate': 7.66768562673968e-05, 'epoch': 0.59}
{'loss': 0.6073, 'grad_norm': 0.3253355026245117, 'learning_rate': 7.646298070591578e-05, 'epoch': 0.59}
{'loss': 0.5791, 'grad_norm': 0.18343676626682281, 'learning_rate': 7.62492190630294e-05, 'epoch': 0.59}
{'loss': 0.6152, 'grad_norm': 0.25451433658599854, 'learning_rate': 7.603557237333877e-05, 'epoch': 0.59}
{'loss': 0.7546, 'grad_norm': 0.39844486117362976, 'learning_rate': 7.582204167088864e-05, 'epoch': 0.59}
{'loss': 0.6823, 'grad_norm': 0.31447330117225647, 'learning_rate': 7.560862798916228e-05, 'epoch': 0.59}
{'loss': 0.6826, 'grad_norm': 0.2807876765727997, 'learning_rate': 7.539533236107675e-05, 'epoch': 0.59}
{'loss': 0.5839, 'grad_norm': 0.32491010427474976, 'learning_rate': 7.518215581897763e-05, 'epoch': 0.59}
{'loss': 0.7431, 'grad_norm': 0.4276570677757263, 'learning_rate': 7.496909939463408e-05, 'epoch': 0.59}
{'loss': 0.5443, 'grad_norm': 0.3042088449001312, 'learning_rate': 7.475616411923403e-05, 'epoch': 0.6}
{'loss': 0.6467, 'grad_norm': 0.2134697139263153, 'learning_rate': 7.454335102337895e-05, 'epoch': 0.6}
{'loss': 0.7027, 'grad_norm': 0.40437284111976624, 'learning_rate': 7.433066113707896e-05, 'epoch': 0.6}
{'loss': 0.7157, 'grad_norm': 0.30702394247055054, 'learning_rate': 7.411809548974792e-05, 'epoch': 0.6}
{'loss': 0.6973, 'grad_norm': 0.3913426399230957, 'learning_rate': 7.390565511019834e-05, 'epoch': 0.6}
{'loss': 0.6219, 'grad_norm': 0.25993385910987854, 'learning_rate': 7.36933410266364e-05, 'epoch': 0.6}
{'loss': 0.7062, 'grad_norm': 0.2563909888267517, 'learning_rate': 7.348115426665705e-05, 'epoch': 0.6}
{'loss': 0.7081, 'grad_norm': 0.30705025792121887, 'learning_rate': 7.326909585723901e-05, 'epoch': 0.6}
{'loss': 0.7837, 'grad_norm': 0.2925786077976227, 'learning_rate': 7.305716682473977e-05, 'epoch': 0.6}
{'loss': 0.6009, 'grad_norm': 0.4058259129524231, 'learning_rate': 7.28453681948906e-05, 'epoch': 0.6}
{'loss': 0.668, 'grad_norm': 0.35185670852661133, 'learning_rate': 7.263370099279172e-05, 'epoch': 0.6}
{'loss': 0.6392, 'grad_norm': 0.2653786242008209, 'learning_rate': 7.242216624290714e-05, 'epoch': 0.6}
{'loss': 0.791, 'grad_norm': 0.23883876204490662, 'learning_rate': 7.221076496905985e-05, 'epoch': 0.6}
{'loss': 0.6311, 'grad_norm': 0.26828211545944214, 'learning_rate': 7.199949819442682e-05, 'epoch': 0.6}
{'loss': 0.6114, 'grad_norm': 0.2581886649131775, 'learning_rate': 7.178836694153405e-05, 'epoch': 0.6}
{'loss': 0.7215, 'grad_norm': 0.26236438751220703, 'learning_rate': 7.157737223225164e-05, 'epoch': 0.61}
{'loss': 0.6898, 'grad_norm': 0.24166499078273773, 'learning_rate': 7.136651508778875e-05, 'epoch': 0.61}
{'loss': 0.7794, 'grad_norm': 0.3919655978679657, 'learning_rate': 7.115579652868878e-05, 'epoch': 0.61}
{'loss': 0.607, 'grad_norm': 0.41359686851501465, 'learning_rate': 7.09452175748244e-05, 'epoch': 0.61}
{'loss': 0.7549, 'grad_norm': 0.3444315195083618, 'learning_rate': 7.073477924539255e-05, 'epoch': 0.61}
{'loss': 0.7371, 'grad_norm': 0.3492349088191986, 'learning_rate': 7.052448255890957e-05, 'epoch': 0.61}
{'loss': 0.6845, 'grad_norm': 0.21072529256343842, 'learning_rate': 7.031432853320629e-05, 'epoch': 0.61}
{'loss': 0.6063, 'grad_norm': 0.23461663722991943, 'learning_rate': 7.010431818542297e-05, 'epoch': 0.61}
{'loss': 0.7016, 'grad_norm': 0.23974986374378204, 'learning_rate': 6.989445253200458e-05, 'epoch': 0.61}
{'loss': 0.6726, 'grad_norm': 0.20439940690994263, 'learning_rate': 6.968473258869566e-05, 'epoch': 0.61}
{'loss': 0.7046, 'grad_norm': 0.35377994179725647, 'learning_rate': 6.947515937053563e-05, 'epoch': 0.61}
{'loss': 0.682, 'grad_norm': 0.48855653405189514, 'learning_rate': 6.926573389185371e-05, 'epoch': 0.61}
{'loss': 0.6918, 'grad_norm': 0.3576665222644806, 'learning_rate': 6.905645716626404e-05, 'epoch': 0.61}
{'loss': 0.6995, 'grad_norm': 0.5046775341033936, 'learning_rate': 6.884733020666086e-05, 'epoch': 0.61}
{'loss': 0.6204, 'grad_norm': 0.4571278989315033, 'learning_rate': 6.86383540252135e-05, 'epoch': 0.61}
{'loss': 0.5457, 'grad_norm': 0.7120900750160217, 'learning_rate': 6.842952963336153e-05, 'epoch': 0.62}
{'loss': 0.77, 'grad_norm': 0.31302502751350403, 'learning_rate': 6.822085804180984e-05, 'epoch': 0.62}
{'loss': 0.6685, 'grad_norm': 0.4688524901866913, 'learning_rate': 6.801234026052388e-05, 'epoch': 0.62}
{'loss': 0.6915, 'grad_norm': 0.27657976746559143, 'learning_rate': 6.780397729872455e-05, 'epoch': 0.62}
{'loss': 0.6461, 'grad_norm': 0.3137328028678894, 'learning_rate': 6.759577016488343e-05, 'epoch': 0.62}
{'loss': 0.546, 'grad_norm': 0.19855733215808868, 'learning_rate': 6.738771986671796e-05, 'epoch': 0.62}
{'loss': 0.6169, 'grad_norm': 0.2218332588672638, 'learning_rate': 6.717982741118647e-05, 'epoch': 0.62}
{'loss': 0.851, 'grad_norm': 0.41562020778656006, 'learning_rate': 6.697209380448333e-05, 'epoch': 0.62}
{'loss': 0.7027, 'grad_norm': 0.2724345922470093, 'learning_rate': 6.676452005203406e-05, 'epoch': 0.62}
{'loss': 0.6415, 'grad_norm': 0.33234503865242004, 'learning_rate': 6.655710715849056e-05, 'epoch': 0.62}
{'loss': 0.7654, 'grad_norm': 0.27952349185943604, 'learning_rate': 6.634985612772611e-05, 'epoch': 0.62}
{'loss': 0.7097, 'grad_norm': 0.45615819096565247, 'learning_rate': 6.614276796283057e-05, 'epoch': 0.62}
{'loss': 0.7125, 'grad_norm': 0.21510355174541473, 'learning_rate': 6.593584366610566e-05, 'epoch': 0.62}
[2026-02-10 00:36:32,296] [WARNING] [stage3.py:2274:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.6315, 'grad_norm': 0.35542601346969604, 'learning_rate': 6.572908423905979e-05, 'epoch': 0.62}
{'loss': 0.6664, 'grad_norm': 0.3884872496128082, 'learning_rate': 6.552249068240358e-05, 'epoch': 0.62}
{'loss': 0.8144, 'grad_norm': 0.4081107974052429, 'learning_rate': 6.531606399604473e-05, 'epoch': 0.63}
{'loss': 0.7135, 'grad_norm': 0.22593791782855988, 'learning_rate': 6.510980517908334e-05, 'epoch': 0.63}
{'loss': 0.7656, 'grad_norm': 0.25771716237068176, 'learning_rate': 6.490371522980696e-05, 'epoch': 0.63}
{'loss': 0.6634, 'grad_norm': 0.37518247961997986, 'learning_rate': 6.469779514568594e-05, 'epoch': 0.63}
{'loss': 0.6549, 'grad_norm': 0.29031145572662354, 'learning_rate': 6.449204592336841e-05, 'epoch': 0.63}
{'loss': 0.6635, 'grad_norm': 0.32078903913497925, 'learning_rate': 6.428646855867553e-05, 'epoch': 0.63}
{'loss': 0.6978, 'grad_norm': 0.47318488359451294, 'learning_rate': 6.40810640465967e-05, 'epoch': 0.63}
{'loss': 0.6851, 'grad_norm': 0.24250087141990662, 'learning_rate': 6.387583338128471e-05, 'epoch': 0.63}
{'loss': 0.6132, 'grad_norm': 0.27447691559791565, 'learning_rate': 6.367077755605094e-05, 'epoch': 0.63}
{'loss': 0.7005, 'grad_norm': 0.3384624421596527, 'learning_rate': 6.34658975633605e-05, 'epoch': 0.63}
{'loss': 0.6264, 'grad_norm': 0.2540697753429413, 'learning_rate': 6.326119439482756e-05, 'epoch': 0.63}
{'loss': 0.6452, 'grad_norm': 0.27849435806274414, 'learning_rate': 6.305666904121044e-05, 'epoch': 0.63}
{'loss': 0.6577, 'grad_norm': 0.36950668692588806, 'learning_rate': 6.285232249240678e-05, 'epoch': 0.63}
{'loss': 0.6626, 'grad_norm': 0.26174286007881165, 'learning_rate': 6.264815573744884e-05, 'epoch': 0.63}
{'loss': 0.7348, 'grad_norm': 0.6901853680610657, 'learning_rate': 6.244416976449875e-05, 'epoch': 0.64}
{'loss': 0.7479, 'grad_norm': 0.3595781624317169, 'learning_rate': 6.224036556084357e-05, 'epoch': 0.64}
{'loss': 0.5704, 'grad_norm': 0.28512877225875854, 'learning_rate': 6.203674411289062e-05, 'epoch': 0.64}
{'loss': 0.6366, 'grad_norm': 0.27960726618766785, 'learning_rate': 6.183330640616273e-05, 'epoch': 0.64}
{'loss': 0.6557, 'grad_norm': 0.3032838702201843, 'learning_rate': 6.163005342529341e-05, 'epoch': 0.64}
{'loss': 0.5817, 'grad_norm': 0.3130605220794678, 'learning_rate': 6.142698615402205e-05, 'epoch': 0.64}
{'loss': 0.701, 'grad_norm': 0.412109911441803, 'learning_rate': 6.122410557518927e-05, 'epoch': 0.64}
{'loss': 0.5116, 'grad_norm': 0.23421986401081085, 'learning_rate': 6.102141267073207e-05, 'epoch': 0.64}
{'loss': 0.7305, 'grad_norm': 0.26239290833473206, 'learning_rate': 6.0818908421679154e-05, 'epoch': 0.64}
{'loss': 0.817, 'grad_norm': 0.2575894594192505, 'learning_rate': 6.0616593808146064e-05, 'epoch': 0.64}
{'loss': 0.6254, 'grad_norm': 0.23618477582931519, 'learning_rate': 6.041446980933061e-05, 'epoch': 0.64}
{'loss': 0.9193, 'grad_norm': 0.31900838017463684, 'learning_rate': 6.021253740350793e-05, 'epoch': 0.64}
{'loss': 0.6055, 'grad_norm': 0.30228981375694275, 'learning_rate': 6.001079756802592e-05, 'epoch': 0.64}
{'loss': 0.6157, 'grad_norm': 0.30449238419532776, 'learning_rate': 5.9809251279300416e-05, 'epoch': 0.64}
{'loss': 0.7234, 'grad_norm': 0.4278387427330017, 'learning_rate': 5.960789951281052e-05, 'epoch': 0.64}
{'loss': 0.612, 'grad_norm': 0.3680821657180786, 'learning_rate': 5.9406743243093807e-05, 'epoch': 0.65}
{'loss': 0.6186, 'grad_norm': 0.26273882389068604, 'learning_rate': 5.9205783443741704e-05, 'epoch': 0.65}
{'loss': 0.7468, 'grad_norm': 0.27635693550109863, 'learning_rate': 5.900502108739465e-05, 'epoch': 0.65}
{'loss': 0.6376, 'grad_norm': 0.2670122981071472, 'learning_rate': 5.880445714573758e-05, 'epoch': 0.65}
{'loss': 0.7417, 'grad_norm': 0.2850199341773987, 'learning_rate': 5.8604092589494994e-05, 'epoch': 0.65}
{'loss': 0.7173, 'grad_norm': 0.2938504219055176, 'learning_rate': 5.840392838842641e-05, 'epoch': 0.65}
[2026-02-10 01:06:31,331] [WARNING] [stage3.py:2274:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.7515, 'grad_norm': 0.47249969840049744, 'learning_rate': 5.82039655113217e-05, 'epoch': 0.65}
{'loss': 0.6901, 'grad_norm': 0.2340518981218338, 'learning_rate': 5.8004204925996164e-05, 'epoch': 0.65}
{'loss': 0.6808, 'grad_norm': 0.2621200680732727, 'learning_rate': 5.780464759928623e-05, 'epoch': 0.65}
{'loss': 0.8163, 'grad_norm': 0.2989453673362732, 'learning_rate': 5.760529449704443e-05, 'epoch': 0.65}
{'loss': 0.6804, 'grad_norm': 0.28151294589042664, 'learning_rate': 5.740614658413484e-05, 'epoch': 0.65}
{'loss': 0.6514, 'grad_norm': 0.3310905694961548, 'learning_rate': 5.720720482442845e-05, 'epoch': 0.65}
{'loss': 0.6402, 'grad_norm': 0.3221997618675232, 'learning_rate': 5.700847018079856e-05, 'epoch': 0.65}
{'loss': 0.6677, 'grad_norm': 0.304528146982193, 'learning_rate': 5.680994361511591e-05, 'epoch': 0.65}
{'loss': 0.7066, 'grad_norm': 0.1893974244594574, 'learning_rate': 5.6611626088244194e-05, 'epoch': 0.65}
{'loss': 0.6437, 'grad_norm': 0.3345821797847748, 'learning_rate': 5.641351856003538e-05, 'epoch': 0.66}
{'loss': 0.7563, 'grad_norm': 0.3564767837524414, 'learning_rate': 5.6215621989325e-05, 'epoch': 0.66}
{'loss': 0.6812, 'grad_norm': 0.17635437846183777, 'learning_rate': 5.601793733392764e-05, 'epoch': 0.66}
{'loss': 0.7018, 'grad_norm': 0.2816945016384125, 'learning_rate': 5.582046555063216e-05, 'epoch': 0.66}
{'loss': 0.6631, 'grad_norm': 0.4362959861755371, 'learning_rate': 5.562320759519712e-05, 'epoch': 0.66}
{'loss': 0.6193, 'grad_norm': 0.31362152099609375, 'learning_rate': 5.542616442234618e-05, 'epoch': 0.66}
{'loss': 0.5551, 'grad_norm': 0.2540707588195801, 'learning_rate': 5.522933698576345e-05, 'epoch': 0.66}
{'loss': 0.6685, 'grad_norm': 0.264137327671051, 'learning_rate': 5.503272623808888e-05, 'epoch': 0.66}
{'loss': 0.5599, 'grad_norm': 0.28557470440864563, 'learning_rate': 5.483633313091363e-05, 'epoch': 0.66}
{'loss': 0.6524, 'grad_norm': 0.3293137848377228, 'learning_rate': 5.464015861477557e-05, 'epoch': 0.66}
{'loss': 0.7428, 'grad_norm': 0.31099560856819153, 'learning_rate': 5.44442036391545e-05, 'epoch': 0.66}
{'loss': 0.6693, 'grad_norm': 0.27320659160614014, 'learning_rate': 5.4248469152467695e-05, 'epoch': 0.66}
[2026-02-10 01:23:14,053] [WARNING] [stage3.py:2274:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.6067, 'grad_norm': 0.3229644298553467, 'learning_rate': 5.4052956102065246e-05, 'epoch': 0.66}
{'loss': 0.7075, 'grad_norm': 0.7133991122245789, 'learning_rate': 5.385766543422551e-05, 'epoch': 0.66}
{'loss': 0.6374, 'grad_norm': 0.2978545129299164, 'learning_rate': 5.366259809415053e-05, 'epoch': 0.67}
{'loss': 0.8365, 'grad_norm': 0.25437113642692566, 'learning_rate': 5.346775502596138e-05, 'epoch': 0.67}
{'loss': 0.6449, 'grad_norm': 0.4019657075405121, 'learning_rate': 5.32731371726938e-05, 'epoch': 0.67}
{'loss': 0.7806, 'grad_norm': 0.42920005321502686, 'learning_rate': 5.307874547629339e-05, 'epoch': 0.67}
{'loss': 0.6039, 'grad_norm': 0.2756383419036865, 'learning_rate': 5.288458087761116e-05, 'epoch': 0.67}
{'loss': 0.5764, 'grad_norm': 0.2843533158302307, 'learning_rate': 5.269064431639901e-05, 'epoch': 0.67}
{'loss': 0.646, 'grad_norm': 0.26723501086235046, 'learning_rate': 5.249693673130511e-05, 'epoch': 0.67}
{'loss': 0.705, 'grad_norm': 0.3170408308506012, 'learning_rate': 5.230345905986944e-05, 'epoch': 0.67}
{'loss': 0.5897, 'grad_norm': 0.32928910851478577, 'learning_rate': 5.2110212238519105e-05, 'epoch': 0.67}
{'loss': 0.6581, 'grad_norm': 0.2678382396697998, 'learning_rate': 5.191719720256407e-05, 'epoch': 0.67}
{'loss': 0.6098, 'grad_norm': 0.23436933755874634, 'learning_rate': 5.17244148861923e-05, 'epoch': 0.67}
{'loss': 0.6264, 'grad_norm': 0.35420605540275574, 'learning_rate': 5.1531866222465466e-05, 'epoch': 0.67}
{'loss': 0.5672, 'grad_norm': 0.2474362701177597, 'learning_rate': 5.1339552143314384e-05, 'epoch': 0.67}
{'loss': 0.5996, 'grad_norm': 0.403046578168869, 'learning_rate': 5.1147473579534465e-05, 'epoch': 0.67}
{'loss': 0.6565, 'grad_norm': 0.33957767486572266, 'learning_rate': 5.0955631460781236e-05, 'epoch': 0.67}
{'loss': 0.7028, 'grad_norm': 0.24834612011909485, 'learning_rate': 5.0764026715565785e-05, 'epoch': 0.68}
{'loss': 0.7268, 'grad_norm': 0.3363821506500244, 'learning_rate': 5.057266027125045e-05, 'epoch': 0.68}
{'loss': 0.6428, 'grad_norm': 1.1950433254241943, 'learning_rate': 5.038153305404407e-05, 'epoch': 0.68}
{'loss': 0.6794, 'grad_norm': 0.3650789260864258, 'learning_rate': 5.01906459889977e-05, 'epoch': 0.68}
{'loss': 0.526, 'grad_norm': 0.1991022676229477, 'learning_rate': 5.000000000000002e-05, 'epoch': 0.68}
{'loss': 0.7281, 'grad_norm': 0.2723667621612549, 'learning_rate': 4.980959600977294e-05, 'epoch': 0.68}
{'loss': 0.8245, 'grad_norm': 0.3531950116157532, 'learning_rate': 4.961943493986708e-05, 'epoch': 0.68}
  warnings.warn(
/opt/conda/lib/python3.11/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
[2026-02-10 01:42:23,348] [WARNING] [stage3.py:2274:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
                                                                           
{'loss': 0.614, 'grad_norm': 0.36185234785079956, 'learning_rate': 4.942951771065736e-05, 'epoch': 0.68}
{'loss': 0.6756, 'grad_norm': 0.40576404333114624, 'learning_rate': 4.9239845241338435e-05, 'epoch': 0.68}
{'loss': 0.6119, 'grad_norm': 0.3451479971408844, 'learning_rate': 4.90504184499205e-05, 'epoch': 0.68}
{'loss': 0.7227, 'grad_norm': 0.22731895744800568, 'learning_rate': 4.886123825322451e-05, 'epoch': 0.68}
{'loss': 0.6569, 'grad_norm': 0.247882679104805, 'learning_rate': 4.8672305566877964e-05, 'epoch': 0.68}
{'loss': 0.67, 'grad_norm': 0.2558461129665375, 'learning_rate': 4.848362130531039e-05, 'epoch': 0.68}
{'loss': 0.7023, 'grad_norm': 0.2385498583316803, 'learning_rate': 4.829518638174905e-05, 'epoch': 0.68}
{'loss': 0.6457, 'grad_norm': 0.26146334409713745, 'learning_rate': 4.810700170821429e-05, 'epoch': 0.68}
{'loss': 0.6566, 'grad_norm': 0.3192654550075531, 'learning_rate': 4.791906819551533e-05, 'epoch': 0.69}
{'loss': 0.6338, 'grad_norm': 0.3520512878894806, 'learning_rate': 4.7731386753245675e-05, 'epoch': 0.69}
{'loss': 0.6391, 'grad_norm': 0.2240782231092453, 'learning_rate': 4.7543958289779e-05, 'epoch': 0.69}
{'loss': 0.7271, 'grad_norm': 0.4710182547569275, 'learning_rate': 4.735678371226441e-05, 'epoch': 0.69}
{'loss': 0.5921, 'grad_norm': 0.23950104415416718, 'learning_rate': 4.716986392662226e-05, 'epoch': 0.69}
{'loss': 0.6658, 'grad_norm': 0.3800589442253113, 'learning_rate': 4.6983199837539705e-05, 'epoch': 0.69}
{'loss': 0.5952, 'grad_norm': 0.27720287442207336, 'learning_rate': 4.6796792348466356e-05, 'epoch': 0.69}
{'loss': 0.6132, 'grad_norm': 0.24726086854934692, 'learning_rate': 4.661064236160987e-05, 'epoch': 0.69}
{'loss': 0.6317, 'grad_norm': 0.36977940797805786, 'learning_rate': 4.642475077793158e-05, 'epoch': 0.69}
{'loss': 0.655, 'grad_norm': 0.24816815555095673, 'learning_rate': 4.6239118497142256e-05, 'epoch': 0.69}
{'loss': 0.7168, 'grad_norm': 0.3795304000377655, 'learning_rate': 4.605374641769752e-05, 'epoch': 0.69}
{'loss': 0.6512, 'grad_norm': 0.337298721075058, 'learning_rate': 4.586863543679368e-05, 'epoch': 0.69}
{'loss': 0.5992, 'grad_norm': 0.2406419962644577, 'learning_rate': 4.568378645036335e-05, 'epoch': 0.69}
{'loss': 0.7113, 'grad_norm': 0.4036928415298462, 'learning_rate': 4.549920035307107e-05, 'epoch': 0.69}
{'loss': 0.676, 'grad_norm': 0.2924361824989319, 'learning_rate': 4.5314878038309005e-05, 'epoch': 0.69}
{'loss': 0.6379, 'grad_norm': 0.29845306277275085, 'learning_rate': 4.513082039819264e-05, 'epoch': 0.7}
{'loss': 0.7127, 'grad_norm': 0.5830411911010742, 'learning_rate': 4.494702832355637e-05, 'epoch': 0.7}
{'loss': 0.7665, 'grad_norm': 0.3616490960121155, 'learning_rate': 4.476350270394942e-05, 'epoch': 0.7}
{'loss': 0.7285, 'grad_norm': 0.2723793685436249, 'learning_rate': 4.4580244427631215e-05, 'epoch': 0.7}
{'loss': 0.5453, 'grad_norm': 0.28486043214797974, 'learning_rate': 4.439725438156731e-05, 'epoch': 0.7}
{'loss': 0.651, 'grad_norm': 0.2159445881843567, 'learning_rate': 4.4214533451425035e-05, 'epoch': 0.7}
{'loss': 0.6075, 'grad_norm': 0.5608281493186951, 'learning_rate': 4.403208252156921e-05, 'epoch': 0.7}
{'loss': 0.5966, 'grad_norm': 0.2892122268676758, 'learning_rate': 4.384990247505781e-05, 'epoch': 0.7}
{'loss': 0.7379, 'grad_norm': 0.42419224977493286, 'learning_rate': 4.3667994193637796e-05, 'epoch': 0.7}
{'loss': 0.588, 'grad_norm': 0.2485555112361908, 'learning_rate': 4.3486358557740814e-05, 'epoch': 0.7}
{'loss': 0.6492, 'grad_norm': 0.33173394203186035, 'learning_rate': 4.3304996446478854e-05, 'epoch': 0.7}
{'loss': 0.7165, 'grad_norm': 0.3832543194293976, 'learning_rate': 4.312390873764006e-05, 'epoch': 0.7}
{'loss': 0.7365, 'grad_norm': 0.2876805067062378, 'learning_rate': 4.2943096307684516e-05, 'epoch': 0.7}
[2026-02-10 02:11:30,577] [WARNING] [stage3.py:2274:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.7429, 'grad_norm': 0.3145165741443634, 'learning_rate': 4.276256003173992e-05, 'epoch': 0.7}
{'loss': 0.6317, 'grad_norm': 0.24651774764060974, 'learning_rate': 4.2582300783597404e-05, 'epoch': 0.71}
{'loss': 0.5885, 'grad_norm': 0.3528970181941986, 'learning_rate': 4.2402319435707274e-05, 'epoch': 0.71}
{'loss': 0.596, 'grad_norm': 0.2564563751220703, 'learning_rate': 4.222261685917489e-05, 'epoch': 0.71}
{'loss': 0.6666, 'grad_norm': 0.3129889965057373, 'learning_rate': 4.204319392375629e-05, 'epoch': 0.71}
{'loss': 0.7965, 'grad_norm': 0.3829151391983032, 'learning_rate': 4.186405149785403e-05, 'epoch': 0.71}
{'loss': 0.6848, 'grad_norm': 0.314290851354599, 'learning_rate': 4.168519044851308e-05, 'epoch': 0.71}
{'loss': 0.7678, 'grad_norm': 0.34936001896858215, 'learning_rate': 4.15066116414165e-05, 'epoch': 0.71}
{'loss': 0.6282, 'grad_norm': 0.2619338929653168, 'learning_rate': 4.132831594088135e-05, 'epoch': 0.71}
{'loss': 0.7337, 'grad_norm': 0.3451847434043884, 'learning_rate': 4.115030420985437e-05, 'epoch': 0.71}
{'loss': 0.6952, 'grad_norm': 0.28967607021331787, 'learning_rate': 4.0972577309908056e-05, 'epoch': 0.71}
{'loss': 0.7606, 'grad_norm': 0.3190164268016815, 'learning_rate': 4.079513610123619e-05, 'epoch': 0.71}
{'loss': 0.6524, 'grad_norm': 0.3291790783405304, 'learning_rate': 4.0617981442649855e-05, 'epoch': 0.71}
{'loss': 0.5936, 'grad_norm': 0.2126004844903946, 'learning_rate': 4.044111419157326e-05, 'epoch': 0.71}
{'loss': 0.7216, 'grad_norm': 0.24014610052108765, 'learning_rate': 4.0264535204039486e-05, 'epoch': 0.71}
{'loss': 0.6213, 'grad_norm': 0.25951024889945984, 'learning_rate': 4.0088245334686594e-05, 'epoch': 0.71}
{'loss': 0.6582, 'grad_norm': 0.3081003427505493, 'learning_rate': 3.991224543675316e-05, 'epoch': 0.72}
{'loss': 0.5365, 'grad_norm': 0.23912382125854492, 'learning_rate': 3.973653636207437e-05, 'epoch': 0.72}
{'loss': 0.7357, 'grad_norm': 0.25157830119132996, 'learning_rate': 3.9561118961077794e-05, 'epoch': 0.72}
{'loss': 0.5189, 'grad_norm': 0.28059837222099304, 'learning_rate': 3.93859940827794e-05, 'epoch': 0.72}
{'loss': 0.7198, 'grad_norm': 0.37771543860435486, 'learning_rate': 3.921116257477927e-05, 'epoch': 0.72}
{'loss': 0.7671, 'grad_norm': 0.32702359557151794, 'learning_rate': 3.903662528325759e-05, 'epoch': 0.72}
{'loss': 0.7563, 'grad_norm': 0.32104527950286865, 'learning_rate': 3.886238305297056e-05, 'epoch': 0.72}
{'loss': 0.7741, 'grad_norm': 0.2522604465484619, 'learning_rate': 3.86884367272463e-05, 'epoch': 0.72}
{'loss': 0.5783, 'grad_norm': 0.29541993141174316, 'learning_rate': 3.851478714798076e-05, 'epoch': 0.72}
{'loss': 0.6948, 'grad_norm': 0.3800109028816223, 'learning_rate': 3.834143515563358e-05, 'epoch': 0.72}
{'loss': 0.6291, 'grad_norm': 0.22701764106750488, 'learning_rate': 3.81683815892242e-05, 'epoch': 0.72}
{'loss': 0.6923, 'grad_norm': 0.475975900888443, 'learning_rate': 3.7995627286327616e-05, 'epoch': 0.72}
{'loss': 0.7751, 'grad_norm': 0.5187282562255859, 'learning_rate': 3.7823173083070405e-05, 'epoch': 0.72}
{'loss': 0.6849, 'grad_norm': 0.28849148750305176, 'learning_rate': 3.7651019814126654e-05, 'epoch': 0.72}
{'loss': 0.7152, 'grad_norm': 0.17211845517158508, 'learning_rate': 3.7479168312713964e-05, 'epoch': 0.72}
{'loss': 0.651, 'grad_norm': 0.295297771692276, 'learning_rate': 3.7307619410589376e-05, 'epoch': 0.73}
{'loss': 0.6399, 'grad_norm': 0.3938048183917999, 'learning_rate': 3.713637393804531e-05, 'epoch': 0.73}
{'loss': 0.7147, 'grad_norm': 0.25371718406677246, 'learning_rate': 3.6965432723905735e-05, 'epoch': 0.73}
{'loss': 0.5451, 'grad_norm': 0.3552720844745636, 'learning_rate': 3.6794796595521866e-05, 'epoch': 0.73}
{'loss': 0.6457, 'grad_norm': 0.5177862644195557, 'learning_rate': 3.662446637876838e-05, 'epoch': 0.73}
{'loss': 0.6367, 'grad_norm': 0.3091638386249542, 'learning_rate': 3.6454442898039356e-05, 'epoch': 0.73}
{'loss': 0.7761, 'grad_norm': 0.31545960903167725, 'learning_rate': 3.628472697624422e-05, 'epoch': 0.73}
{'loss': 0.7327, 'grad_norm': 0.26871415972709656, 'learning_rate': 3.6115319434803894e-05, 'epoch': 0.73}
{'loss': 0.7366, 'grad_norm': 0.28008103370666504, 'learning_rate': 3.594622109364666e-05, 'epoch': 0.73}
{'loss': 0.7604, 'grad_norm': 0.4347539246082306, 'learning_rate': 3.577743277120441e-05, 'epoch': 0.73}
{'loss': 0.6805, 'grad_norm': 0.3282769024372101, 'learning_rate': 3.5608955284408443e-05, 'epoch': 0.73}
{'loss': 0.5614, 'grad_norm': 0.2810003459453583, 'learning_rate': 3.544078944868564e-05, 'epoch': 0.73}
{'loss': 0.673, 'grad_norm': 0.27455466985702515, 'learning_rate': 3.527293607795452e-05, 'epoch': 0.73}
{'loss': 0.5739, 'grad_norm': 0.33147549629211426, 'learning_rate': 3.510539598462127e-05, 'epoch': 0.73}
{'loss': 0.7245, 'grad_norm': 0.2973065674304962, 'learning_rate': 3.493816997957582e-05, 'epoch': 0.73}
{'loss': 0.7941, 'grad_norm': 0.30788862705230713, 'learning_rate': 3.477125887218792e-05, 'epoch': 0.74}
{'loss': 0.6587, 'grad_norm': 0.2598801851272583, 'learning_rate': 3.460466347030319e-05, 'epoch': 0.74}
{'loss': 0.5532, 'grad_norm': 0.34021905064582825, 'learning_rate': 3.4438384580239356e-05, 'epoch': 0.74}
{'loss': 0.5459, 'grad_norm': 0.23895439505577087, 'learning_rate': 3.427242300678213e-05, 'epoch': 0.74}
{'loss': 0.6553, 'grad_norm': 0.35458168387413025, 'learning_rate': 3.410677955318142e-05, 'epoch': 0.74}
{'loss': 0.5068, 'grad_norm': 0.20327702164649963, 'learning_rate': 3.3941455021147496e-05, 'epoch': 0.74}
{'loss': 0.6197, 'grad_norm': 0.34485554695129395, 'learning_rate': 3.377645021084701e-05, 'epoch': 0.74}
{'loss': 0.8655, 'grad_norm': 0.27823466062545776, 'learning_rate': 3.361176592089919e-05, 'epoch': 0.74}
{'loss': 0.7934, 'grad_norm': 2.0650501251220703, 'learning_rate': 3.344740294837192e-05, 'epoch': 0.74}
{'loss': 0.7038, 'grad_norm': 0.305669367313385, 'learning_rate': 3.328336208877803e-05, 'epoch': 0.74}
{'loss': 0.5579, 'grad_norm': 0.21139420568943024, 'learning_rate': 3.311964413607117e-05, 'epoch': 0.74}
{'loss': 0.6878, 'grad_norm': 0.4731910824775696, 'learning_rate': 3.295624988264224e-05, 'epoch': 0.74}
[2026-02-10 02:58:59,949] [WARNING] [stage3.py:2274:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.88, 'grad_norm': 0.5314701199531555, 'learning_rate': 3.279318011931538e-05, 'epoch': 0.74}
{'loss': 0.6151, 'grad_norm': 0.37956246733665466, 'learning_rate': 3.263043563534428e-05, 'epoch': 0.74}
{'loss': 0.654, 'grad_norm': 0.30122411251068115, 'learning_rate': 3.2468017218408206e-05, 'epoch': 0.75}
{'loss': 0.7276, 'grad_norm': 0.6728852391242981, 'learning_rate': 3.2305925654608326e-05, 'epoch': 0.75}
{'loss': 0.6096, 'grad_norm': 0.20412184298038483, 'learning_rate': 3.214416172846381e-05, 'epoch': 0.75}
{'loss': 0.6862, 'grad_norm': 0.29192858934402466, 'learning_rate': 3.198272622290804e-05, 'epoch': 0.75}
{'loss': 0.6813, 'grad_norm': 0.599480926990509, 'learning_rate': 3.1821619919284985e-05, 'epoch': 0.75}
{'loss': 0.6514, 'grad_norm': 0.28197887539863586, 'learning_rate': 3.1660843597345135e-05, 'epoch': 0.75}
{'loss': 0.6256, 'grad_norm': 0.22564902901649475, 'learning_rate': 3.150039803524194e-05, 'epoch': 0.75}
{'loss': 0.6838, 'grad_norm': 0.33603599667549133, 'learning_rate': 3.134028400952799e-05, 'epoch': 0.75}
{'loss': 0.6331, 'grad_norm': 0.2743459641933441, 'learning_rate': 3.1180502295151215e-05, 'epoch': 0.75}
{'loss': 0.6617, 'grad_norm': 0.41599440574645996, 'learning_rate': 3.1021053665451206e-05, 'epoch': 0.75}
{'loss': 0.7762, 'grad_norm': 0.2868070602416992, 'learning_rate': 3.086193889215535e-05, 'epoch': 0.75}
{'loss': 0.662, 'grad_norm': 0.26887035369873047, 'learning_rate': 3.070315874537532e-05, 'epoch': 0.75}
{'loss': 0.7079, 'grad_norm': 0.2700321674346924, 'learning_rate': 3.054471399360308e-05, 'epoch': 0.75}
{'loss': 0.5948, 'grad_norm': 0.33460733294487, 'learning_rate': 3.0386605403707346e-05, 'epoch': 0.75}
{'loss': 0.7203, 'grad_norm': 0.3468165099620819, 'learning_rate': 3.0228833740929797e-05, 'epoch': 0.75}
{'loss': 0.8354, 'grad_norm': 0.22392582893371582, 'learning_rate': 3.0071399768881393e-05, 'epoch': 0.76}
{'loss': 0.4721, 'grad_norm': 0.23750178515911102, 'learning_rate': 2.9914304249538705e-05, 'epoch': 0.76}
{'loss': 0.6345, 'grad_norm': 0.29010072350502014, 'learning_rate': 2.975754794324015e-05, 'epoch': 0.76}
{'loss': 0.6321, 'grad_norm': 0.23750199377536774, 'learning_rate': 2.960113160868244e-05, 'epoch': 0.76}
{'loss': 0.7255, 'grad_norm': 0.3799562156200409, 'learning_rate': 2.944505600291678e-05, 'epoch': 0.76}
[2026-02-10 03:16:29,284] [WARNING] [stage3.py:2274:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.6435, 'grad_norm': 0.2239057570695877, 'learning_rate': 2.9289321881345254e-05, 'epoch': 0.76}
[2026-02-10 03:17:18,615] [WARNING] [stage3.py:2274:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.8146, 'grad_norm': 0.3731195032596588, 'learning_rate': 2.913392999771718e-05, 'epoch': 0.76}
{'loss': 0.6198, 'grad_norm': 0.2615356147289276, 'learning_rate': 2.8978881104125455e-05, 'epoch': 0.76}
{'loss': 0.686, 'grad_norm': 0.2741791903972626, 'learning_rate': 2.8824175951002917e-05, 'epoch': 0.76}
{'loss': 0.6185, 'grad_norm': 0.4327997863292694, 'learning_rate': 2.8669815287118672e-05, 'epoch': 0.76}
{'loss': 0.8037, 'grad_norm': 0.3534095287322998, 'learning_rate': 2.8515799859574588e-05, 'epoch': 0.76}
{'loss': 0.6881, 'grad_norm': 0.2505335211753845, 'learning_rate': 2.8362130413801524e-05, 'epoch': 0.76}
{'loss': 0.6569, 'grad_norm': 0.31958848237991333, 'learning_rate': 2.8208807693555818e-05, 'epoch': 0.76}
{'loss': 0.7265, 'grad_norm': 0.48239248991012573, 'learning_rate': 2.8055832440915676e-05, 'epoch': 0.76}
{'loss': 0.6429, 'grad_norm': 0.3690451979637146, 'learning_rate': 2.7903205396277542e-05, 'epoch': 0.76}
{'loss': 0.6517, 'grad_norm': 0.26167741417884827, 'learning_rate': 2.775092729835257e-05, 'epoch': 0.77}
{'loss': 0.7029, 'grad_norm': 0.2745562493801117, 'learning_rate': 2.7598998884162974e-05, 'epoch': 0.77}
{'loss': 0.499, 'grad_norm': 0.3334554135799408, 'learning_rate': 2.744742088903861e-05, 'epoch': 0.77}
{'loss': 0.6526, 'grad_norm': 0.2575727105140686, 'learning_rate': 2.729619404661321e-05, 'epoch': 0.77}
{'loss': 0.6195, 'grad_norm': 0.6264955401420593, 'learning_rate': 2.7145319088820987e-05, 'epoch': 0.77}
{'loss': 0.7222, 'grad_norm': 0.3814794421195984, 'learning_rate': 2.6994796745893002e-05, 'epoch': 0.77}
{'loss': 0.6849, 'grad_norm': 0.28086990118026733, 'learning_rate': 2.6844627746353724e-05, 'epoch': 0.77}
{'loss': 0.6461, 'grad_norm': 0.3610058128833771, 'learning_rate': 2.669481281701739e-05, 'epoch': 0.77}
{'loss': 0.6958, 'grad_norm': 0.29333481192588806, 'learning_rate': 2.654535268298457e-05, 'epoch': 0.77}
{'loss': 0.5019, 'grad_norm': 0.4505775272846222, 'learning_rate': 2.6396248067638675e-05, 'epoch': 0.77}
[2026-02-10 03:32:40,422] [WARNING] [stage3.py:2274:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.5934, 'grad_norm': 0.4609919488430023, 'learning_rate': 2.6247499692642364e-05, 'epoch': 0.77}
{'loss': 0.5654, 'grad_norm': 0.1997009664773941, 'learning_rate': 2.6099108277934103e-05, 'epoch': 0.77}
{'loss': 0.6695, 'grad_norm': 0.3281610906124115, 'learning_rate': 2.5951074541724697e-05, 'epoch': 0.77}
{'loss': 0.5302, 'grad_norm': 0.3159877359867096, 'learning_rate': 2.580339920049376e-05, 'epoch': 0.77}
{'loss': 0.6345, 'grad_norm': 0.1756899058818817, 'learning_rate': 2.5656082968986373e-05, 'epoch': 0.78}
{'loss': 0.5734, 'grad_norm': 0.31860145926475525, 'learning_rate': 2.5509126560209428e-05, 'epoch': 0.78}
{'loss': 0.7602, 'grad_norm': 0.3244522213935852, 'learning_rate': 2.5362530685428353e-05, 'epoch': 0.78}
{'loss': 0.7548, 'grad_norm': 0.44398579001426697, 'learning_rate': 2.5216296054163546e-05, 'epoch': 0.78}
{'loss': 0.7279, 'grad_norm': 0.3448452949523926, 'learning_rate': 2.507042337418707e-05, 'epoch': 0.78}
{'loss': 0.7042, 'grad_norm': 0.22085551917552948, 'learning_rate': 2.4924913351519084e-05, 'epoch': 0.78}
{'loss': 0.7414, 'grad_norm': 0.35711318254470825, 'learning_rate': 2.477976669042452e-05, 'epoch': 0.78}
{'loss': 0.6988, 'grad_norm': 0.2872649133205414, 'learning_rate': 2.4634984093409662e-05, 'epoch': 0.78}
{'loss': 0.7291, 'grad_norm': 0.2578221261501312, 'learning_rate': 2.4490566261218716e-05, 'epoch': 0.78}
{'loss': 0.5418, 'grad_norm': 0.21442636847496033, 'learning_rate': 2.4346513892830423e-05, 'epoch': 0.78}
{'loss': 0.5666, 'grad_norm': 0.24519170820713043, 'learning_rate': 2.420282768545469e-05, 'epoch': 0.78}
{'loss': 0.551, 'grad_norm': 0.23569954931735992, 'learning_rate': 2.405950833452928e-05, 'epoch': 0.78}
{'loss': 0.5757, 'grad_norm': 0.30175086855888367, 'learning_rate': 2.3916556533716294e-05, 'epoch': 0.78}
{'loss': 0.7311, 'grad_norm': 0.3896578252315521, 'learning_rate': 2.377397297489895e-05, 'epoch': 0.78}
{'loss': 0.8256, 'grad_norm': 0.3079968988895416, 'learning_rate': 2.3631758348178146e-05, 'epoch': 0.78}
{'loss': 0.6858, 'grad_norm': 0.24925197660923004, 'learning_rate': 2.3489913341869195e-05, 'epoch': 0.79}
{'loss': 0.6394, 'grad_norm': 0.33469220995903015, 'learning_rate': 2.3348438642498427e-05, 'epoch': 0.79}
{'loss': 0.7159, 'grad_norm': 0.3004280924797058, 'learning_rate': 2.320733493479992e-05, 'epoch': 0.79}
{'loss': 0.7805, 'grad_norm': 0.31741657853126526, 'learning_rate': 2.3066602901712108e-05, 'epoch': 0.79}
{'loss': 0.7755, 'grad_norm': 0.22007746994495392, 'learning_rate': 2.2926243224374632e-05, 'epoch': 0.79}
{'loss': 0.6648, 'grad_norm': 0.27178144454956055, 'learning_rate': 2.2786256582124865e-05, 'epoch': 0.79}
{'loss': 0.5906, 'grad_norm': 0.22970296442508698, 'learning_rate': 2.2646643652494692e-05, 'epoch': 0.79}
{'loss': 0.7386, 'grad_norm': 0.27609550952911377, 'learning_rate': 2.2507405111207282e-05, 'epoch': 0.79}
{'loss': 0.7598, 'grad_norm': 0.429245263338089, 'learning_rate': 2.2368541632173733e-05, 'epoch': 0.79}
{'loss': 0.7457, 'grad_norm': 0.2550880014896393, 'learning_rate': 2.2230053887489867e-05, 'epoch': 0.79}
{'loss': 0.7955, 'grad_norm': 0.3649497628211975, 'learning_rate': 2.2091942547432955e-05, 'epoch': 0.79}
{'loss': 0.7785, 'grad_norm': 0.3098078966140747, 'learning_rate': 2.1954208280458534e-05, 'epoch': 0.79}
{'loss': 0.6595, 'grad_norm': 0.3064061403274536, 'learning_rate': 2.181685175319702e-05, 'epoch': 0.79}
{'loss': 0.675, 'grad_norm': 0.41620805859565735, 'learning_rate': 2.1679873630450663e-05, 'epoch': 0.79}
{'loss': 0.7531, 'grad_norm': 0.3619442880153656, 'learning_rate': 2.1543274575190188e-05, 'epoch': 0.79}
{'loss': 0.5624, 'grad_norm': 0.20603588223457336, 'learning_rate': 2.1407055248551665e-05, 'epoch': 0.8}
{'loss': 0.6287, 'grad_norm': 0.40980082750320435, 'learning_rate': 2.1271216309833296e-05, 'epoch': 0.8}
{'loss': 0.6103, 'grad_norm': 0.3245057463645935, 'learning_rate': 2.113575841649217e-05, 'epoch': 0.8}
{'loss': 0.5699, 'grad_norm': 0.35371658205986023, 'learning_rate': 2.100068222414121e-05, 'epoch': 0.8}
{'loss': 0.5868, 'grad_norm': 0.2349449247121811, 'learning_rate': 2.0865988386545844e-05, 'epoch': 0.8}
{'loss': 0.6689, 'grad_norm': 0.17819249629974365, 'learning_rate': 2.0731677555620932e-05, 'epoch': 0.8}
{'loss': 0.6702, 'grad_norm': 0.23316243290901184, 'learning_rate': 2.0597750381427604e-05, 'epoch': 0.8}
{'loss': 0.6331, 'grad_norm': 0.24934296309947968, 'learning_rate': 2.0464207512170065e-05, 'epoch': 0.8}
{'loss': 0.589, 'grad_norm': 0.31846311688423157, 'learning_rate': 2.033104959419254e-05, 'epoch': 0.8}
{'loss': 0.5991, 'grad_norm': 0.2283797413110733, 'learning_rate': 2.0198277271976052e-05, 'epoch': 0.8}
{'loss': 0.7525, 'grad_norm': 0.39501944184303284, 'learning_rate': 2.0065891188135422e-05, 'epoch': 0.8}
{'loss': 0.7006, 'grad_norm': 0.34388190507888794, 'learning_rate': 1.993389198341601e-05, 'epoch': 0.8}
{'loss': 0.6583, 'grad_norm': 0.41409608721733093, 'learning_rate': 1.9802280296690722e-05, 'epoch': 0.8}
{'loss': 0.7172, 'grad_norm': 0.23664820194244385, 'learning_rate': 1.9671056764956876e-05, 'epoch': 0.8}
{'loss': 0.7359, 'grad_norm': 0.3872711658477783, 'learning_rate': 1.9540222023333166e-05, 'epoch': 0.8}
{'loss': 0.5907, 'grad_norm': 0.35546213388442993, 'learning_rate': 1.9409776705056516e-05, 'epoch': 0.81}
{'loss': 0.6413, 'grad_norm': 0.5054652690887451, 'learning_rate': 1.927972144147905e-05, 'epoch': 0.81}
{'loss': 0.7661, 'grad_norm': 0.2496161162853241, 'learning_rate': 1.915005686206506e-05, 'epoch': 0.81}
{'loss': 0.6142, 'grad_norm': 0.28403201699256897, 'learning_rate': 1.902078359438788e-05, 'epoch': 0.81}
{'loss': 0.5902, 'grad_norm': 0.25137537717819214, 'learning_rate': 1.8891902264127004e-05, 'epoch': 0.81}
{'loss': 0.5366, 'grad_norm': 0.2862173616886139, 'learning_rate': 1.8763413495064875e-05, 'epoch': 0.81}
{'loss': 0.6848, 'grad_norm': 0.2760644257068634, 'learning_rate': 1.8635317909083983e-05, 'epoch': 0.81}
[2026-02-10 04:17:43,445] [WARNING] [stage3.py:2274:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.6939, 'grad_norm': 0.28372398018836975, 'learning_rate': 1.850761612616381e-05, 'epoch': 0.81}
{'loss': 0.6456, 'grad_norm': 0.3916662633419037, 'learning_rate': 1.8380308764377842e-05, 'epoch': 0.81}
{'loss': 0.6026, 'grad_norm': 0.3153756856918335, 'learning_rate': 1.825339643989058e-05, 'epoch': 0.81}
{'loss': 0.7182, 'grad_norm': 0.20106329023838043, 'learning_rate': 1.812687976695453e-05, 'epoch': 0.81}
[2026-02-10 04:20:56,519] [WARNING] [stage3.py:2274:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.7986, 'grad_norm': 0.3551251292228699, 'learning_rate': 1.8000759357907338e-05, 'epoch': 0.81}
{'loss': 0.5867, 'grad_norm': 0.23244988918304443, 'learning_rate': 1.787503582316864e-05, 'epoch': 0.81}
[2026-02-10 04:22:35,528] [WARNING] [stage3.py:2274:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.5517, 'grad_norm': 0.32518234848976135, 'learning_rate': 1.7749709771237266e-05, 'epoch': 0.81}
{'loss': 0.8148, 'grad_norm': 0.22020557522773743, 'learning_rate': 1.762478180868823e-05, 'epoch': 0.82}
{'loss': 0.7524, 'grad_norm': 0.2686554789543152, 'learning_rate': 1.750025254016978e-05, 'epoch': 0.82}
{'loss': 0.5494, 'grad_norm': 0.3389056622982025, 'learning_rate': 1.7376122568400532e-05, 'epoch': 0.82}
{'loss': 0.6738, 'grad_norm': 0.4294285178184509, 'learning_rate': 1.7252392494166458e-05, 'epoch': 0.82}
{'loss': 0.6331, 'grad_norm': 0.2685394287109375, 'learning_rate': 1.712906291631814e-05, 'epoch': 0.82}
{'loss': 0.6516, 'grad_norm': 0.19899284839630127, 'learning_rate': 1.700613443176766e-05, 'epoch': 0.82}
{'loss': 0.7337, 'grad_norm': 0.2668720781803131, 'learning_rate': 1.6883607635485877e-05, 'epoch': 0.82}
{'loss': 0.6496, 'grad_norm': 0.35933423042297363, 'learning_rate': 1.6761483120499455e-05, 'epoch': 0.82}
{'loss': 0.7848, 'grad_norm': 0.38539499044418335, 'learning_rate': 1.663976147788806e-05, 'epoch': 0.82}
{'loss': 0.6754, 'grad_norm': 0.3193175196647644, 'learning_rate': 1.651844329678145e-05, 'epoch': 0.82}
{'loss': 0.7583, 'grad_norm': 0.3282965421676636, 'learning_rate': 1.6397529164356606e-05, 'epoch': 0.82}
{'loss': 0.5864, 'grad_norm': 0.2378990352153778, 'learning_rate': 1.6277019665835004e-05, 'epoch': 0.82}
{'loss': 0.6207, 'grad_norm': 0.34483465552330017, 'learning_rate': 1.615691538447962e-05, 'epoch': 0.82}
{'loss': 0.8494, 'grad_norm': 0.423422634601593, 'learning_rate': 1.6037216901592243e-05, 'epoch': 0.82}
{'loss': 0.6716, 'grad_norm': 0.6120971441268921, 'learning_rate': 1.5917924796510587e-05, 'epoch': 0.82}
{'loss': 0.5887, 'grad_norm': 0.2639857530593872, 'learning_rate': 1.5799039646605486e-05, 'epoch': 0.83}
{'loss': 0.5874, 'grad_norm': 0.43671077489852905, 'learning_rate': 1.5680562027278157e-05, 'epoch': 0.83}
{'loss': 0.6124, 'grad_norm': 0.3311576247215271, 'learning_rate': 1.5562492511957373e-05, 'epoch': 0.83}
{'loss': 0.6161, 'grad_norm': 0.2842300832271576, 'learning_rate': 1.544483167209664e-05, 'epoch': 0.83}
{'loss': 0.6554, 'grad_norm': 0.35855287313461304, 'learning_rate': 1.5327580077171587e-05, 'epoch': 0.83}
{'loss': 0.614, 'grad_norm': 0.32914867997169495, 'learning_rate': 1.521073829467703e-05, 'epoch': 0.83}
{'loss': 0.5715, 'grad_norm': 0.1681368499994278, 'learning_rate': 1.509430689012432e-05, 'epoch': 0.83}
{'loss': 0.6684, 'grad_norm': 0.2443872094154358, 'learning_rate': 1.4978286427038601e-05, 'epoch': 0.83}
{'loss': 0.7058, 'grad_norm': 0.2526591718196869, 'learning_rate': 1.4862677466956055e-05, 'epoch': 0.83}
{'loss': 0.638, 'grad_norm': 0.24604718387126923, 'learning_rate': 1.474748056942119e-05, 'epoch': 0.83}
{'loss': 0.7093, 'grad_norm': 0.26777440309524536, 'learning_rate': 1.463269629198416e-05, 'epoch': 0.83}
{'loss': 0.691, 'grad_norm': 0.2947142422199249, 'learning_rate': 1.4518325190198078e-05, 'epoch': 0.83}
{'loss': 0.4581, 'grad_norm': 0.29176193475723267, 'learning_rate': 1.4404367817616249e-05, 'epoch': 0.83}
{'loss': 0.6303, 'grad_norm': 0.3543456494808197, 'learning_rate': 1.4290824725789542e-05, 'epoch': 0.83}
[2026-02-10 04:46:39,986] [WARNING] [stage3.py:2274:step] 1 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
{'loss': 0.7136, 'grad_norm': 0.31383398175239563, 'learning_rate': 1.4177696464263723e-05, 'epoch': 0.83}
{'loss': 0.7149, 'grad_norm': 0.40166348218917847, 'learning_rate': 1.406498358057683e-05, 'epoch': 0.84}
{'loss': 0.5509, 'grad_norm': 0.21179258823394775, 'learning_rate': 1.3952686620256428e-05, 'epoch': 0.84}
{'loss': 0.8325, 'grad_norm': 0.44433918595314026, 'learning_rate': 1.3840806126817052e-05, 'epoch': 0.84}
{'loss': 0.6011, 'grad_norm': 0.3723680078983307, 'learning_rate': 1.3729342641757503e-05, 'epoch': 0.84}
{'loss': 0.7456, 'grad_norm': 0.31326964497566223, 'learning_rate': 1.3618296704558364e-05, 'epoch': 0.84}
{'loss': 0.5945, 'grad_norm': 0.4014987349510193, 'learning_rate': 1.3507668852679212e-05, 'epoch': 0.84}
{'loss': 0.6159, 'grad_norm': 0.2691132724285126, 'learning_rate': 1.339745962155613e-05, 'epoch': 0.84}
{'loss': 0.6818, 'grad_norm': 0.3062949776649475, 'learning_rate': 1.328766954459909e-05, 'epoch': 0.84}
{'loss': 0.6896, 'grad_norm': 0.28622162342071533, 'learning_rate': 1.3178299153189366e-05, 'epoch': 0.84}
{'loss': 0.7464, 'grad_norm': 0.27167844772338867, 'learning_rate': 1.3069348976676966e-05, 'epoch': 0.84}
{'loss': 0.6847, 'grad_norm': 0.24602168798446655, 'learning_rate': 1.2960819542378056e-05, 'epoch': 0.84}
  File "/data1/hbshim/Qwen3-VL/qwen-vl-finetune/qwenvl/train/train_qwen.py", line 206, in <module>
    train()
  File "/data1/hbshim/Qwen3-VL/qwen-vl-finetune/qwenvl/train/train_qwen.py", line 195, in train
    trainer.train()
  File "/opt/conda/lib/python3.11/site-packages/transformers/trainer.py", line 2325, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/trainer.py", line 2674, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/trainer.py", line 4020, in training_step
    loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/trainer.py", line 4110, in compute_loss
    outputs = model(**inputs)
              ^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/deepspeed/utils/nvtx.py", line 20, in wrapped_fn
    ret_val = func(*args, **kwargs)
              ^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 2265, in forward
    loss = self.module(*inputs, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1844, in _call_impl
    return inner()
           ^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1790, in inner
    result = forward_call(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/peft/peft_model.py", line 1923, in forward
    return self.base_model(
           ^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1844, in _call_impl
    return inner()
           ^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1790, in inner
    result = forward_call(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/peft/tuners/tuners_utils.py", line 311, in forward
    return self.model.forward(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/utils/generic.py", line 918, in wrapper
    output = func(self, *args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py", line 1476, in forward
    outputs = self.model(
              ^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1844, in _call_impl
    return inner()
           ^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1790, in inner
    result = forward_call(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py", line 1305, in forward
    outputs = self.language_model(
              ^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1844, in _call_impl
    return inner()
           ^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1790, in inner
    result = forward_call(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py", line 891, in forward
    layer_outputs = decoder_layer(
                    ^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/modeling_layers.py", line 93, in __call__
    return self._gradient_checkpointing_func(partial(super().__call__, **kwargs), *args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/_compile.py", line 32, in inner
    return disable_fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py", line 632, in _fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/utils/checkpoint.py", line 489, in checkpoint
    return CheckpointFunction.apply(function, preserve, *args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/autograd/function.py", line 575, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/utils/checkpoint.py", line 264, in forward
    outputs = run_function(*args)
              ^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1844, in _call_impl
    return inner()
           ^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1790, in inner
    result = forward_call(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py", line 741, in forward
    hidden_states, self_attn_weights = self.self_attn(
                                       ^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1844, in _call_impl
    return inner()
           ^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1790, in inner
    result = forward_call(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py", line 646, in forward
    key_states = self.k_proj(hidden_states)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1844, in _call_impl
    return inner()
           ^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1790, in inner
    result = forward_call(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/peft/tuners/lora/layer.py", line 807, in forward
    result = result + lora_B(lora_A(dropout(x))) * scaling
                                    ^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1844, in _call_impl
    return inner()
           ^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1803, in inner
    hook_result = hook(self, args, result)
                  ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/deepspeed/utils/nvtx.py", line 20, in wrapped_fn
    ret_val = func(*args, **kwargs)
              ^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/deepspeed/runtime/zero/parameter_offload.py", line 343, in _post_forward_module_hook
    self.post_sub_module_forward_function(module)
  File "/opt/conda/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/deepspeed/runtime/zero/parameter_offload.py", line 496, in post_sub_module_forward_function
    param_coordinator.release_sub_module(sub_module, forward=True)
  File "/opt/conda/lib/python3.11/site-packages/deepspeed/utils/nvtx.py", line 20, in wrapped_fn
    ret_val = func(*args, **kwargs)
              ^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/deepspeed/runtime/zero/partitioned_param_coordinator.py", line 437, in release_sub_module
    for param in iter_params(submodule, recurse=z3_leaf_module(submodule)):
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/deepspeed/runtime/zero/partitioned_param_coordinator.py", line 40, in iter_params
    return map(lambda pair: pair[1], get_all_parameters(module, recurse))
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/deepspeed/utils/nvtx.py", line 22, in wrapped_fn
    get_accelerator().range_pop()
  File "/opt/conda/lib/python3.11/site-packages/deepspeed/accelerator/cuda_accelerator.py", line 239, in range_pop
    return torch.cuda.nvtx.range_pop()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/cuda/nvtx.py", line 39, in range_pop
    return _nvtx.rangePop()
           ^^^^^^^^^^^^^^^^
KeyboardInterrupt
[rank0]: Traceback (most recent call last):
[rank0]:   File "/data1/hbshim/Qwen3-VL/qwen-vl-finetune/qwenvl/train/train_qwen.py", line 206, in <module>
[rank0]:     train()
[rank0]:   File "/data1/hbshim/Qwen3-VL/qwen-vl-finetune/qwenvl/train/train_qwen.py", line 195, in train
[rank0]:     trainer.train()
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/transformers/trainer.py", line 2325, in train
[rank0]:     return inner_training_loop(
[rank0]:            ^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/transformers/trainer.py", line 2674, in _inner_training_loop
[rank0]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank0]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/transformers/trainer.py", line 4020, in training_step
[rank0]:     loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/transformers/trainer.py", line 4110, in compute_loss
[rank0]:     outputs = model(**inputs)
[rank0]:               ^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/deepspeed/utils/nvtx.py", line 20, in wrapped_fn
[rank0]:     ret_val = func(*args, **kwargs)
[rank0]:               ^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/deepspeed/runtime/engine.py", line 2265, in forward
[rank0]:     loss = self.module(*inputs, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1844, in _call_impl
[rank0]:     return inner()
[rank0]:            ^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1790, in inner
[rank0]:     result = forward_call(*args, **kwargs)
[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/peft/peft_model.py", line 1923, in forward
[rank0]:     return self.base_model(
[rank0]:            ^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1844, in _call_impl
[rank0]:     return inner()
[rank0]:            ^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1790, in inner
[rank0]:     result = forward_call(*args, **kwargs)
[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/peft/tuners/tuners_utils.py", line 311, in forward
[rank0]:     return self.model.forward(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/transformers/utils/generic.py", line 918, in wrapper
[rank0]:     output = func(self, *args, **kwargs)
[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py", line 1476, in forward
[rank0]:     outputs = self.model(
[rank0]:               ^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1844, in _call_impl
[rank0]:     return inner()
[rank0]:            ^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1790, in inner
[rank0]:     result = forward_call(*args, **kwargs)
[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py", line 1305, in forward
[rank0]:     outputs = self.language_model(
[rank0]:               ^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1844, in _call_impl
[rank0]:     return inner()
[rank0]:            ^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1790, in inner
[rank0]:     result = forward_call(*args, **kwargs)
[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py", line 891, in forward
[rank0]:     layer_outputs = decoder_layer(
[rank0]:                     ^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/transformers/modeling_layers.py", line 93, in __call__
[rank0]:     return self._gradient_checkpointing_func(partial(super().__call__, **kwargs), *args)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/torch/_compile.py", line 32, in inner
[rank0]:     return disable_fn(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py", line 632, in _fn
[rank0]:     return fn(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/torch/utils/checkpoint.py", line 489, in checkpoint
[rank0]:     return CheckpointFunction.apply(function, preserve, *args)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/torch/autograd/function.py", line 575, in apply
[rank0]:     return super().apply(*args, **kwargs)  # type: ignore[misc]
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/torch/utils/checkpoint.py", line 264, in forward
[rank0]:     outputs = run_function(*args)
[rank0]:               ^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1844, in _call_impl
[rank0]:     return inner()
[rank0]:            ^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1790, in inner
[rank0]:     result = forward_call(*args, **kwargs)
[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
[rank0]:     return func(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py", line 741, in forward
[rank0]:     hidden_states, self_attn_weights = self.self_attn(
[rank0]:                                        ^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1844, in _call_impl
[rank0]:     return inner()
[rank0]:            ^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1790, in inner
[rank0]:     result = forward_call(*args, **kwargs)
[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
[rank0]:     return func(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py", line 646, in forward
[rank0]:     key_states = self.k_proj(hidden_states)
[rank0]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1844, in _call_impl
[rank0]:     return inner()
[rank0]:            ^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1790, in inner
[rank0]:     result = forward_call(*args, **kwargs)
[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/peft/tuners/lora/layer.py", line 807, in forward
[rank0]:     result = result + lora_B(lora_A(dropout(x))) * scaling
[rank0]:                                     ^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1844, in _call_impl
[rank0]:     return inner()
[rank0]:            ^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1803, in inner
[rank0]:     hook_result = hook(self, args, result)
[rank0]:                   ^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/deepspeed/utils/nvtx.py", line 20, in wrapped_fn
[rank0]:     ret_val = func(*args, **kwargs)
[rank0]:               ^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/deepspeed/runtime/zero/parameter_offload.py", line 343, in _post_forward_module_hook
[rank0]:     self.post_sub_module_forward_function(module)
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank0]:     return func(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/deepspeed/runtime/zero/parameter_offload.py", line 496, in post_sub_module_forward_function
[rank0]:     param_coordinator.release_sub_module(sub_module, forward=True)
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/deepspeed/utils/nvtx.py", line 20, in wrapped_fn
[rank0]:     ret_val = func(*args, **kwargs)
[rank0]:               ^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank0]:     return func(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/deepspeed/runtime/zero/partitioned_param_coordinator.py", line 437, in release_sub_module
[rank0]:     for param in iter_params(submodule, recurse=z3_leaf_module(submodule)):
[rank0]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/deepspeed/runtime/zero/partitioned_param_coordinator.py", line 40, in iter_params
[rank0]:     return map(lambda pair: pair[1], get_all_parameters(module, recurse))
[rank0]:                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/deepspeed/utils/nvtx.py", line 22, in wrapped_fn
[rank0]:     get_accelerator().range_pop()
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/deepspeed/accelerator/cuda_accelerator.py", line 239, in range_pop
[rank0]:     return torch.cuda.nvtx.range_pop()
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/lib/python3.11/site-packages/torch/cuda/nvtx.py", line 39, in range_pop
[rank0]:     return _nvtx.rangePop()
[rank0]:            ^^^^^^^^^^^^^^^^
[rank0]: KeyboardInterrupt
