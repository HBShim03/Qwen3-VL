{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.0,
  "eval_steps": 200,
  "global_step": 1325,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0007547881875648646,
      "grad_norm": 0.8823592066764832,
      "learning_rate": 0.0,
      "loss": 0.9136,
      "step": 1
    },
    {
      "epoch": 0.0015095763751297292,
      "grad_norm": 1.642726182937622,
      "learning_rate": 5e-06,
      "loss": 0.9473,
      "step": 2
    },
    {
      "epoch": 0.0022643645626945937,
      "grad_norm": 1.641934871673584,
      "learning_rate": 1e-05,
      "loss": 1.0053,
      "step": 3
    },
    {
      "epoch": 0.0030191527502594585,
      "grad_norm": 0.9007014036178589,
      "learning_rate": 1.5e-05,
      "loss": 0.8952,
      "step": 4
    },
    {
      "epoch": 0.003773940937824323,
      "grad_norm": 0.7431212067604065,
      "learning_rate": 2e-05,
      "loss": 0.8428,
      "step": 5
    },
    {
      "epoch": 0.004528729125389187,
      "grad_norm": 0.9500766396522522,
      "learning_rate": 2.5e-05,
      "loss": 0.9178,
      "step": 6
    },
    {
      "epoch": 0.005283517312954052,
      "grad_norm": 0.7075731158256531,
      "learning_rate": 3e-05,
      "loss": 0.8991,
      "step": 7
    },
    {
      "epoch": 0.006038305500518917,
      "grad_norm": 1.1034184694290161,
      "learning_rate": 3.5e-05,
      "loss": 0.8388,
      "step": 8
    },
    {
      "epoch": 0.006793093688083782,
      "grad_norm": 0.475452721118927,
      "learning_rate": 4e-05,
      "loss": 0.7062,
      "step": 9
    },
    {
      "epoch": 0.007547881875648646,
      "grad_norm": 0.6972818970680237,
      "learning_rate": 4.5e-05,
      "loss": 0.8285,
      "step": 10
    },
    {
      "epoch": 0.00830267006321351,
      "grad_norm": 0.5843617916107178,
      "learning_rate": 5e-05,
      "loss": 0.6109,
      "step": 11
    },
    {
      "epoch": 0.009057458250778375,
      "grad_norm": 0.5740149617195129,
      "learning_rate": 5.500000000000001e-05,
      "loss": 0.7274,
      "step": 12
    },
    {
      "epoch": 0.00981224643834324,
      "grad_norm": 0.389401912689209,
      "learning_rate": 6e-05,
      "loss": 0.8731,
      "step": 13
    },
    {
      "epoch": 0.010567034625908104,
      "grad_norm": 0.37597405910491943,
      "learning_rate": 6.500000000000001e-05,
      "loss": 0.7318,
      "step": 14
    },
    {
      "epoch": 0.011321822813472968,
      "grad_norm": 0.47433096170425415,
      "learning_rate": 7e-05,
      "loss": 0.7395,
      "step": 15
    },
    {
      "epoch": 0.012076611001037834,
      "grad_norm": 0.33056309819221497,
      "learning_rate": 7.500000000000001e-05,
      "loss": 0.7538,
      "step": 16
    },
    {
      "epoch": 0.012831399188602698,
      "grad_norm": 0.6347435712814331,
      "learning_rate": 8e-05,
      "loss": 0.7633,
      "step": 17
    },
    {
      "epoch": 0.013586187376167564,
      "grad_norm": 0.3856741786003113,
      "learning_rate": 8.5e-05,
      "loss": 0.6901,
      "step": 18
    },
    {
      "epoch": 0.014340975563732428,
      "grad_norm": 0.7780704498291016,
      "learning_rate": 9e-05,
      "loss": 0.7572,
      "step": 19
    },
    {
      "epoch": 0.015095763751297292,
      "grad_norm": 0.26749667525291443,
      "learning_rate": 9.5e-05,
      "loss": 0.6886,
      "step": 20
    },
    {
      "epoch": 0.015850551938862156,
      "grad_norm": 0.2789722681045532,
      "learning_rate": 0.0001,
      "loss": 0.7522,
      "step": 21
    },
    {
      "epoch": 0.01660534012642702,
      "grad_norm": 0.5060850381851196,
      "learning_rate": 0.000105,
      "loss": 0.7446,
      "step": 22
    },
    {
      "epoch": 0.017360128313991887,
      "grad_norm": 0.33821266889572144,
      "learning_rate": 0.00011000000000000002,
      "loss": 0.8736,
      "step": 23
    },
    {
      "epoch": 0.01811491650155675,
      "grad_norm": 0.3613731861114502,
      "learning_rate": 0.00011499999999999999,
      "loss": 0.6574,
      "step": 24
    },
    {
      "epoch": 0.018869704689121615,
      "grad_norm": 0.4367201626300812,
      "learning_rate": 0.00012,
      "loss": 0.7554,
      "step": 25
    },
    {
      "epoch": 0.01962449287668648,
      "grad_norm": 0.30760806798934937,
      "learning_rate": 0.000125,
      "loss": 0.7025,
      "step": 26
    },
    {
      "epoch": 0.020379281064251343,
      "grad_norm": 0.33561980724334717,
      "learning_rate": 0.00013000000000000002,
      "loss": 0.6614,
      "step": 27
    },
    {
      "epoch": 0.02113406925181621,
      "grad_norm": 0.4311313033103943,
      "learning_rate": 0.00013500000000000003,
      "loss": 0.6971,
      "step": 28
    },
    {
      "epoch": 0.021888857439381074,
      "grad_norm": 0.2878349721431732,
      "learning_rate": 0.00014,
      "loss": 0.7259,
      "step": 29
    },
    {
      "epoch": 0.022643645626945937,
      "grad_norm": 0.35702139139175415,
      "learning_rate": 0.000145,
      "loss": 0.7564,
      "step": 30
    },
    {
      "epoch": 0.023398433814510802,
      "grad_norm": 0.30265408754348755,
      "learning_rate": 0.00015000000000000001,
      "loss": 0.6953,
      "step": 31
    },
    {
      "epoch": 0.024153222002075668,
      "grad_norm": 0.6335870027542114,
      "learning_rate": 0.000155,
      "loss": 0.6375,
      "step": 32
    },
    {
      "epoch": 0.024908010189640534,
      "grad_norm": 0.4407832622528076,
      "learning_rate": 0.00016,
      "loss": 0.762,
      "step": 33
    },
    {
      "epoch": 0.025662798377205396,
      "grad_norm": 0.38564324378967285,
      "learning_rate": 0.000165,
      "loss": 0.7045,
      "step": 34
    },
    {
      "epoch": 0.02641758656477026,
      "grad_norm": 0.4804876148700714,
      "learning_rate": 0.00017,
      "loss": 0.8208,
      "step": 35
    },
    {
      "epoch": 0.027172374752335127,
      "grad_norm": 0.3384403586387634,
      "learning_rate": 0.000175,
      "loss": 0.8322,
      "step": 36
    },
    {
      "epoch": 0.02792716293989999,
      "grad_norm": 0.31018969416618347,
      "learning_rate": 0.00018,
      "loss": 0.6044,
      "step": 37
    },
    {
      "epoch": 0.028681951127464855,
      "grad_norm": 0.30921491980552673,
      "learning_rate": 0.00018500000000000002,
      "loss": 0.712,
      "step": 38
    },
    {
      "epoch": 0.02943673931502972,
      "grad_norm": 0.4475257396697998,
      "learning_rate": 0.00019,
      "loss": 0.7636,
      "step": 39
    },
    {
      "epoch": 0.030191527502594583,
      "grad_norm": 0.4507211148738861,
      "learning_rate": 0.000195,
      "loss": 0.7078,
      "step": 40
    },
    {
      "epoch": 0.03094631569015945,
      "grad_norm": 0.4725607633590698,
      "learning_rate": 0.0002,
      "loss": 0.8881,
      "step": 41
    },
    {
      "epoch": 0.03170110387772431,
      "grad_norm": 0.34004345536231995,
      "learning_rate": 0.00019999970114310632,
      "loss": 0.7252,
      "step": 42
    },
    {
      "epoch": 0.03245589206528918,
      "grad_norm": 0.3153171241283417,
      "learning_rate": 0.00019999880457421162,
      "loss": 0.7067,
      "step": 43
    },
    {
      "epoch": 0.03321068025285404,
      "grad_norm": 0.22517748177051544,
      "learning_rate": 0.0001999973102986748,
      "loss": 0.7168,
      "step": 44
    },
    {
      "epoch": 0.03396546844041891,
      "grad_norm": 0.3331288993358612,
      "learning_rate": 0.00019999521832542733,
      "loss": 0.6461,
      "step": 45
    },
    {
      "epoch": 0.034720256627983774,
      "grad_norm": 0.41049453616142273,
      "learning_rate": 0.00019999252866697325,
      "loss": 0.6679,
      "step": 46
    },
    {
      "epoch": 0.03547504481554864,
      "grad_norm": 0.3206935226917267,
      "learning_rate": 0.00019998924133938904,
      "loss": 0.6042,
      "step": 47
    },
    {
      "epoch": 0.0362298330031135,
      "grad_norm": 0.34653720259666443,
      "learning_rate": 0.00019998535636232346,
      "loss": 0.6303,
      "step": 48
    },
    {
      "epoch": 0.036984621190678364,
      "grad_norm": 0.3010280728340149,
      "learning_rate": 0.00019998087375899758,
      "loss": 0.6196,
      "step": 49
    },
    {
      "epoch": 0.03773940937824323,
      "grad_norm": 0.308113157749176,
      "learning_rate": 0.0001999757935562045,
      "loss": 0.7237,
      "step": 50
    },
    {
      "epoch": 0.038494197565808096,
      "grad_norm": 0.3670858144760132,
      "learning_rate": 0.00019997011578430938,
      "loss": 0.761,
      "step": 51
    },
    {
      "epoch": 0.03924898575337296,
      "grad_norm": 0.3819713592529297,
      "learning_rate": 0.00019996384047724896,
      "loss": 0.6802,
      "step": 52
    },
    {
      "epoch": 0.04000377394093783,
      "grad_norm": 0.36416247487068176,
      "learning_rate": 0.00019995696767253164,
      "loss": 0.7394,
      "step": 53
    },
    {
      "epoch": 0.040758562128502686,
      "grad_norm": 0.2088291198015213,
      "learning_rate": 0.00019994949741123716,
      "loss": 0.6272,
      "step": 54
    },
    {
      "epoch": 0.04151335031606755,
      "grad_norm": 0.2550797760486603,
      "learning_rate": 0.00019994142973801625,
      "loss": 0.5946,
      "step": 55
    },
    {
      "epoch": 0.04226813850363242,
      "grad_norm": 0.3637729585170746,
      "learning_rate": 0.00019993276470109056,
      "loss": 0.679,
      "step": 56
    },
    {
      "epoch": 0.04302292669119728,
      "grad_norm": 0.37080737948417664,
      "learning_rate": 0.00019992350235225215,
      "loss": 0.8754,
      "step": 57
    },
    {
      "epoch": 0.04377771487876215,
      "grad_norm": 0.33178114891052246,
      "learning_rate": 0.00019991364274686338,
      "loss": 0.6239,
      "step": 58
    },
    {
      "epoch": 0.044532503066327014,
      "grad_norm": 0.2114521861076355,
      "learning_rate": 0.00019990318594385648,
      "loss": 0.6087,
      "step": 59
    },
    {
      "epoch": 0.04528729125389187,
      "grad_norm": 0.4825761318206787,
      "learning_rate": 0.00019989213200573318,
      "loss": 0.7397,
      "step": 60
    },
    {
      "epoch": 0.04604207944145674,
      "grad_norm": 0.4382496476173401,
      "learning_rate": 0.00019988048099856443,
      "loss": 0.8251,
      "step": 61
    },
    {
      "epoch": 0.046796867629021605,
      "grad_norm": 0.44048553705215454,
      "learning_rate": 0.00019986823299198987,
      "loss": 0.667,
      "step": 62
    },
    {
      "epoch": 0.04755165581658647,
      "grad_norm": 0.23685874044895172,
      "learning_rate": 0.00019985538805921755,
      "loss": 0.6813,
      "step": 63
    },
    {
      "epoch": 0.048306444004151336,
      "grad_norm": 0.2989141345024109,
      "learning_rate": 0.0001998419462770234,
      "loss": 0.6725,
      "step": 64
    },
    {
      "epoch": 0.0490612321917162,
      "grad_norm": 0.34377098083496094,
      "learning_rate": 0.00019982790772575076,
      "loss": 0.6224,
      "step": 65
    },
    {
      "epoch": 0.04981602037928107,
      "grad_norm": 0.3544982075691223,
      "learning_rate": 0.00019981327248931008,
      "loss": 0.4974,
      "step": 66
    },
    {
      "epoch": 0.050570808566845926,
      "grad_norm": 0.35429951548576355,
      "learning_rate": 0.0001997980406551781,
      "loss": 0.6364,
      "step": 67
    },
    {
      "epoch": 0.05132559675441079,
      "grad_norm": 0.31863752007484436,
      "learning_rate": 0.0001997822123143976,
      "loss": 0.6463,
      "step": 68
    },
    {
      "epoch": 0.05208038494197566,
      "grad_norm": 0.2613579332828522,
      "learning_rate": 0.00019976578756157682,
      "loss": 0.7258,
      "step": 69
    },
    {
      "epoch": 0.05283517312954052,
      "grad_norm": 0.2954058051109314,
      "learning_rate": 0.00019974876649488872,
      "loss": 0.714,
      "step": 70
    },
    {
      "epoch": 0.05358996131710539,
      "grad_norm": 0.19182057678699493,
      "learning_rate": 0.00019973114921607055,
      "loss": 0.668,
      "step": 71
    },
    {
      "epoch": 0.054344749504670255,
      "grad_norm": 0.2872665524482727,
      "learning_rate": 0.00019971293583042327,
      "loss": 0.7864,
      "step": 72
    },
    {
      "epoch": 0.055099537692235113,
      "grad_norm": 0.31992822885513306,
      "learning_rate": 0.00019969412644681075,
      "loss": 0.8715,
      "step": 73
    },
    {
      "epoch": 0.05585432587979998,
      "grad_norm": 0.2771041989326477,
      "learning_rate": 0.00019967472117765928,
      "loss": 0.673,
      "step": 74
    },
    {
      "epoch": 0.056609114067364845,
      "grad_norm": 0.3785921335220337,
      "learning_rate": 0.00019965472013895686,
      "loss": 0.7632,
      "step": 75
    },
    {
      "epoch": 0.05736390225492971,
      "grad_norm": 0.2798221707344055,
      "learning_rate": 0.00019963412345025237,
      "loss": 0.716,
      "step": 76
    },
    {
      "epoch": 0.058118690442494576,
      "grad_norm": 0.2995758652687073,
      "learning_rate": 0.0001996129312346552,
      "loss": 0.8442,
      "step": 77
    },
    {
      "epoch": 0.05887347863005944,
      "grad_norm": 0.276738703250885,
      "learning_rate": 0.00019959114361883402,
      "loss": 0.5454,
      "step": 78
    },
    {
      "epoch": 0.0596282668176243,
      "grad_norm": 0.5093935132026672,
      "learning_rate": 0.00019956876073301641,
      "loss": 0.7042,
      "step": 79
    },
    {
      "epoch": 0.060383055005189167,
      "grad_norm": 0.5285617709159851,
      "learning_rate": 0.00019954578271098807,
      "loss": 0.7913,
      "step": 80
    },
    {
      "epoch": 0.06113784319275403,
      "grad_norm": 0.36493420600891113,
      "learning_rate": 0.00019952220969009175,
      "loss": 0.801,
      "step": 81
    },
    {
      "epoch": 0.0618926313803189,
      "grad_norm": 0.35064393281936646,
      "learning_rate": 0.00019949804181122662,
      "loss": 0.6501,
      "step": 82
    },
    {
      "epoch": 0.06264741956788376,
      "grad_norm": 0.2235589176416397,
      "learning_rate": 0.00019947327921884743,
      "loss": 0.6198,
      "step": 83
    },
    {
      "epoch": 0.06340220775544862,
      "grad_norm": 0.4122348129749298,
      "learning_rate": 0.00019944792206096366,
      "loss": 0.7054,
      "step": 84
    },
    {
      "epoch": 0.0641569959430135,
      "grad_norm": 0.25660645961761475,
      "learning_rate": 0.0001994219704891385,
      "loss": 0.73,
      "step": 85
    },
    {
      "epoch": 0.06491178413057835,
      "grad_norm": 0.3921108543872833,
      "learning_rate": 0.00019939542465848806,
      "loss": 0.6178,
      "step": 86
    },
    {
      "epoch": 0.06566657231814323,
      "grad_norm": 0.2418316751718521,
      "learning_rate": 0.00019936828472768043,
      "loss": 0.5668,
      "step": 87
    },
    {
      "epoch": 0.06642136050570809,
      "grad_norm": 0.32362619042396545,
      "learning_rate": 0.00019934055085893477,
      "loss": 0.6786,
      "step": 88
    },
    {
      "epoch": 0.06717614869327294,
      "grad_norm": 0.30986520648002625,
      "learning_rate": 0.00019931222321802018,
      "loss": 0.7956,
      "step": 89
    },
    {
      "epoch": 0.06793093688083782,
      "grad_norm": 0.4109676778316498,
      "learning_rate": 0.0001992833019742549,
      "loss": 0.6547,
      "step": 90
    },
    {
      "epoch": 0.06868572506840268,
      "grad_norm": 0.3359375,
      "learning_rate": 0.00019925378730050518,
      "loss": 0.616,
      "step": 91
    },
    {
      "epoch": 0.06944051325596755,
      "grad_norm": 0.40008968114852905,
      "learning_rate": 0.0001992236793731843,
      "loss": 0.704,
      "step": 92
    },
    {
      "epoch": 0.0701953014435324,
      "grad_norm": 0.43852949142456055,
      "learning_rate": 0.00019919297837225151,
      "loss": 0.6903,
      "step": 93
    },
    {
      "epoch": 0.07095008963109728,
      "grad_norm": 0.22611723840236664,
      "learning_rate": 0.00019916168448121094,
      "loss": 0.7772,
      "step": 94
    },
    {
      "epoch": 0.07170487781866214,
      "grad_norm": 0.3206667900085449,
      "learning_rate": 0.00019912979788711042,
      "loss": 0.6745,
      "step": 95
    },
    {
      "epoch": 0.072459666006227,
      "grad_norm": 0.46277180314064026,
      "learning_rate": 0.00019909731878054056,
      "loss": 0.8696,
      "step": 96
    },
    {
      "epoch": 0.07321445419379187,
      "grad_norm": 0.4287870228290558,
      "learning_rate": 0.0001990642473556335,
      "loss": 0.6767,
      "step": 97
    },
    {
      "epoch": 0.07396924238135673,
      "grad_norm": 0.28005966544151306,
      "learning_rate": 0.0001990305838100616,
      "loss": 0.7259,
      "step": 98
    },
    {
      "epoch": 0.0747240305689216,
      "grad_norm": 0.2376147210597992,
      "learning_rate": 0.00019899632834503663,
      "loss": 0.5974,
      "step": 99
    },
    {
      "epoch": 0.07547881875648646,
      "grad_norm": 0.2933925986289978,
      "learning_rate": 0.00019896148116530817,
      "loss": 0.642,
      "step": 100
    },
    {
      "epoch": 0.07623360694405133,
      "grad_norm": 0.38229891657829285,
      "learning_rate": 0.00019892604247916262,
      "loss": 0.6402,
      "step": 101
    },
    {
      "epoch": 0.07698839513161619,
      "grad_norm": 0.341551810503006,
      "learning_rate": 0.00019889001249842188,
      "loss": 0.6694,
      "step": 102
    },
    {
      "epoch": 0.07774318331918105,
      "grad_norm": 0.30395540595054626,
      "learning_rate": 0.00019885339143844215,
      "loss": 0.692,
      "step": 103
    },
    {
      "epoch": 0.07849797150674592,
      "grad_norm": 0.2941325902938843,
      "learning_rate": 0.00019881617951811252,
      "loss": 0.8613,
      "step": 104
    },
    {
      "epoch": 0.07925275969431078,
      "grad_norm": 0.3080998361110687,
      "learning_rate": 0.0001987783769598538,
      "loss": 0.5798,
      "step": 105
    },
    {
      "epoch": 0.08000754788187565,
      "grad_norm": 0.29955485463142395,
      "learning_rate": 0.00019873998398961705,
      "loss": 0.6975,
      "step": 106
    },
    {
      "epoch": 0.08076233606944051,
      "grad_norm": 0.3712860643863678,
      "learning_rate": 0.00019870100083688242,
      "loss": 0.618,
      "step": 107
    },
    {
      "epoch": 0.08151712425700537,
      "grad_norm": 0.18920089304447174,
      "learning_rate": 0.00019866142773465747,
      "loss": 0.5891,
      "step": 108
    },
    {
      "epoch": 0.08227191244457024,
      "grad_norm": 0.2550795078277588,
      "learning_rate": 0.0001986212649194762,
      "loss": 0.7276,
      "step": 109
    },
    {
      "epoch": 0.0830267006321351,
      "grad_norm": 0.2553851902484894,
      "learning_rate": 0.00019858051263139728,
      "loss": 0.7357,
      "step": 110
    },
    {
      "epoch": 0.08378148881969998,
      "grad_norm": 0.25085899233818054,
      "learning_rate": 0.00019853917111400268,
      "loss": 0.6033,
      "step": 111
    },
    {
      "epoch": 0.08453627700726483,
      "grad_norm": 0.294486403465271,
      "learning_rate": 0.00019849724061439642,
      "loss": 0.6543,
      "step": 112
    },
    {
      "epoch": 0.08529106519482971,
      "grad_norm": 0.29376140236854553,
      "learning_rate": 0.00019845472138320283,
      "loss": 0.7385,
      "step": 113
    },
    {
      "epoch": 0.08604585338239457,
      "grad_norm": 0.30178770422935486,
      "learning_rate": 0.00019841161367456525,
      "loss": 0.6033,
      "step": 114
    },
    {
      "epoch": 0.08680064156995942,
      "grad_norm": 0.2190847247838974,
      "learning_rate": 0.00019836791774614437,
      "loss": 0.6128,
      "step": 115
    },
    {
      "epoch": 0.0875554297575243,
      "grad_norm": 0.2834864556789398,
      "learning_rate": 0.00019832363385911684,
      "loss": 0.7393,
      "step": 116
    },
    {
      "epoch": 0.08831021794508916,
      "grad_norm": 0.3586079776287079,
      "learning_rate": 0.0001982787622781735,
      "loss": 0.7098,
      "step": 117
    },
    {
      "epoch": 0.08906500613265403,
      "grad_norm": 0.28676337003707886,
      "learning_rate": 0.00019823330327151798,
      "loss": 0.7809,
      "step": 118
    },
    {
      "epoch": 0.08981979432021889,
      "grad_norm": 0.2737531065940857,
      "learning_rate": 0.00019818725711086505,
      "loss": 0.6688,
      "step": 119
    },
    {
      "epoch": 0.09057458250778375,
      "grad_norm": 0.4378947913646698,
      "learning_rate": 0.00019814062407143897,
      "loss": 0.7624,
      "step": 120
    },
    {
      "epoch": 0.09132937069534862,
      "grad_norm": 0.21714790165424347,
      "learning_rate": 0.0001980934044319718,
      "loss": 0.673,
      "step": 121
    },
    {
      "epoch": 0.09208415888291348,
      "grad_norm": 0.3061344027519226,
      "learning_rate": 0.0001980455984747019,
      "loss": 0.7404,
      "step": 122
    },
    {
      "epoch": 0.09283894707047835,
      "grad_norm": 0.2839871048927307,
      "learning_rate": 0.00019799720648537195,
      "loss": 0.6696,
      "step": 123
    },
    {
      "epoch": 0.09359373525804321,
      "grad_norm": 0.32642772793769836,
      "learning_rate": 0.00019794822875322771,
      "loss": 0.6574,
      "step": 124
    },
    {
      "epoch": 0.09434852344560808,
      "grad_norm": 0.42660561203956604,
      "learning_rate": 0.0001978986655710157,
      "loss": 0.6579,
      "step": 125
    },
    {
      "epoch": 0.09510331163317294,
      "grad_norm": 0.26811227202415466,
      "learning_rate": 0.00019784851723498198,
      "loss": 0.6819,
      "step": 126
    },
    {
      "epoch": 0.0958580998207378,
      "grad_norm": 0.9889476299285889,
      "learning_rate": 0.00019779778404487,
      "loss": 0.694,
      "step": 127
    },
    {
      "epoch": 0.09661288800830267,
      "grad_norm": 0.41780486702919006,
      "learning_rate": 0.00019774646630391912,
      "loss": 0.6084,
      "step": 128
    },
    {
      "epoch": 0.09736767619586753,
      "grad_norm": 0.4480604827404022,
      "learning_rate": 0.00019769456431886244,
      "loss": 0.7525,
      "step": 129
    },
    {
      "epoch": 0.0981224643834324,
      "grad_norm": 0.2764771282672882,
      "learning_rate": 0.00019764207839992538,
      "loss": 0.6025,
      "step": 130
    },
    {
      "epoch": 0.09887725257099726,
      "grad_norm": 0.5212093591690063,
      "learning_rate": 0.00019758900886082342,
      "loss": 0.7585,
      "step": 131
    },
    {
      "epoch": 0.09963204075856213,
      "grad_norm": 0.30642473697662354,
      "learning_rate": 0.00019753535601876063,
      "loss": 0.6102,
      "step": 132
    },
    {
      "epoch": 0.100386828946127,
      "grad_norm": 0.2716676592826843,
      "learning_rate": 0.00019748112019442736,
      "loss": 0.6093,
      "step": 133
    },
    {
      "epoch": 0.10114161713369185,
      "grad_norm": 0.2719815671443939,
      "learning_rate": 0.0001974263017119986,
      "loss": 0.7293,
      "step": 134
    },
    {
      "epoch": 0.10189640532125673,
      "grad_norm": 0.3023988902568817,
      "learning_rate": 0.00019737090089913203,
      "loss": 0.797,
      "step": 135
    },
    {
      "epoch": 0.10265119350882158,
      "grad_norm": 0.24601341784000397,
      "learning_rate": 0.0001973149180869659,
      "loss": 0.5587,
      "step": 136
    },
    {
      "epoch": 0.10340598169638646,
      "grad_norm": 0.4639371335506439,
      "learning_rate": 0.00019725835361011728,
      "loss": 0.7169,
      "step": 137
    },
    {
      "epoch": 0.10416076988395132,
      "grad_norm": 0.42391127347946167,
      "learning_rate": 0.00019720120780667975,
      "loss": 0.6317,
      "step": 138
    },
    {
      "epoch": 0.10491555807151617,
      "grad_norm": 0.5134103894233704,
      "learning_rate": 0.0001971434810182217,
      "loss": 0.6597,
      "step": 139
    },
    {
      "epoch": 0.10567034625908105,
      "grad_norm": 0.45565682649612427,
      "learning_rate": 0.00019708517358978407,
      "loss": 0.6996,
      "step": 140
    },
    {
      "epoch": 0.1064251344466459,
      "grad_norm": 0.37529659271240234,
      "learning_rate": 0.00019702628586987846,
      "loss": 0.8784,
      "step": 141
    },
    {
      "epoch": 0.10717992263421078,
      "grad_norm": 0.43524169921875,
      "learning_rate": 0.00019696681821048485,
      "loss": 0.7305,
      "step": 142
    },
    {
      "epoch": 0.10793471082177564,
      "grad_norm": 0.21315518021583557,
      "learning_rate": 0.00019690677096704962,
      "loss": 0.7113,
      "step": 143
    },
    {
      "epoch": 0.10868949900934051,
      "grad_norm": 0.4599730968475342,
      "learning_rate": 0.0001968461444984835,
      "loss": 0.7566,
      "step": 144
    },
    {
      "epoch": 0.10944428719690537,
      "grad_norm": 0.4195539653301239,
      "learning_rate": 0.00019678493916715914,
      "loss": 0.5487,
      "step": 145
    },
    {
      "epoch": 0.11019907538447023,
      "grad_norm": 0.23062950372695923,
      "learning_rate": 0.00019672315533890932,
      "loss": 0.5628,
      "step": 146
    },
    {
      "epoch": 0.1109538635720351,
      "grad_norm": 0.3901035785675049,
      "learning_rate": 0.0001966607933830245,
      "loss": 0.6518,
      "step": 147
    },
    {
      "epoch": 0.11170865175959996,
      "grad_norm": 0.20518217980861664,
      "learning_rate": 0.0001965978536722507,
      "loss": 0.6153,
      "step": 148
    },
    {
      "epoch": 0.11246343994716483,
      "grad_norm": 0.3409348130226135,
      "learning_rate": 0.00019653433658278717,
      "loss": 0.5917,
      "step": 149
    },
    {
      "epoch": 0.11321822813472969,
      "grad_norm": 0.26337701082229614,
      "learning_rate": 0.00019647024249428439,
      "loss": 0.5443,
      "step": 150
    },
    {
      "epoch": 0.11397301632229456,
      "grad_norm": 0.4579305648803711,
      "learning_rate": 0.00019640557178984152,
      "loss": 0.6502,
      "step": 151
    },
    {
      "epoch": 0.11472780450985942,
      "grad_norm": 0.3901978135108948,
      "learning_rate": 0.0001963403248560043,
      "loss": 0.5473,
      "step": 152
    },
    {
      "epoch": 0.11548259269742428,
      "grad_norm": 0.29236140847206116,
      "learning_rate": 0.00019627450208276265,
      "loss": 0.8047,
      "step": 153
    },
    {
      "epoch": 0.11623738088498915,
      "grad_norm": 0.38311052322387695,
      "learning_rate": 0.00019620810386354837,
      "loss": 0.6741,
      "step": 154
    },
    {
      "epoch": 0.11699216907255401,
      "grad_norm": 0.44901713728904724,
      "learning_rate": 0.00019614113059523273,
      "loss": 0.637,
      "step": 155
    },
    {
      "epoch": 0.11774695726011888,
      "grad_norm": 0.35317760705947876,
      "learning_rate": 0.00019607358267812418,
      "loss": 0.6994,
      "step": 156
    },
    {
      "epoch": 0.11850174544768374,
      "grad_norm": 0.28513699769973755,
      "learning_rate": 0.000196005460515966,
      "loss": 0.7716,
      "step": 157
    },
    {
      "epoch": 0.1192565336352486,
      "grad_norm": 0.2733972668647766,
      "learning_rate": 0.00019593676451593376,
      "loss": 0.655,
      "step": 158
    },
    {
      "epoch": 0.12001132182281347,
      "grad_norm": 0.41872259974479675,
      "learning_rate": 0.00019586749508863284,
      "loss": 0.8666,
      "step": 159
    },
    {
      "epoch": 0.12076611001037833,
      "grad_norm": 0.35546740889549255,
      "learning_rate": 0.00019579765264809615,
      "loss": 0.6548,
      "step": 160
    },
    {
      "epoch": 0.1215208981979432,
      "grad_norm": 0.4072185158729553,
      "learning_rate": 0.00019572723761178166,
      "loss": 0.6775,
      "step": 161
    },
    {
      "epoch": 0.12227568638550806,
      "grad_norm": 0.2985815107822418,
      "learning_rate": 0.00019565625040056974,
      "loss": 0.7853,
      "step": 162
    },
    {
      "epoch": 0.12303047457307294,
      "grad_norm": 0.2687723636627197,
      "learning_rate": 0.0001955846914387607,
      "loss": 0.5818,
      "step": 163
    },
    {
      "epoch": 0.1237852627606378,
      "grad_norm": 0.3940920829772949,
      "learning_rate": 0.00019551256115407233,
      "loss": 0.731,
      "step": 164
    },
    {
      "epoch": 0.12454005094820265,
      "grad_norm": 0.3881567120552063,
      "learning_rate": 0.0001954398599776373,
      "loss": 0.5537,
      "step": 165
    },
    {
      "epoch": 0.12529483913576753,
      "grad_norm": 0.2047734260559082,
      "learning_rate": 0.00019536658834400057,
      "loss": 0.6474,
      "step": 166
    },
    {
      "epoch": 0.12604962732333239,
      "grad_norm": 0.25426235795021057,
      "learning_rate": 0.0001952927466911168,
      "loss": 0.7266,
      "step": 167
    },
    {
      "epoch": 0.12680441551089724,
      "grad_norm": 0.33263349533081055,
      "learning_rate": 0.00019521833546034772,
      "loss": 0.7208,
      "step": 168
    },
    {
      "epoch": 0.12755920369846213,
      "grad_norm": 0.3126710057258606,
      "learning_rate": 0.00019514335509645948,
      "loss": 0.5744,
      "step": 169
    },
    {
      "epoch": 0.128313991886027,
      "grad_norm": 0.22363238036632538,
      "learning_rate": 0.00019506780604762008,
      "loss": 0.7257,
      "step": 170
    },
    {
      "epoch": 0.12906878007359185,
      "grad_norm": 0.34322279691696167,
      "learning_rate": 0.00019499168876539665,
      "loss": 0.5922,
      "step": 171
    },
    {
      "epoch": 0.1298235682611567,
      "grad_norm": 0.42941129207611084,
      "learning_rate": 0.0001949150037047526,
      "loss": 0.5655,
      "step": 172
    },
    {
      "epoch": 0.13057835644872157,
      "grad_norm": 0.3786267638206482,
      "learning_rate": 0.0001948377513240452,
      "loss": 0.6006,
      "step": 173
    },
    {
      "epoch": 0.13133314463628645,
      "grad_norm": 0.33901646733283997,
      "learning_rate": 0.00019475993208502247,
      "loss": 0.583,
      "step": 174
    },
    {
      "epoch": 0.1320879328238513,
      "grad_norm": 0.420240581035614,
      "learning_rate": 0.0001946815464528208,
      "loss": 0.6458,
      "step": 175
    },
    {
      "epoch": 0.13284272101141617,
      "grad_norm": 0.3275746703147888,
      "learning_rate": 0.0001946025948959619,
      "loss": 0.7486,
      "step": 176
    },
    {
      "epoch": 0.13359750919898103,
      "grad_norm": 0.32880279421806335,
      "learning_rate": 0.00019452307788635014,
      "loss": 0.6397,
      "step": 177
    },
    {
      "epoch": 0.1343522973865459,
      "grad_norm": 0.30060914158821106,
      "learning_rate": 0.00019444299589926965,
      "loss": 0.7615,
      "step": 178
    },
    {
      "epoch": 0.13510708557411077,
      "grad_norm": 0.5487265586853027,
      "learning_rate": 0.00019436234941338145,
      "loss": 0.6723,
      "step": 179
    },
    {
      "epoch": 0.13586187376167563,
      "grad_norm": 0.3416660726070404,
      "learning_rate": 0.00019428113891072077,
      "loss": 0.6651,
      "step": 180
    },
    {
      "epoch": 0.1366166619492405,
      "grad_norm": 0.21141138672828674,
      "learning_rate": 0.00019419936487669395,
      "loss": 0.6874,
      "step": 181
    },
    {
      "epoch": 0.13737145013680535,
      "grad_norm": 0.3268417716026306,
      "learning_rate": 0.0001941170278000757,
      "loss": 0.6644,
      "step": 182
    },
    {
      "epoch": 0.13812623832437024,
      "grad_norm": 0.28593721985816956,
      "learning_rate": 0.000194034128173006,
      "loss": 0.7146,
      "step": 183
    },
    {
      "epoch": 0.1388810265119351,
      "grad_norm": 0.2519207298755646,
      "learning_rate": 0.0001939506664909874,
      "loss": 0.814,
      "step": 184
    },
    {
      "epoch": 0.13963581469949995,
      "grad_norm": 0.3746621608734131,
      "learning_rate": 0.0001938666432528819,
      "loss": 0.7398,
      "step": 185
    },
    {
      "epoch": 0.1403906028870648,
      "grad_norm": 0.3600379526615143,
      "learning_rate": 0.00019378205896090794,
      "loss": 0.6306,
      "step": 186
    },
    {
      "epoch": 0.14114539107462967,
      "grad_norm": 0.28264644742012024,
      "learning_rate": 0.00019369691412063754,
      "loss": 0.7061,
      "step": 187
    },
    {
      "epoch": 0.14190017926219456,
      "grad_norm": 0.358509361743927,
      "learning_rate": 0.00019361120924099313,
      "loss": 0.6409,
      "step": 188
    },
    {
      "epoch": 0.14265496744975942,
      "grad_norm": 0.22825415432453156,
      "learning_rate": 0.00019352494483424457,
      "loss": 0.7078,
      "step": 189
    },
    {
      "epoch": 0.14340975563732428,
      "grad_norm": 0.2554618716239929,
      "learning_rate": 0.00019343812141600611,
      "loss": 0.7803,
      "step": 190
    },
    {
      "epoch": 0.14416454382488914,
      "grad_norm": 0.27842429280281067,
      "learning_rate": 0.00019335073950523335,
      "loss": 0.6622,
      "step": 191
    },
    {
      "epoch": 0.144919332012454,
      "grad_norm": 0.40653809905052185,
      "learning_rate": 0.00019326279962421995,
      "loss": 0.7797,
      "step": 192
    },
    {
      "epoch": 0.14567412020001888,
      "grad_norm": 0.30867183208465576,
      "learning_rate": 0.00019317430229859474,
      "loss": 0.6799,
      "step": 193
    },
    {
      "epoch": 0.14642890838758374,
      "grad_norm": 0.33473870158195496,
      "learning_rate": 0.00019308524805731843,
      "loss": 0.5984,
      "step": 194
    },
    {
      "epoch": 0.1471836965751486,
      "grad_norm": 0.4197801649570465,
      "learning_rate": 0.0001929956374326805,
      "loss": 0.6299,
      "step": 195
    },
    {
      "epoch": 0.14793848476271346,
      "grad_norm": 0.3883046507835388,
      "learning_rate": 0.000192905470960296,
      "loss": 0.6834,
      "step": 196
    },
    {
      "epoch": 0.14869327295027832,
      "grad_norm": 0.22020788490772247,
      "learning_rate": 0.00019281474917910238,
      "loss": 0.6345,
      "step": 197
    },
    {
      "epoch": 0.1494480611378432,
      "grad_norm": 0.3509766757488251,
      "learning_rate": 0.00019272347263135623,
      "loss": 0.8263,
      "step": 198
    },
    {
      "epoch": 0.15020284932540806,
      "grad_norm": 0.5603117942810059,
      "learning_rate": 0.00019263164186263004,
      "loss": 0.6482,
      "step": 199
    },
    {
      "epoch": 0.15095763751297292,
      "grad_norm": 0.44352757930755615,
      "learning_rate": 0.000192539257421809,
      "loss": 0.7511,
      "step": 200
    },
    {
      "epoch": 0.15095763751297292,
      "eval_loss": 0.6794905066490173,
      "eval_runtime": 3174.3951,
      "eval_samples_per_second": 2.968,
      "eval_steps_per_second": 0.742,
      "step": 200
    },
    {
      "epoch": 0.15171242570053778,
      "grad_norm": 0.34596073627471924,
      "learning_rate": 0.00019244631986108766,
      "loss": 0.7065,
      "step": 201
    },
    {
      "epoch": 0.15246721388810267,
      "grad_norm": 0.3751802146434784,
      "learning_rate": 0.0001923528297359666,
      "loss": 0.6269,
      "step": 202
    },
    {
      "epoch": 0.15322200207566752,
      "grad_norm": 0.5476672649383545,
      "learning_rate": 0.0001922587876052492,
      "loss": 0.7441,
      "step": 203
    },
    {
      "epoch": 0.15397679026323238,
      "grad_norm": 0.32258132100105286,
      "learning_rate": 0.00019216419403103824,
      "loss": 0.7384,
      "step": 204
    },
    {
      "epoch": 0.15473157845079724,
      "grad_norm": 0.21251769363880157,
      "learning_rate": 0.0001920690495787326,
      "loss": 0.5287,
      "step": 205
    },
    {
      "epoch": 0.1554863666383621,
      "grad_norm": 0.33415278792381287,
      "learning_rate": 0.00019197335481702373,
      "loss": 0.6015,
      "step": 206
    },
    {
      "epoch": 0.156241154825927,
      "grad_norm": 0.3216487765312195,
      "learning_rate": 0.00019187711031789243,
      "loss": 0.6988,
      "step": 207
    },
    {
      "epoch": 0.15699594301349185,
      "grad_norm": 0.22597168385982513,
      "learning_rate": 0.00019178031665660533,
      "loss": 0.7085,
      "step": 208
    },
    {
      "epoch": 0.1577507312010567,
      "grad_norm": 0.267799973487854,
      "learning_rate": 0.00019168297441171151,
      "loss": 0.604,
      "step": 209
    },
    {
      "epoch": 0.15850551938862156,
      "grad_norm": 0.3644607365131378,
      "learning_rate": 0.00019158508416503898,
      "loss": 0.6409,
      "step": 210
    },
    {
      "epoch": 0.15926030757618642,
      "grad_norm": 0.22939035296440125,
      "learning_rate": 0.00019148664650169125,
      "loss": 0.6097,
      "step": 211
    },
    {
      "epoch": 0.1600150957637513,
      "grad_norm": 0.2865079343318939,
      "learning_rate": 0.00019138766201004385,
      "loss": 0.5769,
      "step": 212
    },
    {
      "epoch": 0.16076988395131617,
      "grad_norm": 0.302094429731369,
      "learning_rate": 0.00019128813128174062,
      "loss": 0.645,
      "step": 213
    },
    {
      "epoch": 0.16152467213888103,
      "grad_norm": 0.312997430562973,
      "learning_rate": 0.00019118805491169052,
      "loss": 0.778,
      "step": 214
    },
    {
      "epoch": 0.16227946032644588,
      "grad_norm": 0.4161328971385956,
      "learning_rate": 0.00019108743349806383,
      "loss": 0.7063,
      "step": 215
    },
    {
      "epoch": 0.16303424851401074,
      "grad_norm": 0.5003939270973206,
      "learning_rate": 0.00019098626764228852,
      "loss": 0.6168,
      "step": 216
    },
    {
      "epoch": 0.16378903670157563,
      "grad_norm": 0.3498615026473999,
      "learning_rate": 0.000190884557949047,
      "loss": 0.7692,
      "step": 217
    },
    {
      "epoch": 0.1645438248891405,
      "grad_norm": 0.3049536943435669,
      "learning_rate": 0.000190782305026272,
      "loss": 0.7467,
      "step": 218
    },
    {
      "epoch": 0.16529861307670535,
      "grad_norm": 0.26575079560279846,
      "learning_rate": 0.00019067950948514341,
      "loss": 0.7013,
      "step": 219
    },
    {
      "epoch": 0.1660534012642702,
      "grad_norm": 0.3005254566669464,
      "learning_rate": 0.00019057617194008432,
      "loss": 0.7197,
      "step": 220
    },
    {
      "epoch": 0.16680818945183506,
      "grad_norm": 0.2785617411136627,
      "learning_rate": 0.00019047229300875752,
      "loss": 0.6443,
      "step": 221
    },
    {
      "epoch": 0.16756297763939995,
      "grad_norm": 0.2824345529079437,
      "learning_rate": 0.00019036787331206165,
      "loss": 0.5902,
      "step": 222
    },
    {
      "epoch": 0.1683177658269648,
      "grad_norm": 0.392140656709671,
      "learning_rate": 0.00019026291347412765,
      "loss": 0.6028,
      "step": 223
    },
    {
      "epoch": 0.16907255401452967,
      "grad_norm": 0.3040975034236908,
      "learning_rate": 0.000190157414122315,
      "loss": 0.723,
      "step": 224
    },
    {
      "epoch": 0.16982734220209453,
      "grad_norm": 0.28929564356803894,
      "learning_rate": 0.00019005137588720778,
      "loss": 0.7261,
      "step": 225
    },
    {
      "epoch": 0.17058213038965941,
      "grad_norm": 0.39628052711486816,
      "learning_rate": 0.00018994479940261124,
      "loss": 0.6469,
      "step": 226
    },
    {
      "epoch": 0.17133691857722427,
      "grad_norm": 0.45476675033569336,
      "learning_rate": 0.00018983768530554765,
      "loss": 0.8436,
      "step": 227
    },
    {
      "epoch": 0.17209170676478913,
      "grad_norm": 0.34851524233818054,
      "learning_rate": 0.00018973003423625274,
      "loss": 0.6805,
      "step": 228
    },
    {
      "epoch": 0.172846494952354,
      "grad_norm": 0.8829070925712585,
      "learning_rate": 0.0001896218468381718,
      "loss": 0.8139,
      "step": 229
    },
    {
      "epoch": 0.17360128313991885,
      "grad_norm": 0.33831629157066345,
      "learning_rate": 0.00018951312375795586,
      "loss": 0.7545,
      "step": 230
    },
    {
      "epoch": 0.17435607132748374,
      "grad_norm": 0.3012145757675171,
      "learning_rate": 0.00018940386564545773,
      "loss": 0.5629,
      "step": 231
    },
    {
      "epoch": 0.1751108595150486,
      "grad_norm": 0.4154110848903656,
      "learning_rate": 0.00018929407315372823,
      "loss": 0.7462,
      "step": 232
    },
    {
      "epoch": 0.17586564770261345,
      "grad_norm": 0.31389981508255005,
      "learning_rate": 0.0001891837469390122,
      "loss": 0.7248,
      "step": 233
    },
    {
      "epoch": 0.1766204358901783,
      "grad_norm": 0.3273885250091553,
      "learning_rate": 0.00018907288766074464,
      "loss": 0.5042,
      "step": 234
    },
    {
      "epoch": 0.17737522407774317,
      "grad_norm": 0.31653034687042236,
      "learning_rate": 0.00018896149598154676,
      "loss": 0.7507,
      "step": 235
    },
    {
      "epoch": 0.17813001226530806,
      "grad_norm": 0.2567409873008728,
      "learning_rate": 0.00018884957256722195,
      "loss": 0.5516,
      "step": 236
    },
    {
      "epoch": 0.17888480045287292,
      "grad_norm": 0.3265131711959839,
      "learning_rate": 0.0001887371180867519,
      "loss": 0.724,
      "step": 237
    },
    {
      "epoch": 0.17963958864043778,
      "grad_norm": 0.3561280071735382,
      "learning_rate": 0.00018862413321229257,
      "loss": 0.6527,
      "step": 238
    },
    {
      "epoch": 0.18039437682800263,
      "grad_norm": 0.3154006004333496,
      "learning_rate": 0.00018851061861917013,
      "loss": 0.7331,
      "step": 239
    },
    {
      "epoch": 0.1811491650155675,
      "grad_norm": 0.39442533254623413,
      "learning_rate": 0.0001883965749858769,
      "loss": 0.6254,
      "step": 240
    },
    {
      "epoch": 0.18190395320313238,
      "grad_norm": 0.31928640604019165,
      "learning_rate": 0.00018828200299406746,
      "loss": 0.6006,
      "step": 241
    },
    {
      "epoch": 0.18265874139069724,
      "grad_norm": 0.2486024796962738,
      "learning_rate": 0.00018816690332855437,
      "loss": 0.5127,
      "step": 242
    },
    {
      "epoch": 0.1834135295782621,
      "grad_norm": 0.5027188062667847,
      "learning_rate": 0.00018805127667730426,
      "loss": 0.7967,
      "step": 243
    },
    {
      "epoch": 0.18416831776582696,
      "grad_norm": 0.3381574749946594,
      "learning_rate": 0.00018793512373143346,
      "loss": 0.699,
      "step": 244
    },
    {
      "epoch": 0.18492310595339184,
      "grad_norm": 0.32686078548431396,
      "learning_rate": 0.0001878184451852042,
      "loss": 0.7135,
      "step": 245
    },
    {
      "epoch": 0.1856778941409567,
      "grad_norm": 0.21698115766048431,
      "learning_rate": 0.00018770124173602025,
      "loss": 0.6209,
      "step": 246
    },
    {
      "epoch": 0.18643268232852156,
      "grad_norm": 0.24695181846618652,
      "learning_rate": 0.00018758351408442277,
      "loss": 0.73,
      "step": 247
    },
    {
      "epoch": 0.18718747051608642,
      "grad_norm": 0.3362651765346527,
      "learning_rate": 0.0001874652629340862,
      "loss": 0.7089,
      "step": 248
    },
    {
      "epoch": 0.18794225870365128,
      "grad_norm": 0.37460288405418396,
      "learning_rate": 0.00018734648899181388,
      "loss": 0.6923,
      "step": 249
    },
    {
      "epoch": 0.18869704689121616,
      "grad_norm": 0.4275560677051544,
      "learning_rate": 0.00018722719296753414,
      "loss": 0.6437,
      "step": 250
    },
    {
      "epoch": 0.18945183507878102,
      "grad_norm": 0.254033625125885,
      "learning_rate": 0.00018710737557429567,
      "loss": 0.7156,
      "step": 251
    },
    {
      "epoch": 0.19020662326634588,
      "grad_norm": 0.2965236008167267,
      "learning_rate": 0.00018698703752826362,
      "loss": 0.7244,
      "step": 252
    },
    {
      "epoch": 0.19096141145391074,
      "grad_norm": 0.2673816680908203,
      "learning_rate": 0.00018686617954871508,
      "loss": 0.5498,
      "step": 253
    },
    {
      "epoch": 0.1917161996414756,
      "grad_norm": 0.2932083010673523,
      "learning_rate": 0.00018674480235803483,
      "loss": 0.6089,
      "step": 254
    },
    {
      "epoch": 0.19247098782904049,
      "grad_norm": 0.30117106437683105,
      "learning_rate": 0.00018662290668171108,
      "loss": 0.8056,
      "step": 255
    },
    {
      "epoch": 0.19322577601660534,
      "grad_norm": 0.2237786501646042,
      "learning_rate": 0.00018650049324833105,
      "loss": 0.7103,
      "step": 256
    },
    {
      "epoch": 0.1939805642041702,
      "grad_norm": 0.2463475614786148,
      "learning_rate": 0.00018637756278957682,
      "loss": 0.5183,
      "step": 257
    },
    {
      "epoch": 0.19473535239173506,
      "grad_norm": 0.2965491712093353,
      "learning_rate": 0.00018625411604022057,
      "loss": 0.6843,
      "step": 258
    },
    {
      "epoch": 0.19549014057929992,
      "grad_norm": 0.2884419560432434,
      "learning_rate": 0.00018613015373812063,
      "loss": 0.6878,
      "step": 259
    },
    {
      "epoch": 0.1962449287668648,
      "grad_norm": 0.31829413771629333,
      "learning_rate": 0.00018600567662421672,
      "loss": 0.6507,
      "step": 260
    },
    {
      "epoch": 0.19699971695442967,
      "grad_norm": 0.30075740814208984,
      "learning_rate": 0.00018588068544252574,
      "loss": 0.6402,
      "step": 261
    },
    {
      "epoch": 0.19775450514199452,
      "grad_norm": 0.41655394434928894,
      "learning_rate": 0.0001857551809401372,
      "loss": 0.6831,
      "step": 262
    },
    {
      "epoch": 0.19850929332955938,
      "grad_norm": 0.3430669605731964,
      "learning_rate": 0.00018562916386720882,
      "loss": 0.6898,
      "step": 263
    },
    {
      "epoch": 0.19926408151712427,
      "grad_norm": 0.4906931221485138,
      "learning_rate": 0.000185502634976962,
      "loss": 0.7298,
      "step": 264
    },
    {
      "epoch": 0.20001886970468913,
      "grad_norm": 0.3162873685359955,
      "learning_rate": 0.00018537559502567738,
      "loss": 0.7151,
      "step": 265
    },
    {
      "epoch": 0.200773657892254,
      "grad_norm": 0.33731403946876526,
      "learning_rate": 0.0001852480447726903,
      "loss": 0.7529,
      "step": 266
    },
    {
      "epoch": 0.20152844607981885,
      "grad_norm": 0.32586508989334106,
      "learning_rate": 0.00018511998498038613,
      "loss": 0.7416,
      "step": 267
    },
    {
      "epoch": 0.2022832342673837,
      "grad_norm": 0.32427093386650085,
      "learning_rate": 0.00018499141641419598,
      "loss": 0.6048,
      "step": 268
    },
    {
      "epoch": 0.2030380224549486,
      "grad_norm": 0.4141908884048462,
      "learning_rate": 0.00018486233984259186,
      "loss": 0.7379,
      "step": 269
    },
    {
      "epoch": 0.20379281064251345,
      "grad_norm": 0.45414382219314575,
      "learning_rate": 0.0001847327560370822,
      "loss": 0.6751,
      "step": 270
    },
    {
      "epoch": 0.2045475988300783,
      "grad_norm": 0.3885006010532379,
      "learning_rate": 0.00018460266577220732,
      "loss": 0.5995,
      "step": 271
    },
    {
      "epoch": 0.20530238701764317,
      "grad_norm": 0.21765853464603424,
      "learning_rate": 0.00018447206982553466,
      "loss": 0.6598,
      "step": 272
    },
    {
      "epoch": 0.20605717520520803,
      "grad_norm": 0.22797980904579163,
      "learning_rate": 0.0001843409689776542,
      "loss": 0.6549,
      "step": 273
    },
    {
      "epoch": 0.2068119633927729,
      "grad_norm": 0.27839913964271545,
      "learning_rate": 0.00018420936401217382,
      "loss": 0.7903,
      "step": 274
    },
    {
      "epoch": 0.20756675158033777,
      "grad_norm": 0.3231392800807953,
      "learning_rate": 0.00018407725571571447,
      "loss": 0.7126,
      "step": 275
    },
    {
      "epoch": 0.20832153976790263,
      "grad_norm": 0.313760906457901,
      "learning_rate": 0.00018394464487790567,
      "loss": 0.6776,
      "step": 276
    },
    {
      "epoch": 0.2090763279554675,
      "grad_norm": 0.4140918552875519,
      "learning_rate": 0.0001838115322913807,
      "loss": 0.6756,
      "step": 277
    },
    {
      "epoch": 0.20983111614303235,
      "grad_norm": 0.27158915996551514,
      "learning_rate": 0.00018367791875177188,
      "loss": 0.748,
      "step": 278
    },
    {
      "epoch": 0.21058590433059723,
      "grad_norm": 0.28893473744392395,
      "learning_rate": 0.00018354380505770568,
      "loss": 0.6567,
      "step": 279
    },
    {
      "epoch": 0.2113406925181621,
      "grad_norm": 0.41130170226097107,
      "learning_rate": 0.00018340919201079818,
      "loss": 0.6017,
      "step": 280
    },
    {
      "epoch": 0.21209548070572695,
      "grad_norm": 0.2319715917110443,
      "learning_rate": 0.00018327408041565014,
      "loss": 0.6061,
      "step": 281
    },
    {
      "epoch": 0.2128502688932918,
      "grad_norm": 0.376864492893219,
      "learning_rate": 0.00018313847107984213,
      "loss": 0.6515,
      "step": 282
    },
    {
      "epoch": 0.2136050570808567,
      "grad_norm": 0.34840720891952515,
      "learning_rate": 0.00018300236481392994,
      "loss": 0.5741,
      "step": 283
    },
    {
      "epoch": 0.21435984526842156,
      "grad_norm": 0.301894873380661,
      "learning_rate": 0.0001828657624314394,
      "loss": 0.716,
      "step": 284
    },
    {
      "epoch": 0.21511463345598641,
      "grad_norm": 0.3022066056728363,
      "learning_rate": 0.00018272866474886183,
      "loss": 0.5548,
      "step": 285
    },
    {
      "epoch": 0.21586942164355127,
      "grad_norm": 0.395130455493927,
      "learning_rate": 0.00018259107258564896,
      "loss": 0.723,
      "step": 286
    },
    {
      "epoch": 0.21662420983111613,
      "grad_norm": 0.37950971722602844,
      "learning_rate": 0.00018245298676420813,
      "loss": 0.7897,
      "step": 287
    },
    {
      "epoch": 0.21737899801868102,
      "grad_norm": 0.31151631474494934,
      "learning_rate": 0.00018231440810989735,
      "loss": 0.6465,
      "step": 288
    },
    {
      "epoch": 0.21813378620624588,
      "grad_norm": 0.288680762052536,
      "learning_rate": 0.0001821753374510203,
      "loss": 0.5773,
      "step": 289
    },
    {
      "epoch": 0.21888857439381074,
      "grad_norm": 0.26681849360466003,
      "learning_rate": 0.00018203577561882152,
      "loss": 0.7482,
      "step": 290
    },
    {
      "epoch": 0.2196433625813756,
      "grad_norm": 0.3379916846752167,
      "learning_rate": 0.00018189572344748132,
      "loss": 0.8919,
      "step": 291
    },
    {
      "epoch": 0.22039815076894045,
      "grad_norm": 0.356862872838974,
      "learning_rate": 0.00018175518177411082,
      "loss": 0.6058,
      "step": 292
    },
    {
      "epoch": 0.22115293895650534,
      "grad_norm": 0.2890401780605316,
      "learning_rate": 0.00018161415143874699,
      "loss": 0.6524,
      "step": 293
    },
    {
      "epoch": 0.2219077271440702,
      "grad_norm": 0.41182008385658264,
      "learning_rate": 0.00018147263328434758,
      "loss": 0.8752,
      "step": 294
    },
    {
      "epoch": 0.22266251533163506,
      "grad_norm": 0.3726820945739746,
      "learning_rate": 0.0001813306281567861,
      "loss": 0.6892,
      "step": 295
    },
    {
      "epoch": 0.22341730351919992,
      "grad_norm": 0.4811568856239319,
      "learning_rate": 0.00018118813690484685,
      "loss": 0.5969,
      "step": 296
    },
    {
      "epoch": 0.22417209170676478,
      "grad_norm": 0.3516576588153839,
      "learning_rate": 0.0001810451603802196,
      "loss": 0.6601,
      "step": 297
    },
    {
      "epoch": 0.22492687989432966,
      "grad_norm": 0.3037455976009369,
      "learning_rate": 0.00018090169943749476,
      "loss": 0.7366,
      "step": 298
    },
    {
      "epoch": 0.22568166808189452,
      "grad_norm": 0.29319751262664795,
      "learning_rate": 0.0001807577549341582,
      "loss": 0.7247,
      "step": 299
    },
    {
      "epoch": 0.22643645626945938,
      "grad_norm": 0.38595646619796753,
      "learning_rate": 0.00018061332773058604,
      "loss": 0.7748,
      "step": 300
    },
    {
      "epoch": 0.22719124445702424,
      "grad_norm": 0.5518132448196411,
      "learning_rate": 0.0001804684186900396,
      "loss": 0.7177,
      "step": 301
    },
    {
      "epoch": 0.22794603264458912,
      "grad_norm": 0.1819998323917389,
      "learning_rate": 0.0001803230286786602,
      "loss": 0.664,
      "step": 302
    },
    {
      "epoch": 0.22870082083215398,
      "grad_norm": 0.21313196420669556,
      "learning_rate": 0.000180177158565464,
      "loss": 0.6885,
      "step": 303
    },
    {
      "epoch": 0.22945560901971884,
      "grad_norm": 0.31841692328453064,
      "learning_rate": 0.0001800308092223367,
      "loss": 0.6281,
      "step": 304
    },
    {
      "epoch": 0.2302103972072837,
      "grad_norm": 0.2982133626937866,
      "learning_rate": 0.00017988398152402857,
      "loss": 0.7052,
      "step": 305
    },
    {
      "epoch": 0.23096518539484856,
      "grad_norm": 0.27477920055389404,
      "learning_rate": 0.00017973667634814898,
      "loss": 0.696,
      "step": 306
    },
    {
      "epoch": 0.23171997358241345,
      "grad_norm": 0.19482381641864777,
      "learning_rate": 0.00017958889457516132,
      "loss": 0.6604,
      "step": 307
    },
    {
      "epoch": 0.2324747617699783,
      "grad_norm": 0.2530665099620819,
      "learning_rate": 0.00017944063708837758,
      "loss": 0.6132,
      "step": 308
    },
    {
      "epoch": 0.23322954995754316,
      "grad_norm": 0.2836398184299469,
      "learning_rate": 0.0001792919047739532,
      "loss": 0.7657,
      "step": 309
    },
    {
      "epoch": 0.23398433814510802,
      "grad_norm": 0.21867422759532928,
      "learning_rate": 0.00017914269852088173,
      "loss": 0.7093,
      "step": 310
    },
    {
      "epoch": 0.23473912633267288,
      "grad_norm": 0.3810270428657532,
      "learning_rate": 0.00017899301922098956,
      "loss": 0.6104,
      "step": 311
    },
    {
      "epoch": 0.23549391452023777,
      "grad_norm": 0.37887415289878845,
      "learning_rate": 0.00017884286776893044,
      "loss": 0.586,
      "step": 312
    },
    {
      "epoch": 0.23624870270780263,
      "grad_norm": 0.38885000348091125,
      "learning_rate": 0.0001786922450621803,
      "loss": 0.628,
      "step": 313
    },
    {
      "epoch": 0.23700349089536749,
      "grad_norm": 0.3270227313041687,
      "learning_rate": 0.0001785411520010319,
      "loss": 0.7586,
      "step": 314
    },
    {
      "epoch": 0.23775827908293234,
      "grad_norm": 0.3204311728477478,
      "learning_rate": 0.00017838958948858922,
      "loss": 0.7273,
      "step": 315
    },
    {
      "epoch": 0.2385130672704972,
      "grad_norm": 0.2161773443222046,
      "learning_rate": 0.00017823755843076233,
      "loss": 0.7461,
      "step": 316
    },
    {
      "epoch": 0.2392678554580621,
      "grad_norm": 0.23664119839668274,
      "learning_rate": 0.0001780850597362618,
      "loss": 0.6722,
      "step": 317
    },
    {
      "epoch": 0.24002264364562695,
      "grad_norm": 0.2905615568161011,
      "learning_rate": 0.0001779320943165934,
      "loss": 0.6524,
      "step": 318
    },
    {
      "epoch": 0.2407774318331918,
      "grad_norm": 0.2318246215581894,
      "learning_rate": 0.0001777786630860525,
      "loss": 0.629,
      "step": 319
    },
    {
      "epoch": 0.24153222002075667,
      "grad_norm": 0.2641376852989197,
      "learning_rate": 0.00017762476696171872,
      "loss": 0.6247,
      "step": 320
    },
    {
      "epoch": 0.24228700820832155,
      "grad_norm": 0.36179983615875244,
      "learning_rate": 0.0001774704068634504,
      "loss": 0.6875,
      "step": 321
    },
    {
      "epoch": 0.2430417963958864,
      "grad_norm": 0.3244355618953705,
      "learning_rate": 0.00017731558371387914,
      "loss": 0.7249,
      "step": 322
    },
    {
      "epoch": 0.24379658458345127,
      "grad_norm": 0.23393860459327698,
      "learning_rate": 0.0001771602984384043,
      "loss": 0.5155,
      "step": 323
    },
    {
      "epoch": 0.24455137277101613,
      "grad_norm": 0.4317353665828705,
      "learning_rate": 0.0001770045519651873,
      "loss": 0.5379,
      "step": 324
    },
    {
      "epoch": 0.245306160958581,
      "grad_norm": 0.2263685166835785,
      "learning_rate": 0.00017684834522514633,
      "loss": 0.6106,
      "step": 325
    },
    {
      "epoch": 0.24606094914614587,
      "grad_norm": 0.21616962552070618,
      "learning_rate": 0.00017669167915195059,
      "loss": 0.5801,
      "step": 326
    },
    {
      "epoch": 0.24681573733371073,
      "grad_norm": 0.2501189708709717,
      "learning_rate": 0.00017653455468201484,
      "loss": 0.7698,
      "step": 327
    },
    {
      "epoch": 0.2475705255212756,
      "grad_norm": 0.3016868531703949,
      "learning_rate": 0.00017637697275449363,
      "loss": 0.7222,
      "step": 328
    },
    {
      "epoch": 0.24832531370884045,
      "grad_norm": 0.3143859803676605,
      "learning_rate": 0.00017621893431127593,
      "loss": 0.5721,
      "step": 329
    },
    {
      "epoch": 0.2490801018964053,
      "grad_norm": 0.22783298790454865,
      "learning_rate": 0.00017606044029697931,
      "loss": 0.7474,
      "step": 330
    },
    {
      "epoch": 0.2498348900839702,
      "grad_norm": 0.2215566486120224,
      "learning_rate": 0.00017590149165894428,
      "loss": 0.6838,
      "step": 331
    },
    {
      "epoch": 0.25058967827153505,
      "grad_norm": 0.329342246055603,
      "learning_rate": 0.00017574208934722883,
      "loss": 0.5208,
      "step": 332
    },
    {
      "epoch": 0.2513444664590999,
      "grad_norm": 0.2224375605583191,
      "learning_rate": 0.00017558223431460252,
      "loss": 0.7876,
      "step": 333
    },
    {
      "epoch": 0.25209925464666477,
      "grad_norm": 0.3262568712234497,
      "learning_rate": 0.0001754219275165409,
      "loss": 0.6121,
      "step": 334
    },
    {
      "epoch": 0.25285404283422963,
      "grad_norm": 0.2445134073495865,
      "learning_rate": 0.00017526116991121987,
      "loss": 0.7263,
      "step": 335
    },
    {
      "epoch": 0.2536088310217945,
      "grad_norm": 0.5565853118896484,
      "learning_rate": 0.00017509996245950978,
      "loss": 0.6832,
      "step": 336
    },
    {
      "epoch": 0.25436361920935935,
      "grad_norm": 0.3017730116844177,
      "learning_rate": 0.00017493830612496972,
      "loss": 0.6681,
      "step": 337
    },
    {
      "epoch": 0.25511840739692426,
      "grad_norm": 0.3190392553806305,
      "learning_rate": 0.000174776201873842,
      "loss": 0.6867,
      "step": 338
    },
    {
      "epoch": 0.2558731955844891,
      "grad_norm": 0.25729265809059143,
      "learning_rate": 0.00017461365067504602,
      "loss": 0.7962,
      "step": 339
    },
    {
      "epoch": 0.256627983772054,
      "grad_norm": 0.2557692229747772,
      "learning_rate": 0.0001744506535001727,
      "loss": 0.6844,
      "step": 340
    },
    {
      "epoch": 0.25738277195961884,
      "grad_norm": 0.4036548137664795,
      "learning_rate": 0.00017428721132347862,
      "loss": 0.5553,
      "step": 341
    },
    {
      "epoch": 0.2581375601471837,
      "grad_norm": 0.4107062816619873,
      "learning_rate": 0.00017412332512188025,
      "loss": 0.807,
      "step": 342
    },
    {
      "epoch": 0.25889234833474856,
      "grad_norm": 0.27572718262672424,
      "learning_rate": 0.00017395899587494798,
      "loss": 0.6005,
      "step": 343
    },
    {
      "epoch": 0.2596471365223134,
      "grad_norm": 0.23729629814624786,
      "learning_rate": 0.00017379422456490043,
      "loss": 0.6766,
      "step": 344
    },
    {
      "epoch": 0.2604019247098783,
      "grad_norm": 0.3297464847564697,
      "learning_rate": 0.00017362901217659834,
      "loss": 0.6966,
      "step": 345
    },
    {
      "epoch": 0.26115671289744313,
      "grad_norm": 0.2169588953256607,
      "learning_rate": 0.00017346335969753898,
      "loss": 0.5765,
      "step": 346
    },
    {
      "epoch": 0.26191150108500805,
      "grad_norm": 0.3496280908584595,
      "learning_rate": 0.0001732972681178501,
      "loss": 0.7145,
      "step": 347
    },
    {
      "epoch": 0.2626662892725729,
      "grad_norm": 0.24215684831142426,
      "learning_rate": 0.0001731307384302839,
      "loss": 0.5138,
      "step": 348
    },
    {
      "epoch": 0.26342107746013776,
      "grad_norm": 0.4194636344909668,
      "learning_rate": 0.00017296377163021132,
      "loss": 0.704,
      "step": 349
    },
    {
      "epoch": 0.2641758656477026,
      "grad_norm": 0.3017561733722687,
      "learning_rate": 0.00017279636871561592,
      "loss": 0.636,
      "step": 350
    },
    {
      "epoch": 0.2649306538352675,
      "grad_norm": 0.4656320810317993,
      "learning_rate": 0.00017262853068708805,
      "loss": 0.7529,
      "step": 351
    },
    {
      "epoch": 0.26568544202283234,
      "grad_norm": 0.1368282437324524,
      "learning_rate": 0.00017246025854781872,
      "loss": 0.6565,
      "step": 352
    },
    {
      "epoch": 0.2664402302103972,
      "grad_norm": 0.31819191575050354,
      "learning_rate": 0.00017229155330359368,
      "loss": 0.6464,
      "step": 353
    },
    {
      "epoch": 0.26719501839796206,
      "grad_norm": 0.24931600689888,
      "learning_rate": 0.0001721224159627875,
      "loss": 0.71,
      "step": 354
    },
    {
      "epoch": 0.2679498065855269,
      "grad_norm": 0.4986264109611511,
      "learning_rate": 0.0001719528475363573,
      "loss": 0.594,
      "step": 355
    },
    {
      "epoch": 0.2687045947730918,
      "grad_norm": 0.2737301290035248,
      "learning_rate": 0.000171782849037837,
      "loss": 0.6733,
      "step": 356
    },
    {
      "epoch": 0.2694593829606567,
      "grad_norm": 0.3205852508544922,
      "learning_rate": 0.00017161242148333106,
      "loss": 0.7603,
      "step": 357
    },
    {
      "epoch": 0.27021417114822155,
      "grad_norm": 0.3131175935268402,
      "learning_rate": 0.00017144156589150847,
      "loss": 0.6952,
      "step": 358
    },
    {
      "epoch": 0.2709689593357864,
      "grad_norm": 0.3164890706539154,
      "learning_rate": 0.00017127028328359662,
      "loss": 0.7452,
      "step": 359
    },
    {
      "epoch": 0.27172374752335127,
      "grad_norm": 0.272285133600235,
      "learning_rate": 0.0001710985746833753,
      "loss": 0.6954,
      "step": 360
    },
    {
      "epoch": 0.2724785357109161,
      "grad_norm": 0.3353912830352783,
      "learning_rate": 0.00017092644111717052,
      "loss": 0.6505,
      "step": 361
    },
    {
      "epoch": 0.273233323898481,
      "grad_norm": 0.2539770007133484,
      "learning_rate": 0.00017075388361384832,
      "loss": 0.6495,
      "step": 362
    },
    {
      "epoch": 0.27398811208604584,
      "grad_norm": 0.2616039514541626,
      "learning_rate": 0.00017058090320480867,
      "loss": 0.4579,
      "step": 363
    },
    {
      "epoch": 0.2747429002736107,
      "grad_norm": 0.3671461343765259,
      "learning_rate": 0.0001704075009239793,
      "loss": 0.5878,
      "step": 364
    },
    {
      "epoch": 0.27549768846117556,
      "grad_norm": 0.20687252283096313,
      "learning_rate": 0.0001702336778078096,
      "loss": 0.7457,
      "step": 365
    },
    {
      "epoch": 0.2762524766487405,
      "grad_norm": 0.2836979925632477,
      "learning_rate": 0.0001700594348952643,
      "loss": 0.7537,
      "step": 366
    },
    {
      "epoch": 0.27700726483630533,
      "grad_norm": 3.0308964252471924,
      "learning_rate": 0.0001698847732278173,
      "loss": 0.7961,
      "step": 367
    },
    {
      "epoch": 0.2777620530238702,
      "grad_norm": 0.35083821415901184,
      "learning_rate": 0.00016970969384944544,
      "loss": 0.7553,
      "step": 368
    },
    {
      "epoch": 0.27851684121143505,
      "grad_norm": 0.2578086256980896,
      "learning_rate": 0.0001695341978066223,
      "loss": 0.8249,
      "step": 369
    },
    {
      "epoch": 0.2792716293989999,
      "grad_norm": 0.26898667216300964,
      "learning_rate": 0.000169358286148312,
      "loss": 0.8461,
      "step": 370
    },
    {
      "epoch": 0.28002641758656477,
      "grad_norm": 0.3986821472644806,
      "learning_rate": 0.00016918195992596274,
      "loss": 0.5939,
      "step": 371
    },
    {
      "epoch": 0.2807812057741296,
      "grad_norm": 0.4767318665981293,
      "learning_rate": 0.0001690052201935006,
      "loss": 0.7575,
      "step": 372
    },
    {
      "epoch": 0.2815359939616945,
      "grad_norm": 0.3131029009819031,
      "learning_rate": 0.00016882806800732337,
      "loss": 0.6798,
      "step": 373
    },
    {
      "epoch": 0.28229078214925934,
      "grad_norm": 0.40840452909469604,
      "learning_rate": 0.00016865050442629414,
      "loss": 0.6778,
      "step": 374
    },
    {
      "epoch": 0.2830455703368242,
      "grad_norm": 0.26832523941993713,
      "learning_rate": 0.00016847253051173485,
      "loss": 0.8661,
      "step": 375
    },
    {
      "epoch": 0.2838003585243891,
      "grad_norm": 0.25915059447288513,
      "learning_rate": 0.00016829414732742013,
      "loss": 0.6103,
      "step": 376
    },
    {
      "epoch": 0.284555146711954,
      "grad_norm": 0.310707151889801,
      "learning_rate": 0.0001681153559395709,
      "loss": 0.6531,
      "step": 377
    },
    {
      "epoch": 0.28530993489951884,
      "grad_norm": 0.3665522634983063,
      "learning_rate": 0.0001679361574168479,
      "loss": 0.7499,
      "step": 378
    },
    {
      "epoch": 0.2860647230870837,
      "grad_norm": 0.5007505416870117,
      "learning_rate": 0.0001677565528303455,
      "loss": 0.62,
      "step": 379
    },
    {
      "epoch": 0.28681951127464855,
      "grad_norm": 0.4910266697406769,
      "learning_rate": 0.00016757654325358495,
      "loss": 0.6095,
      "step": 380
    },
    {
      "epoch": 0.2875742994622134,
      "grad_norm": 0.3768068253993988,
      "learning_rate": 0.00016739612976250835,
      "loss": 0.5533,
      "step": 381
    },
    {
      "epoch": 0.28832908764977827,
      "grad_norm": 0.255376935005188,
      "learning_rate": 0.00016721531343547207,
      "loss": 0.6546,
      "step": 382
    },
    {
      "epoch": 0.28908387583734313,
      "grad_norm": 0.2904762029647827,
      "learning_rate": 0.0001670340953532401,
      "loss": 0.7059,
      "step": 383
    },
    {
      "epoch": 0.289838664024908,
      "grad_norm": 0.47322767972946167,
      "learning_rate": 0.00016685247659897802,
      "loss": 0.6272,
      "step": 384
    },
    {
      "epoch": 0.2905934522124729,
      "grad_norm": 0.37261244654655457,
      "learning_rate": 0.00016667045825824616,
      "loss": 0.5645,
      "step": 385
    },
    {
      "epoch": 0.29134824040003776,
      "grad_norm": 0.3857574760913849,
      "learning_rate": 0.00016648804141899316,
      "loss": 0.7062,
      "step": 386
    },
    {
      "epoch": 0.2921030285876026,
      "grad_norm": 0.3181384801864624,
      "learning_rate": 0.0001663052271715497,
      "loss": 0.5787,
      "step": 387
    },
    {
      "epoch": 0.2928578167751675,
      "grad_norm": 0.4400133788585663,
      "learning_rate": 0.00016612201660862167,
      "loss": 0.6479,
      "step": 388
    },
    {
      "epoch": 0.29361260496273234,
      "grad_norm": 0.3236318528652191,
      "learning_rate": 0.00016593841082528394,
      "loss": 0.6933,
      "step": 389
    },
    {
      "epoch": 0.2943673931502972,
      "grad_norm": 0.3779964745044708,
      "learning_rate": 0.0001657544109189735,
      "loss": 0.7052,
      "step": 390
    },
    {
      "epoch": 0.29512218133786206,
      "grad_norm": 0.3512745797634125,
      "learning_rate": 0.00016557001798948325,
      "loss": 0.5621,
      "step": 391
    },
    {
      "epoch": 0.2958769695254269,
      "grad_norm": 0.4718324542045593,
      "learning_rate": 0.0001653852331389551,
      "loss": 0.7533,
      "step": 392
    },
    {
      "epoch": 0.2966317577129918,
      "grad_norm": 0.437738835811615,
      "learning_rate": 0.00016520005747187356,
      "loss": 0.6842,
      "step": 393
    },
    {
      "epoch": 0.29738654590055663,
      "grad_norm": 0.3484424352645874,
      "learning_rate": 0.0001650144920950592,
      "loss": 0.6926,
      "step": 394
    },
    {
      "epoch": 0.29814133408812155,
      "grad_norm": 0.40034154057502747,
      "learning_rate": 0.0001648285381176618,
      "loss": 0.6481,
      "step": 395
    },
    {
      "epoch": 0.2988961222756864,
      "grad_norm": 0.3650650084018707,
      "learning_rate": 0.0001646421966511539,
      "loss": 0.6545,
      "step": 396
    },
    {
      "epoch": 0.29965091046325126,
      "grad_norm": 0.5419185161590576,
      "learning_rate": 0.00016445546880932424,
      "loss": 0.7257,
      "step": 397
    },
    {
      "epoch": 0.3004056986508161,
      "grad_norm": 0.5710060596466064,
      "learning_rate": 0.00016426835570827077,
      "loss": 0.7689,
      "step": 398
    },
    {
      "epoch": 0.301160486838381,
      "grad_norm": 0.2896265983581543,
      "learning_rate": 0.00016408085846639434,
      "loss": 0.6339,
      "step": 399
    },
    {
      "epoch": 0.30191527502594584,
      "grad_norm": 0.40119701623916626,
      "learning_rate": 0.0001638929782043918,
      "loss": 0.6849,
      "step": 400
    },
    {
      "epoch": 0.30191527502594584,
      "eval_loss": 0.6770414113998413,
      "eval_runtime": 3185.6093,
      "eval_samples_per_second": 2.958,
      "eval_steps_per_second": 0.74,
      "step": 400
    },
    {
      "epoch": 0.3026700632135107,
      "grad_norm": 0.5973523855209351,
      "learning_rate": 0.00016370471604524938,
      "loss": 0.6528,
      "step": 401
    },
    {
      "epoch": 0.30342485140107556,
      "grad_norm": 0.26034030318260193,
      "learning_rate": 0.000163516073114236,
      "loss": 0.7558,
      "step": 402
    },
    {
      "epoch": 0.3041796395886404,
      "grad_norm": 0.6025131940841675,
      "learning_rate": 0.00016332705053889642,
      "loss": 0.605,
      "step": 403
    },
    {
      "epoch": 0.30493442777620533,
      "grad_norm": 0.3499335050582886,
      "learning_rate": 0.00016313764944904464,
      "loss": 0.6569,
      "step": 404
    },
    {
      "epoch": 0.3056892159637702,
      "grad_norm": 0.2714961767196655,
      "learning_rate": 0.00016294787097675712,
      "loss": 0.6428,
      "step": 405
    },
    {
      "epoch": 0.30644400415133505,
      "grad_norm": 0.38107413053512573,
      "learning_rate": 0.0001627577162563659,
      "loss": 0.7986,
      "step": 406
    },
    {
      "epoch": 0.3071987923388999,
      "grad_norm": 0.34171032905578613,
      "learning_rate": 0.000162567186424452,
      "loss": 0.7347,
      "step": 407
    },
    {
      "epoch": 0.30795358052646477,
      "grad_norm": 0.21756866574287415,
      "learning_rate": 0.0001623762826198385,
      "loss": 0.6315,
      "step": 408
    },
    {
      "epoch": 0.3087083687140296,
      "grad_norm": 0.36248984932899475,
      "learning_rate": 0.00016218500598358374,
      "loss": 0.6992,
      "step": 409
    },
    {
      "epoch": 0.3094631569015945,
      "grad_norm": 0.3666752874851227,
      "learning_rate": 0.0001619933576589746,
      "loss": 0.7917,
      "step": 410
    },
    {
      "epoch": 0.31021794508915934,
      "grad_norm": 0.28193244338035583,
      "learning_rate": 0.0001618013387915194,
      "loss": 0.725,
      "step": 411
    },
    {
      "epoch": 0.3109727332767242,
      "grad_norm": 0.2753708064556122,
      "learning_rate": 0.00016160895052894157,
      "loss": 0.6744,
      "step": 412
    },
    {
      "epoch": 0.31172752146428906,
      "grad_norm": 0.47796395421028137,
      "learning_rate": 0.00016141619402117215,
      "loss": 0.6317,
      "step": 413
    },
    {
      "epoch": 0.312482309651854,
      "grad_norm": 0.34628790616989136,
      "learning_rate": 0.00016122307042034338,
      "loss": 0.6767,
      "step": 414
    },
    {
      "epoch": 0.31323709783941883,
      "grad_norm": 0.356696218252182,
      "learning_rate": 0.00016102958088078171,
      "loss": 0.884,
      "step": 415
    },
    {
      "epoch": 0.3139918860269837,
      "grad_norm": 0.2873910069465637,
      "learning_rate": 0.00016083572655900072,
      "loss": 0.7587,
      "step": 416
    },
    {
      "epoch": 0.31474667421454855,
      "grad_norm": 0.21538046002388,
      "learning_rate": 0.00016064150861369448,
      "loss": 0.656,
      "step": 417
    },
    {
      "epoch": 0.3155014624021134,
      "grad_norm": 0.29107072949409485,
      "learning_rate": 0.00016044692820573042,
      "loss": 0.8229,
      "step": 418
    },
    {
      "epoch": 0.31625625058967827,
      "grad_norm": 0.2940387427806854,
      "learning_rate": 0.00016025198649814243,
      "loss": 0.5428,
      "step": 419
    },
    {
      "epoch": 0.3170110387772431,
      "grad_norm": 0.3358670175075531,
      "learning_rate": 0.00016005668465612398,
      "loss": 0.7175,
      "step": 420
    },
    {
      "epoch": 0.317765826964808,
      "grad_norm": 0.25990045070648193,
      "learning_rate": 0.00015986102384702114,
      "loss": 0.5738,
      "step": 421
    },
    {
      "epoch": 0.31852061515237284,
      "grad_norm": 0.3921523094177246,
      "learning_rate": 0.0001596650052403255,
      "loss": 0.6593,
      "step": 422
    },
    {
      "epoch": 0.3192754033399377,
      "grad_norm": 0.3242965340614319,
      "learning_rate": 0.00015946863000766733,
      "loss": 0.8155,
      "step": 423
    },
    {
      "epoch": 0.3200301915275026,
      "grad_norm": 0.7003850936889648,
      "learning_rate": 0.00015927189932280847,
      "loss": 0.5787,
      "step": 424
    },
    {
      "epoch": 0.3207849797150675,
      "grad_norm": 0.32812967896461487,
      "learning_rate": 0.00015907481436163532,
      "loss": 0.798,
      "step": 425
    },
    {
      "epoch": 0.32153976790263233,
      "grad_norm": 0.3473857045173645,
      "learning_rate": 0.00015887737630215188,
      "loss": 0.5705,
      "step": 426
    },
    {
      "epoch": 0.3222945560901972,
      "grad_norm": 0.3646244406700134,
      "learning_rate": 0.00015867958632447265,
      "loss": 0.6944,
      "step": 427
    },
    {
      "epoch": 0.32304934427776205,
      "grad_norm": 0.3457433879375458,
      "learning_rate": 0.00015848144561081558,
      "loss": 0.6176,
      "step": 428
    },
    {
      "epoch": 0.3238041324653269,
      "grad_norm": 0.39542683959007263,
      "learning_rate": 0.0001582829553454951,
      "loss": 0.6966,
      "step": 429
    },
    {
      "epoch": 0.32455892065289177,
      "grad_norm": 0.38131049275398254,
      "learning_rate": 0.00015808411671491485,
      "loss": 0.8036,
      "step": 430
    },
    {
      "epoch": 0.32531370884045663,
      "grad_norm": 0.21356287598609924,
      "learning_rate": 0.00015788493090756073,
      "loss": 0.6909,
      "step": 431
    },
    {
      "epoch": 0.3260684970280215,
      "grad_norm": 0.2744656205177307,
      "learning_rate": 0.00015768539911399377,
      "loss": 0.6145,
      "step": 432
    },
    {
      "epoch": 0.3268232852155864,
      "grad_norm": 0.32322800159454346,
      "learning_rate": 0.00015748552252684302,
      "loss": 0.5659,
      "step": 433
    },
    {
      "epoch": 0.32757807340315126,
      "grad_norm": 0.4603092670440674,
      "learning_rate": 0.0001572853023407984,
      "loss": 0.7083,
      "step": 434
    },
    {
      "epoch": 0.3283328615907161,
      "grad_norm": 0.33349546790122986,
      "learning_rate": 0.00015708473975260354,
      "loss": 0.7794,
      "step": 435
    },
    {
      "epoch": 0.329087649778281,
      "grad_norm": 0.22801972925662994,
      "learning_rate": 0.00015688383596104875,
      "loss": 0.6194,
      "step": 436
    },
    {
      "epoch": 0.32984243796584584,
      "grad_norm": 0.3285421133041382,
      "learning_rate": 0.00015668259216696363,
      "loss": 0.5502,
      "step": 437
    },
    {
      "epoch": 0.3305972261534107,
      "grad_norm": 0.2136164754629135,
      "learning_rate": 0.0001564810095732101,
      "loss": 0.6288,
      "step": 438
    },
    {
      "epoch": 0.33135201434097555,
      "grad_norm": 0.2594079375267029,
      "learning_rate": 0.00015627908938467514,
      "loss": 0.7085,
      "step": 439
    },
    {
      "epoch": 0.3321068025285404,
      "grad_norm": 0.7204721570014954,
      "learning_rate": 0.00015607683280826354,
      "loss": 0.681,
      "step": 440
    },
    {
      "epoch": 0.33286159071610527,
      "grad_norm": 0.22249309718608856,
      "learning_rate": 0.0001558742410528907,
      "loss": 0.6783,
      "step": 441
    },
    {
      "epoch": 0.33361637890367013,
      "grad_norm": 0.3357514441013336,
      "learning_rate": 0.00015567131532947555,
      "loss": 0.7189,
      "step": 442
    },
    {
      "epoch": 0.33437116709123504,
      "grad_norm": 0.24805423617362976,
      "learning_rate": 0.00015546805685093306,
      "loss": 0.6891,
      "step": 443
    },
    {
      "epoch": 0.3351259552787999,
      "grad_norm": 0.348077654838562,
      "learning_rate": 0.00015526446683216723,
      "loss": 0.8047,
      "step": 444
    },
    {
      "epoch": 0.33588074346636476,
      "grad_norm": 0.2656930685043335,
      "learning_rate": 0.0001550605464900636,
      "loss": 0.7032,
      "step": 445
    },
    {
      "epoch": 0.3366355316539296,
      "grad_norm": 0.35993796586990356,
      "learning_rate": 0.00015485629704348224,
      "loss": 0.7434,
      "step": 446
    },
    {
      "epoch": 0.3373903198414945,
      "grad_norm": 0.4892837405204773,
      "learning_rate": 0.00015465171971325019,
      "loss": 0.645,
      "step": 447
    },
    {
      "epoch": 0.33814510802905934,
      "grad_norm": 0.21095840632915497,
      "learning_rate": 0.0001544468157221544,
      "loss": 0.7138,
      "step": 448
    },
    {
      "epoch": 0.3388998962166242,
      "grad_norm": 0.27165964245796204,
      "learning_rate": 0.00015424158629493432,
      "loss": 0.7222,
      "step": 449
    },
    {
      "epoch": 0.33965468440418906,
      "grad_norm": 0.3951316177845001,
      "learning_rate": 0.00015403603265827441,
      "loss": 0.6106,
      "step": 450
    },
    {
      "epoch": 0.3404094725917539,
      "grad_norm": 0.20997364819049835,
      "learning_rate": 0.00015383015604079724,
      "loss": 0.6952,
      "step": 451
    },
    {
      "epoch": 0.34116426077931883,
      "grad_norm": 0.6255149245262146,
      "learning_rate": 0.0001536239576730556,
      "loss": 0.7964,
      "step": 452
    },
    {
      "epoch": 0.3419190489668837,
      "grad_norm": 0.3550339341163635,
      "learning_rate": 0.00015341743878752564,
      "loss": 0.6575,
      "step": 453
    },
    {
      "epoch": 0.34267383715444855,
      "grad_norm": 0.23317494988441467,
      "learning_rate": 0.0001532106006185992,
      "loss": 0.5977,
      "step": 454
    },
    {
      "epoch": 0.3434286253420134,
      "grad_norm": 0.5441181659698486,
      "learning_rate": 0.00015300344440257656,
      "loss": 0.601,
      "step": 455
    },
    {
      "epoch": 0.34418341352957826,
      "grad_norm": 0.6544070243835449,
      "learning_rate": 0.00015279597137765895,
      "loss": 0.8827,
      "step": 456
    },
    {
      "epoch": 0.3449382017171431,
      "grad_norm": 0.3958653509616852,
      "learning_rate": 0.00015258818278394125,
      "loss": 0.7246,
      "step": 457
    },
    {
      "epoch": 0.345692989904708,
      "grad_norm": 0.4025174379348755,
      "learning_rate": 0.00015238007986340452,
      "loss": 0.7682,
      "step": 458
    },
    {
      "epoch": 0.34644777809227284,
      "grad_norm": 0.3396991789340973,
      "learning_rate": 0.00015217166385990863,
      "loss": 0.6121,
      "step": 459
    },
    {
      "epoch": 0.3472025662798377,
      "grad_norm": 0.3001101613044739,
      "learning_rate": 0.00015196293601918477,
      "loss": 0.6932,
      "step": 460
    },
    {
      "epoch": 0.34795735446740256,
      "grad_norm": 0.41470980644226074,
      "learning_rate": 0.00015175389758882802,
      "loss": 0.6614,
      "step": 461
    },
    {
      "epoch": 0.34871214265496747,
      "grad_norm": 0.17638808488845825,
      "learning_rate": 0.0001515445498182899,
      "loss": 0.6849,
      "step": 462
    },
    {
      "epoch": 0.34946693084253233,
      "grad_norm": 0.42796841263771057,
      "learning_rate": 0.0001513348939588709,
      "loss": 0.5629,
      "step": 463
    },
    {
      "epoch": 0.3502217190300972,
      "grad_norm": 0.2622910737991333,
      "learning_rate": 0.00015112493126371294,
      "loss": 0.6046,
      "step": 464
    },
    {
      "epoch": 0.35097650721766205,
      "grad_norm": 0.3727681338787079,
      "learning_rate": 0.0001509146629877921,
      "loss": 0.7551,
      "step": 465
    },
    {
      "epoch": 0.3517312954052269,
      "grad_norm": 0.6900455951690674,
      "learning_rate": 0.00015070409038791076,
      "loss": 0.6613,
      "step": 466
    },
    {
      "epoch": 0.35248608359279177,
      "grad_norm": 0.4090961217880249,
      "learning_rate": 0.00015049321472269042,
      "loss": 0.7645,
      "step": 467
    },
    {
      "epoch": 0.3532408717803566,
      "grad_norm": 0.8267813920974731,
      "learning_rate": 0.00015028203725256405,
      "loss": 0.6564,
      "step": 468
    },
    {
      "epoch": 0.3539956599679215,
      "grad_norm": 0.9609864354133606,
      "learning_rate": 0.00015007055923976843,
      "loss": 0.7073,
      "step": 469
    },
    {
      "epoch": 0.35475044815548634,
      "grad_norm": 0.33631542325019836,
      "learning_rate": 0.00014985878194833684,
      "loss": 0.6055,
      "step": 470
    },
    {
      "epoch": 0.35550523634305126,
      "grad_norm": 0.26877859234809875,
      "learning_rate": 0.00014964670664409136,
      "loss": 0.6618,
      "step": 471
    },
    {
      "epoch": 0.3562600245306161,
      "grad_norm": 0.6064300537109375,
      "learning_rate": 0.00014943433459463528,
      "loss": 0.6947,
      "step": 472
    },
    {
      "epoch": 0.357014812718181,
      "grad_norm": 3.0996265411376953,
      "learning_rate": 0.00014922166706934567,
      "loss": 0.6161,
      "step": 473
    },
    {
      "epoch": 0.35776960090574583,
      "grad_norm": 0.453563928604126,
      "learning_rate": 0.00014900870533936556,
      "loss": 0.5081,
      "step": 474
    },
    {
      "epoch": 0.3585243890933107,
      "grad_norm": 0.23358814418315887,
      "learning_rate": 0.00014879545067759674,
      "loss": 0.7076,
      "step": 475
    },
    {
      "epoch": 0.35927917728087555,
      "grad_norm": 0.30657958984375,
      "learning_rate": 0.00014858190435869154,
      "loss": 0.6208,
      "step": 476
    },
    {
      "epoch": 0.3600339654684404,
      "grad_norm": 0.5702426433563232,
      "learning_rate": 0.00014836806765904586,
      "loss": 0.69,
      "step": 477
    },
    {
      "epoch": 0.36078875365600527,
      "grad_norm": 0.4912424385547638,
      "learning_rate": 0.00014815394185679112,
      "loss": 0.5702,
      "step": 478
    },
    {
      "epoch": 0.3615435418435701,
      "grad_norm": 0.40705421566963196,
      "learning_rate": 0.00014793952823178677,
      "loss": 0.5994,
      "step": 479
    },
    {
      "epoch": 0.362298330031135,
      "grad_norm": 0.3784513771533966,
      "learning_rate": 0.00014772482806561256,
      "loss": 0.7333,
      "step": 480
    },
    {
      "epoch": 0.3630531182186999,
      "grad_norm": 0.2849465608596802,
      "learning_rate": 0.00014750984264156103,
      "loss": 0.5378,
      "step": 481
    },
    {
      "epoch": 0.36380790640626476,
      "grad_norm": 0.31071770191192627,
      "learning_rate": 0.00014729457324462968,
      "loss": 0.7752,
      "step": 482
    },
    {
      "epoch": 0.3645626945938296,
      "grad_norm": 0.25153058767318726,
      "learning_rate": 0.00014707902116151337,
      "loss": 0.6597,
      "step": 483
    },
    {
      "epoch": 0.3653174827813945,
      "grad_norm": 1.0263921022415161,
      "learning_rate": 0.00014686318768059666,
      "loss": 0.6797,
      "step": 484
    },
    {
      "epoch": 0.36607227096895933,
      "grad_norm": 0.35840871930122375,
      "learning_rate": 0.00014664707409194598,
      "loss": 0.7332,
      "step": 485
    },
    {
      "epoch": 0.3668270591565242,
      "grad_norm": 0.32288914918899536,
      "learning_rate": 0.00014643068168730203,
      "loss": 0.5192,
      "step": 486
    },
    {
      "epoch": 0.36758184734408905,
      "grad_norm": 0.5730283856391907,
      "learning_rate": 0.0001462140117600721,
      "loss": 0.8065,
      "step": 487
    },
    {
      "epoch": 0.3683366355316539,
      "grad_norm": 0.31875982880592346,
      "learning_rate": 0.00014599706560532218,
      "loss": 0.7451,
      "step": 488
    },
    {
      "epoch": 0.36909142371921877,
      "grad_norm": 0.4373991787433624,
      "learning_rate": 0.00014577984451976937,
      "loss": 0.9,
      "step": 489
    },
    {
      "epoch": 0.3698462119067837,
      "grad_norm": 0.423440158367157,
      "learning_rate": 0.00014556234980177407,
      "loss": 0.6638,
      "step": 490
    },
    {
      "epoch": 0.37060100009434854,
      "grad_norm": 0.4163542687892914,
      "learning_rate": 0.00014534458275133213,
      "loss": 0.6368,
      "step": 491
    },
    {
      "epoch": 0.3713557882819134,
      "grad_norm": 0.3316734731197357,
      "learning_rate": 0.0001451265446700673,
      "loss": 0.8034,
      "step": 492
    },
    {
      "epoch": 0.37211057646947826,
      "grad_norm": 0.49622923135757446,
      "learning_rate": 0.0001449082368612232,
      "loss": 0.8632,
      "step": 493
    },
    {
      "epoch": 0.3728653646570431,
      "grad_norm": 0.28081008791923523,
      "learning_rate": 0.00014468966062965573,
      "loss": 0.5995,
      "step": 494
    },
    {
      "epoch": 0.373620152844608,
      "grad_norm": 0.5027595162391663,
      "learning_rate": 0.00014447081728182517,
      "loss": 0.7501,
      "step": 495
    },
    {
      "epoch": 0.37437494103217284,
      "grad_norm": 0.30450406670570374,
      "learning_rate": 0.00014425170812578837,
      "loss": 0.6544,
      "step": 496
    },
    {
      "epoch": 0.3751297292197377,
      "grad_norm": 0.19666950404644012,
      "learning_rate": 0.00014403233447119094,
      "loss": 0.5938,
      "step": 497
    },
    {
      "epoch": 0.37588451740730255,
      "grad_norm": 0.49227479100227356,
      "learning_rate": 0.00014381269762925953,
      "loss": 0.6802,
      "step": 498
    },
    {
      "epoch": 0.3766393055948674,
      "grad_norm": 0.378364622592926,
      "learning_rate": 0.00014359279891279375,
      "loss": 0.6665,
      "step": 499
    },
    {
      "epoch": 0.3773940937824323,
      "grad_norm": 0.3026007115840912,
      "learning_rate": 0.0001433726396361586,
      "loss": 0.6094,
      "step": 500
    },
    {
      "epoch": 0.3781488819699972,
      "grad_norm": 0.25980129837989807,
      "learning_rate": 0.00014315222111527639,
      "loss": 0.6541,
      "step": 501
    },
    {
      "epoch": 0.37890367015756204,
      "grad_norm": 0.39522433280944824,
      "learning_rate": 0.00014293154466761904,
      "loss": 0.8433,
      "step": 502
    },
    {
      "epoch": 0.3796584583451269,
      "grad_norm": 0.48211669921875,
      "learning_rate": 0.00014271061161220006,
      "loss": 0.745,
      "step": 503
    },
    {
      "epoch": 0.38041324653269176,
      "grad_norm": 0.3391760587692261,
      "learning_rate": 0.00014248942326956687,
      "loss": 0.4865,
      "step": 504
    },
    {
      "epoch": 0.3811680347202566,
      "grad_norm": 0.23306915163993835,
      "learning_rate": 0.0001422679809617926,
      "loss": 0.6936,
      "step": 505
    },
    {
      "epoch": 0.3819228229078215,
      "grad_norm": 0.6203911900520325,
      "learning_rate": 0.0001420462860124685,
      "loss": 0.6166,
      "step": 506
    },
    {
      "epoch": 0.38267761109538634,
      "grad_norm": 0.37532174587249756,
      "learning_rate": 0.00014182433974669584,
      "loss": 0.6057,
      "step": 507
    },
    {
      "epoch": 0.3834323992829512,
      "grad_norm": 0.264089971780777,
      "learning_rate": 0.00014160214349107805,
      "loss": 0.696,
      "step": 508
    },
    {
      "epoch": 0.3841871874705161,
      "grad_norm": 0.41374915838241577,
      "learning_rate": 0.00014137969857371275,
      "loss": 0.5928,
      "step": 509
    },
    {
      "epoch": 0.38494197565808097,
      "grad_norm": 0.23142048716545105,
      "learning_rate": 0.00014115700632418395,
      "loss": 0.6835,
      "step": 510
    },
    {
      "epoch": 0.38569676384564583,
      "grad_norm": 0.47379082441329956,
      "learning_rate": 0.0001409340680735539,
      "loss": 0.7562,
      "step": 511
    },
    {
      "epoch": 0.3864515520332107,
      "grad_norm": 0.5065134167671204,
      "learning_rate": 0.00014071088515435522,
      "loss": 0.6157,
      "step": 512
    },
    {
      "epoch": 0.38720634022077555,
      "grad_norm": 0.210488960146904,
      "learning_rate": 0.00014048745890058305,
      "loss": 0.6259,
      "step": 513
    },
    {
      "epoch": 0.3879611284083404,
      "grad_norm": 0.2527678906917572,
      "learning_rate": 0.00014026379064768684,
      "loss": 0.8281,
      "step": 514
    },
    {
      "epoch": 0.38871591659590526,
      "grad_norm": 0.37317609786987305,
      "learning_rate": 0.00014003988173256267,
      "loss": 0.7292,
      "step": 515
    },
    {
      "epoch": 0.3894707047834701,
      "grad_norm": 0.28718671202659607,
      "learning_rate": 0.0001398157334935449,
      "loss": 0.6422,
      "step": 516
    },
    {
      "epoch": 0.390225492971035,
      "grad_norm": 0.3415776789188385,
      "learning_rate": 0.00013959134727039853,
      "loss": 0.7049,
      "step": 517
    },
    {
      "epoch": 0.39098028115859984,
      "grad_norm": 0.39606761932373047,
      "learning_rate": 0.00013936672440431093,
      "loss": 0.5637,
      "step": 518
    },
    {
      "epoch": 0.39173506934616475,
      "grad_norm": 0.2546222507953644,
      "learning_rate": 0.00013914186623788396,
      "loss": 0.69,
      "step": 519
    },
    {
      "epoch": 0.3924898575337296,
      "grad_norm": 0.33258792757987976,
      "learning_rate": 0.00013891677411512584,
      "loss": 0.7216,
      "step": 520
    },
    {
      "epoch": 0.3932446457212945,
      "grad_norm": 0.4044175148010254,
      "learning_rate": 0.00013869144938144325,
      "loss": 0.6807,
      "step": 521
    },
    {
      "epoch": 0.39399943390885933,
      "grad_norm": 0.25340041518211365,
      "learning_rate": 0.00013846589338363318,
      "loss": 0.5915,
      "step": 522
    },
    {
      "epoch": 0.3947542220964242,
      "grad_norm": 0.37404292821884155,
      "learning_rate": 0.00013824010746987494,
      "loss": 0.6589,
      "step": 523
    },
    {
      "epoch": 0.39550901028398905,
      "grad_norm": 0.34541505575180054,
      "learning_rate": 0.00013801409298972207,
      "loss": 0.6709,
      "step": 524
    },
    {
      "epoch": 0.3962637984715539,
      "grad_norm": 0.20135702192783356,
      "learning_rate": 0.00013778785129409425,
      "loss": 0.6174,
      "step": 525
    },
    {
      "epoch": 0.39701858665911877,
      "grad_norm": 0.3613041341304779,
      "learning_rate": 0.0001375613837352693,
      "loss": 0.724,
      "step": 526
    },
    {
      "epoch": 0.3977733748466836,
      "grad_norm": 0.23469164967536926,
      "learning_rate": 0.00013733469166687506,
      "loss": 0.5402,
      "step": 527
    },
    {
      "epoch": 0.39852816303424854,
      "grad_norm": 0.5846571326255798,
      "learning_rate": 0.00013710777644388122,
      "loss": 0.726,
      "step": 528
    },
    {
      "epoch": 0.3992829512218134,
      "grad_norm": 0.4044141173362732,
      "learning_rate": 0.0001368806394225914,
      "loss": 0.7276,
      "step": 529
    },
    {
      "epoch": 0.40003773940937826,
      "grad_norm": 0.3186561167240143,
      "learning_rate": 0.00013665328196063494,
      "loss": 0.66,
      "step": 530
    },
    {
      "epoch": 0.4007925275969431,
      "grad_norm": 0.37770381569862366,
      "learning_rate": 0.00013642570541695867,
      "loss": 0.6226,
      "step": 531
    },
    {
      "epoch": 0.401547315784508,
      "grad_norm": 0.30173158645629883,
      "learning_rate": 0.000136197911151819,
      "loss": 0.7928,
      "step": 532
    },
    {
      "epoch": 0.40230210397207283,
      "grad_norm": 0.4657934010028839,
      "learning_rate": 0.0001359699005267736,
      "loss": 0.75,
      "step": 533
    },
    {
      "epoch": 0.4030568921596377,
      "grad_norm": 0.25598782300949097,
      "learning_rate": 0.00013574167490467348,
      "loss": 0.7108,
      "step": 534
    },
    {
      "epoch": 0.40381168034720255,
      "grad_norm": 0.19423525035381317,
      "learning_rate": 0.00013551323564965463,
      "loss": 0.6332,
      "step": 535
    },
    {
      "epoch": 0.4045664685347674,
      "grad_norm": 0.2570939064025879,
      "learning_rate": 0.00013528458412712997,
      "loss": 0.6122,
      "step": 536
    },
    {
      "epoch": 0.40532125672233227,
      "grad_norm": 0.49614325165748596,
      "learning_rate": 0.00013505572170378118,
      "loss": 0.6653,
      "step": 537
    },
    {
      "epoch": 0.4060760449098972,
      "grad_norm": 0.2192016988992691,
      "learning_rate": 0.0001348266497475505,
      "loss": 0.6411,
      "step": 538
    },
    {
      "epoch": 0.40683083309746204,
      "grad_norm": 0.4769265353679657,
      "learning_rate": 0.00013459736962763262,
      "loss": 0.5749,
      "step": 539
    },
    {
      "epoch": 0.4075856212850269,
      "grad_norm": 0.46662965416908264,
      "learning_rate": 0.00013436788271446642,
      "loss": 0.6597,
      "step": 540
    },
    {
      "epoch": 0.40834040947259176,
      "grad_norm": 0.43655163049697876,
      "learning_rate": 0.0001341381903797268,
      "loss": 0.7678,
      "step": 541
    },
    {
      "epoch": 0.4090951976601566,
      "grad_norm": 0.2933693528175354,
      "learning_rate": 0.00013390829399631654,
      "loss": 0.7249,
      "step": 542
    },
    {
      "epoch": 0.4098499858477215,
      "grad_norm": 0.24131454527378082,
      "learning_rate": 0.000133678194938358,
      "loss": 0.5081,
      "step": 543
    },
    {
      "epoch": 0.41060477403528634,
      "grad_norm": 0.44995570182800293,
      "learning_rate": 0.000133447894581185,
      "loss": 0.6863,
      "step": 544
    },
    {
      "epoch": 0.4113595622228512,
      "grad_norm": 0.2655003070831299,
      "learning_rate": 0.0001332173943013345,
      "loss": 0.7466,
      "step": 545
    },
    {
      "epoch": 0.41211435041041605,
      "grad_norm": 0.5177380442619324,
      "learning_rate": 0.00013298669547653848,
      "loss": 0.7957,
      "step": 546
    },
    {
      "epoch": 0.41286913859798097,
      "grad_norm": 0.41877666115760803,
      "learning_rate": 0.00013275579948571558,
      "loss": 0.6684,
      "step": 547
    },
    {
      "epoch": 0.4136239267855458,
      "grad_norm": 0.494854211807251,
      "learning_rate": 0.00013252470770896304,
      "loss": 0.6498,
      "step": 548
    },
    {
      "epoch": 0.4143787149731107,
      "grad_norm": 0.3143676817417145,
      "learning_rate": 0.0001322934215275482,
      "loss": 0.7007,
      "step": 549
    },
    {
      "epoch": 0.41513350316067554,
      "grad_norm": 0.23990648984909058,
      "learning_rate": 0.0001320619423239005,
      "loss": 0.6041,
      "step": 550
    },
    {
      "epoch": 0.4158882913482404,
      "grad_norm": 0.48864981532096863,
      "learning_rate": 0.00013183027148160303,
      "loss": 0.8086,
      "step": 551
    },
    {
      "epoch": 0.41664307953580526,
      "grad_norm": 0.6873610615730286,
      "learning_rate": 0.00013159841038538436,
      "loss": 0.6422,
      "step": 552
    },
    {
      "epoch": 0.4173978677233701,
      "grad_norm": 0.47649645805358887,
      "learning_rate": 0.00013136636042111022,
      "loss": 0.7295,
      "step": 553
    },
    {
      "epoch": 0.418152655910935,
      "grad_norm": 0.3211478590965271,
      "learning_rate": 0.0001311341229757753,
      "loss": 0.7231,
      "step": 554
    },
    {
      "epoch": 0.41890744409849984,
      "grad_norm": 0.45983749628067017,
      "learning_rate": 0.00013090169943749476,
      "loss": 0.7231,
      "step": 555
    },
    {
      "epoch": 0.4196622322860647,
      "grad_norm": 0.3740045130252838,
      "learning_rate": 0.00013066909119549615,
      "loss": 0.7139,
      "step": 556
    },
    {
      "epoch": 0.4204170204736296,
      "grad_norm": 0.2977611720561981,
      "learning_rate": 0.00013043629964011103,
      "loss": 0.6837,
      "step": 557
    },
    {
      "epoch": 0.42117180866119447,
      "grad_norm": 0.4304244816303253,
      "learning_rate": 0.00013020332616276657,
      "loss": 0.6774,
      "step": 558
    },
    {
      "epoch": 0.4219265968487593,
      "grad_norm": 0.26074954867362976,
      "learning_rate": 0.0001299701721559774,
      "loss": 0.5522,
      "step": 559
    },
    {
      "epoch": 0.4226813850363242,
      "grad_norm": 0.4610307514667511,
      "learning_rate": 0.0001297368390133372,
      "loss": 0.647,
      "step": 560
    },
    {
      "epoch": 0.42343617322388905,
      "grad_norm": 0.22406107187271118,
      "learning_rate": 0.0001295033281295103,
      "loss": 0.7137,
      "step": 561
    },
    {
      "epoch": 0.4241909614114539,
      "grad_norm": 0.28973710536956787,
      "learning_rate": 0.00012926964090022343,
      "loss": 0.6067,
      "step": 562
    },
    {
      "epoch": 0.42494574959901876,
      "grad_norm": 0.3004072904586792,
      "learning_rate": 0.00012903577872225736,
      "loss": 0.6345,
      "step": 563
    },
    {
      "epoch": 0.4257005377865836,
      "grad_norm": 0.26585152745246887,
      "learning_rate": 0.00012880174299343864,
      "loss": 0.5349,
      "step": 564
    },
    {
      "epoch": 0.4264553259741485,
      "grad_norm": 0.41220977902412415,
      "learning_rate": 0.00012856753511263105,
      "loss": 0.6534,
      "step": 565
    },
    {
      "epoch": 0.4272101141617134,
      "grad_norm": 0.4015585780143738,
      "learning_rate": 0.0001283331564797274,
      "loss": 0.7718,
      "step": 566
    },
    {
      "epoch": 0.42796490234927825,
      "grad_norm": 0.3679286241531372,
      "learning_rate": 0.00012809860849564105,
      "loss": 0.6498,
      "step": 567
    },
    {
      "epoch": 0.4287196905368431,
      "grad_norm": 0.5227369070053101,
      "learning_rate": 0.0001278638925622977,
      "loss": 0.7421,
      "step": 568
    },
    {
      "epoch": 0.42947447872440797,
      "grad_norm": 0.3502371311187744,
      "learning_rate": 0.00012762901008262677,
      "loss": 0.7363,
      "step": 569
    },
    {
      "epoch": 0.43022926691197283,
      "grad_norm": 0.2513252794742584,
      "learning_rate": 0.0001273939624605533,
      "loss": 0.6438,
      "step": 570
    },
    {
      "epoch": 0.4309840550995377,
      "grad_norm": 0.45923471450805664,
      "learning_rate": 0.00012715875110098928,
      "loss": 0.6208,
      "step": 571
    },
    {
      "epoch": 0.43173884328710255,
      "grad_norm": 0.4466950297355652,
      "learning_rate": 0.00012692337740982548,
      "loss": 0.6354,
      "step": 572
    },
    {
      "epoch": 0.4324936314746674,
      "grad_norm": 0.2635452151298523,
      "learning_rate": 0.00012668784279392287,
      "loss": 0.6498,
      "step": 573
    },
    {
      "epoch": 0.43324841966223226,
      "grad_norm": 0.25499603152275085,
      "learning_rate": 0.00012645214866110433,
      "loss": 0.725,
      "step": 574
    },
    {
      "epoch": 0.4340032078497971,
      "grad_norm": 0.3467075824737549,
      "learning_rate": 0.00012621629642014623,
      "loss": 0.6173,
      "step": 575
    },
    {
      "epoch": 0.43475799603736204,
      "grad_norm": 0.40304774045944214,
      "learning_rate": 0.00012598028748076986,
      "loss": 0.6735,
      "step": 576
    },
    {
      "epoch": 0.4355127842249269,
      "grad_norm": 0.3392985463142395,
      "learning_rate": 0.00012574412325363325,
      "loss": 0.6644,
      "step": 577
    },
    {
      "epoch": 0.43626757241249176,
      "grad_norm": 0.4042972922325134,
      "learning_rate": 0.0001255078051503225,
      "loss": 0.7034,
      "step": 578
    },
    {
      "epoch": 0.4370223606000566,
      "grad_norm": 0.39231210947036743,
      "learning_rate": 0.00012527133458334352,
      "loss": 0.7777,
      "step": 579
    },
    {
      "epoch": 0.4377771487876215,
      "grad_norm": 0.6262338161468506,
      "learning_rate": 0.0001250347129661135,
      "loss": 0.6665,
      "step": 580
    },
    {
      "epoch": 0.43853193697518633,
      "grad_norm": 0.5488032698631287,
      "learning_rate": 0.00012479794171295247,
      "loss": 0.7274,
      "step": 581
    },
    {
      "epoch": 0.4392867251627512,
      "grad_norm": 0.39269718527793884,
      "learning_rate": 0.00012456102223907484,
      "loss": 0.7421,
      "step": 582
    },
    {
      "epoch": 0.44004151335031605,
      "grad_norm": 0.25569015741348267,
      "learning_rate": 0.00012432395596058098,
      "loss": 0.7141,
      "step": 583
    },
    {
      "epoch": 0.4407963015378809,
      "grad_norm": 0.33476385474205017,
      "learning_rate": 0.0001240867442944487,
      "loss": 0.7421,
      "step": 584
    },
    {
      "epoch": 0.4415510897254458,
      "grad_norm": 0.351534903049469,
      "learning_rate": 0.0001238493886585248,
      "loss": 0.6337,
      "step": 585
    },
    {
      "epoch": 0.4423058779130107,
      "grad_norm": 0.2245737761259079,
      "learning_rate": 0.00012361189047151676,
      "loss": 0.7833,
      "step": 586
    },
    {
      "epoch": 0.44306066610057554,
      "grad_norm": 0.3804994523525238,
      "learning_rate": 0.0001233742511529839,
      "loss": 0.6776,
      "step": 587
    },
    {
      "epoch": 0.4438154542881404,
      "grad_norm": 0.22379575669765472,
      "learning_rate": 0.0001231364721233292,
      "loss": 0.6539,
      "step": 588
    },
    {
      "epoch": 0.44457024247570526,
      "grad_norm": 0.3842039406299591,
      "learning_rate": 0.00012289855480379075,
      "loss": 0.6597,
      "step": 589
    },
    {
      "epoch": 0.4453250306632701,
      "grad_norm": 0.2128482162952423,
      "learning_rate": 0.0001226605006164331,
      "loss": 0.7132,
      "step": 590
    },
    {
      "epoch": 0.446079818850835,
      "grad_norm": 0.3095869719982147,
      "learning_rate": 0.00012242231098413898,
      "loss": 0.5688,
      "step": 591
    },
    {
      "epoch": 0.44683460703839983,
      "grad_norm": 0.3448277413845062,
      "learning_rate": 0.0001221839873306007,
      "loss": 0.6315,
      "step": 592
    },
    {
      "epoch": 0.4475893952259647,
      "grad_norm": 0.35315608978271484,
      "learning_rate": 0.00012194553108031152,
      "loss": 0.7332,
      "step": 593
    },
    {
      "epoch": 0.44834418341352955,
      "grad_norm": 0.22846956551074982,
      "learning_rate": 0.0001217069436585574,
      "loss": 0.7217,
      "step": 594
    },
    {
      "epoch": 0.44909897160109447,
      "grad_norm": 0.18365837633609772,
      "learning_rate": 0.00012146822649140819,
      "loss": 0.6141,
      "step": 595
    },
    {
      "epoch": 0.4498537597886593,
      "grad_norm": 0.37444087862968445,
      "learning_rate": 0.00012122938100570938,
      "loss": 0.6296,
      "step": 596
    },
    {
      "epoch": 0.4506085479762242,
      "grad_norm": 0.2772369980812073,
      "learning_rate": 0.00012099040862907332,
      "loss": 0.5325,
      "step": 597
    },
    {
      "epoch": 0.45136333616378904,
      "grad_norm": 0.313412070274353,
      "learning_rate": 0.00012075131078987083,
      "loss": 0.6796,
      "step": 598
    },
    {
      "epoch": 0.4521181243513539,
      "grad_norm": 0.3974037766456604,
      "learning_rate": 0.00012051208891722275,
      "loss": 0.7102,
      "step": 599
    },
    {
      "epoch": 0.45287291253891876,
      "grad_norm": 0.37136954069137573,
      "learning_rate": 0.00012027274444099109,
      "loss": 0.6639,
      "step": 600
    },
    {
      "epoch": 0.45287291253891876,
      "eval_loss": 0.6726822257041931,
      "eval_runtime": 3173.3656,
      "eval_samples_per_second": 2.969,
      "eval_steps_per_second": 0.742,
      "step": 600
    },
    {
      "epoch": 0.4536277007264836,
      "grad_norm": 0.41198915243148804,
      "learning_rate": 0.00012003327879177085,
      "loss": 0.7533,
      "step": 601
    },
    {
      "epoch": 0.4543824889140485,
      "grad_norm": 0.27736011147499084,
      "learning_rate": 0.0001197936934008812,
      "loss": 0.5495,
      "step": 602
    },
    {
      "epoch": 0.45513727710161334,
      "grad_norm": 0.28189051151275635,
      "learning_rate": 0.0001195539897003571,
      "loss": 0.7122,
      "step": 603
    },
    {
      "epoch": 0.45589206528917825,
      "grad_norm": 0.31096139550209045,
      "learning_rate": 0.00011931416912294055,
      "loss": 0.6716,
      "step": 604
    },
    {
      "epoch": 0.4566468534767431,
      "grad_norm": 0.40522509813308716,
      "learning_rate": 0.00011907423310207229,
      "loss": 0.5314,
      "step": 605
    },
    {
      "epoch": 0.45740164166430797,
      "grad_norm": 0.28217482566833496,
      "learning_rate": 0.00011883418307188292,
      "loss": 0.5459,
      "step": 606
    },
    {
      "epoch": 0.4581564298518728,
      "grad_norm": 0.18943274021148682,
      "learning_rate": 0.0001185940204671846,
      "loss": 0.5258,
      "step": 607
    },
    {
      "epoch": 0.4589112180394377,
      "grad_norm": 0.8548486828804016,
      "learning_rate": 0.00011835374672346232,
      "loss": 0.693,
      "step": 608
    },
    {
      "epoch": 0.45966600622700254,
      "grad_norm": 0.38210728764533997,
      "learning_rate": 0.00011811336327686538,
      "loss": 0.7006,
      "step": 609
    },
    {
      "epoch": 0.4604207944145674,
      "grad_norm": 0.22812196612358093,
      "learning_rate": 0.00011787287156419877,
      "loss": 0.7473,
      "step": 610
    },
    {
      "epoch": 0.46117558260213226,
      "grad_norm": 0.5812028646469116,
      "learning_rate": 0.00011763227302291463,
      "loss": 0.6993,
      "step": 611
    },
    {
      "epoch": 0.4619303707896971,
      "grad_norm": 0.282056599855423,
      "learning_rate": 0.00011739156909110363,
      "loss": 0.5313,
      "step": 612
    },
    {
      "epoch": 0.462685158977262,
      "grad_norm": 0.411687433719635,
      "learning_rate": 0.00011715076120748631,
      "loss": 0.6267,
      "step": 613
    },
    {
      "epoch": 0.4634399471648269,
      "grad_norm": 0.48448804020881653,
      "learning_rate": 0.00011690985081140465,
      "loss": 0.6667,
      "step": 614
    },
    {
      "epoch": 0.46419473535239175,
      "grad_norm": 0.3493572473526001,
      "learning_rate": 0.00011666883934281323,
      "loss": 0.6568,
      "step": 615
    },
    {
      "epoch": 0.4649495235399566,
      "grad_norm": 0.36401376128196716,
      "learning_rate": 0.0001164277282422709,
      "loss": 0.5057,
      "step": 616
    },
    {
      "epoch": 0.46570431172752147,
      "grad_norm": 0.41817259788513184,
      "learning_rate": 0.00011618651895093191,
      "loss": 0.8069,
      "step": 617
    },
    {
      "epoch": 0.46645909991508633,
      "grad_norm": 0.5590718388557434,
      "learning_rate": 0.00011594521291053746,
      "loss": 0.6543,
      "step": 618
    },
    {
      "epoch": 0.4672138881026512,
      "grad_norm": 0.22987601161003113,
      "learning_rate": 0.00011570381156340701,
      "loss": 0.6139,
      "step": 619
    },
    {
      "epoch": 0.46796867629021605,
      "grad_norm": 0.32411065697669983,
      "learning_rate": 0.00011546231635242973,
      "loss": 0.6535,
      "step": 620
    },
    {
      "epoch": 0.4687234644777809,
      "grad_norm": 0.4728913903236389,
      "learning_rate": 0.00011522072872105576,
      "loss": 0.7305,
      "step": 621
    },
    {
      "epoch": 0.46947825266534576,
      "grad_norm": 0.420791894197464,
      "learning_rate": 0.00011497905011328769,
      "loss": 0.8334,
      "step": 622
    },
    {
      "epoch": 0.4702330408529107,
      "grad_norm": 0.28049832582473755,
      "learning_rate": 0.00011473728197367188,
      "loss": 0.6477,
      "step": 623
    },
    {
      "epoch": 0.47098782904047554,
      "grad_norm": 0.34951382875442505,
      "learning_rate": 0.00011449542574728985,
      "loss": 0.6359,
      "step": 624
    },
    {
      "epoch": 0.4717426172280404,
      "grad_norm": 0.36156588792800903,
      "learning_rate": 0.00011425348287974955,
      "loss": 0.6145,
      "step": 625
    },
    {
      "epoch": 0.47249740541560525,
      "grad_norm": 0.3050723075866699,
      "learning_rate": 0.00011401145481717694,
      "loss": 0.6171,
      "step": 626
    },
    {
      "epoch": 0.4732521936031701,
      "grad_norm": 0.32427462935447693,
      "learning_rate": 0.00011376934300620706,
      "loss": 0.6573,
      "step": 627
    },
    {
      "epoch": 0.47400698179073497,
      "grad_norm": 0.25388333201408386,
      "learning_rate": 0.0001135271488939756,
      "loss": 0.5843,
      "step": 628
    },
    {
      "epoch": 0.47476176997829983,
      "grad_norm": 0.24875135719776154,
      "learning_rate": 0.00011328487392811019,
      "loss": 0.6323,
      "step": 629
    },
    {
      "epoch": 0.4755165581658647,
      "grad_norm": 0.2961283326148987,
      "learning_rate": 0.00011304251955672165,
      "loss": 0.7944,
      "step": 630
    },
    {
      "epoch": 0.47627134635342955,
      "grad_norm": 0.23136718571186066,
      "learning_rate": 0.00011280008722839552,
      "loss": 0.6302,
      "step": 631
    },
    {
      "epoch": 0.4770261345409944,
      "grad_norm": 0.2261289805173874,
      "learning_rate": 0.00011255757839218324,
      "loss": 0.5547,
      "step": 632
    },
    {
      "epoch": 0.4777809227285593,
      "grad_norm": 0.463712215423584,
      "learning_rate": 0.00011231499449759355,
      "loss": 0.6469,
      "step": 633
    },
    {
      "epoch": 0.4785357109161242,
      "grad_norm": 0.36539945006370544,
      "learning_rate": 0.00011207233699458382,
      "loss": 0.6644,
      "step": 634
    },
    {
      "epoch": 0.47929049910368904,
      "grad_norm": 0.39580103754997253,
      "learning_rate": 0.00011182960733355141,
      "loss": 0.6211,
      "step": 635
    },
    {
      "epoch": 0.4800452872912539,
      "grad_norm": 0.4169537127017975,
      "learning_rate": 0.00011158680696532502,
      "loss": 0.6606,
      "step": 636
    },
    {
      "epoch": 0.48080007547881876,
      "grad_norm": 0.41441041231155396,
      "learning_rate": 0.00011134393734115587,
      "loss": 0.708,
      "step": 637
    },
    {
      "epoch": 0.4815548636663836,
      "grad_norm": 0.29651451110839844,
      "learning_rate": 0.0001111009999127092,
      "loss": 0.6719,
      "step": 638
    },
    {
      "epoch": 0.4823096518539485,
      "grad_norm": 0.25648635625839233,
      "learning_rate": 0.00011085799613205552,
      "loss": 0.5927,
      "step": 639
    },
    {
      "epoch": 0.48306444004151333,
      "grad_norm": 0.31446126103401184,
      "learning_rate": 0.00011061492745166193,
      "loss": 0.7638,
      "step": 640
    },
    {
      "epoch": 0.4838192282290782,
      "grad_norm": 0.5408953428268433,
      "learning_rate": 0.00011037179532438344,
      "loss": 0.813,
      "step": 641
    },
    {
      "epoch": 0.4845740164166431,
      "grad_norm": 0.2603418827056885,
      "learning_rate": 0.00011012860120345428,
      "loss": 0.6792,
      "step": 642
    },
    {
      "epoch": 0.48532880460420796,
      "grad_norm": 0.3458217978477478,
      "learning_rate": 0.00010988534654247928,
      "loss": 0.5958,
      "step": 643
    },
    {
      "epoch": 0.4860835927917728,
      "grad_norm": 0.4672873020172119,
      "learning_rate": 0.00010964203279542509,
      "loss": 0.6377,
      "step": 644
    },
    {
      "epoch": 0.4868383809793377,
      "grad_norm": 0.3017179071903229,
      "learning_rate": 0.00010939866141661147,
      "loss": 0.4943,
      "step": 645
    },
    {
      "epoch": 0.48759316916690254,
      "grad_norm": 0.3604383170604706,
      "learning_rate": 0.00010915523386070277,
      "loss": 0.737,
      "step": 646
    },
    {
      "epoch": 0.4883479573544674,
      "grad_norm": 0.26211005449295044,
      "learning_rate": 0.00010891175158269899,
      "loss": 0.7375,
      "step": 647
    },
    {
      "epoch": 0.48910274554203226,
      "grad_norm": 0.28630051016807556,
      "learning_rate": 0.00010866821603792732,
      "loss": 0.6702,
      "step": 648
    },
    {
      "epoch": 0.4898575337295971,
      "grad_norm": 0.3138788938522339,
      "learning_rate": 0.00010842462868203329,
      "loss": 0.7597,
      "step": 649
    },
    {
      "epoch": 0.490612321917162,
      "grad_norm": 0.31483638286590576,
      "learning_rate": 0.00010818099097097207,
      "loss": 0.5918,
      "step": 650
    },
    {
      "epoch": 0.49136711010472683,
      "grad_norm": 0.20824091136455536,
      "learning_rate": 0.0001079373043609999,
      "loss": 0.6542,
      "step": 651
    },
    {
      "epoch": 0.49212189829229175,
      "grad_norm": 0.277408242225647,
      "learning_rate": 0.0001076935703086652,
      "loss": 0.6531,
      "step": 652
    },
    {
      "epoch": 0.4928766864798566,
      "grad_norm": 0.2789154648780823,
      "learning_rate": 0.00010744979027080002,
      "loss": 0.6554,
      "step": 653
    },
    {
      "epoch": 0.49363147466742147,
      "grad_norm": 0.2698747515678406,
      "learning_rate": 0.00010720596570451127,
      "loss": 0.5528,
      "step": 654
    },
    {
      "epoch": 0.4943862628549863,
      "grad_norm": 0.355709046125412,
      "learning_rate": 0.00010696209806717199,
      "loss": 0.6895,
      "step": 655
    },
    {
      "epoch": 0.4951410510425512,
      "grad_norm": 0.5847623348236084,
      "learning_rate": 0.00010671818881641269,
      "loss": 0.7171,
      "step": 656
    },
    {
      "epoch": 0.49589583923011604,
      "grad_norm": 0.7119383215904236,
      "learning_rate": 0.00010647423941011254,
      "loss": 0.7309,
      "step": 657
    },
    {
      "epoch": 0.4966506274176809,
      "grad_norm": 0.39026495814323425,
      "learning_rate": 0.00010623025130639084,
      "loss": 0.6384,
      "step": 658
    },
    {
      "epoch": 0.49740541560524576,
      "grad_norm": 0.24413372576236725,
      "learning_rate": 0.00010598622596359809,
      "loss": 0.7357,
      "step": 659
    },
    {
      "epoch": 0.4981602037928106,
      "grad_norm": 0.3472595810890198,
      "learning_rate": 0.00010574216484030741,
      "loss": 0.6448,
      "step": 660
    },
    {
      "epoch": 0.49891499198037553,
      "grad_norm": 0.33154767751693726,
      "learning_rate": 0.0001054980693953058,
      "loss": 0.6139,
      "step": 661
    },
    {
      "epoch": 0.4996697801679404,
      "grad_norm": 0.8366484045982361,
      "learning_rate": 0.00010525394108758537,
      "loss": 0.5481,
      "step": 662
    },
    {
      "epoch": 0.5004245683555052,
      "grad_norm": 0.357353538274765,
      "learning_rate": 0.00010500978137633469,
      "loss": 0.6755,
      "step": 663
    },
    {
      "epoch": 0.5011793565430701,
      "grad_norm": 1.083487629890442,
      "learning_rate": 0.00010476559172092999,
      "loss": 0.7612,
      "step": 664
    },
    {
      "epoch": 0.5019341447306349,
      "grad_norm": 0.32230255007743835,
      "learning_rate": 0.00010452137358092653,
      "loss": 0.6856,
      "step": 665
    },
    {
      "epoch": 0.5026889329181998,
      "grad_norm": 0.3222487270832062,
      "learning_rate": 0.0001042771284160498,
      "loss": 0.6275,
      "step": 666
    },
    {
      "epoch": 0.5034437211057647,
      "grad_norm": 0.3279520869255066,
      "learning_rate": 0.00010403285768618683,
      "loss": 0.6548,
      "step": 667
    },
    {
      "epoch": 0.5041985092933295,
      "grad_norm": 0.2512964606285095,
      "learning_rate": 0.00010378856285137743,
      "loss": 0.8313,
      "step": 668
    },
    {
      "epoch": 0.5049532974808945,
      "grad_norm": 0.3418545722961426,
      "learning_rate": 0.00010354424537180554,
      "loss": 0.7026,
      "step": 669
    },
    {
      "epoch": 0.5057080856684593,
      "grad_norm": 0.29745858907699585,
      "learning_rate": 0.0001032999067077904,
      "loss": 0.6,
      "step": 670
    },
    {
      "epoch": 0.5064628738560242,
      "grad_norm": 0.21576832234859467,
      "learning_rate": 0.00010305554831977788,
      "loss": 0.7078,
      "step": 671
    },
    {
      "epoch": 0.507217662043589,
      "grad_norm": 0.3043188154697418,
      "learning_rate": 0.0001028111716683318,
      "loss": 0.7577,
      "step": 672
    },
    {
      "epoch": 0.5079724502311539,
      "grad_norm": 0.22976846992969513,
      "learning_rate": 0.00010256677821412508,
      "loss": 0.5686,
      "step": 673
    },
    {
      "epoch": 0.5087272384187187,
      "grad_norm": 0.39115506410598755,
      "learning_rate": 0.00010232236941793105,
      "loss": 0.8038,
      "step": 674
    },
    {
      "epoch": 0.5094820266062836,
      "grad_norm": 0.2932363450527191,
      "learning_rate": 0.00010207794674061484,
      "loss": 0.6615,
      "step": 675
    },
    {
      "epoch": 0.5102368147938485,
      "grad_norm": 0.25258541107177734,
      "learning_rate": 0.00010183351164312447,
      "loss": 0.6088,
      "step": 676
    },
    {
      "epoch": 0.5109916029814133,
      "grad_norm": 0.2571875751018524,
      "learning_rate": 0.0001015890655864822,
      "loss": 0.5681,
      "step": 677
    },
    {
      "epoch": 0.5117463911689782,
      "grad_norm": 0.23690778017044067,
      "learning_rate": 0.00010134461003177584,
      "loss": 0.5578,
      "step": 678
    },
    {
      "epoch": 0.512501179356543,
      "grad_norm": 0.3330206871032715,
      "learning_rate": 0.00010110014644014993,
      "loss": 0.6523,
      "step": 679
    },
    {
      "epoch": 0.513255967544108,
      "grad_norm": 0.3168037533760071,
      "learning_rate": 0.00010085567627279709,
      "loss": 0.7069,
      "step": 680
    },
    {
      "epoch": 0.5140107557316728,
      "grad_norm": 0.23814819753170013,
      "learning_rate": 0.00010061120099094916,
      "loss": 0.69,
      "step": 681
    },
    {
      "epoch": 0.5147655439192377,
      "grad_norm": 0.49730536341667175,
      "learning_rate": 0.00010036672205586866,
      "loss": 0.5888,
      "step": 682
    },
    {
      "epoch": 0.5155203321068025,
      "grad_norm": 0.439862459897995,
      "learning_rate": 0.00010012224092883985,
      "loss": 0.7586,
      "step": 683
    },
    {
      "epoch": 0.5162751202943674,
      "grad_norm": 0.22201918065547943,
      "learning_rate": 9.987775907116016e-05,
      "loss": 0.6447,
      "step": 684
    },
    {
      "epoch": 0.5170299084819323,
      "grad_norm": 0.4231988787651062,
      "learning_rate": 9.963327794413136e-05,
      "loss": 0.791,
      "step": 685
    },
    {
      "epoch": 0.5177846966694971,
      "grad_norm": 0.37123748660087585,
      "learning_rate": 9.938879900905086e-05,
      "loss": 0.5711,
      "step": 686
    },
    {
      "epoch": 0.518539484857062,
      "grad_norm": 0.40220722556114197,
      "learning_rate": 9.914432372720293e-05,
      "loss": 0.6442,
      "step": 687
    },
    {
      "epoch": 0.5192942730446268,
      "grad_norm": 0.23780198395252228,
      "learning_rate": 9.889985355985008e-05,
      "loss": 0.6169,
      "step": 688
    },
    {
      "epoch": 0.5200490612321917,
      "grad_norm": 0.3231881558895111,
      "learning_rate": 9.865538996822417e-05,
      "loss": 0.6517,
      "step": 689
    },
    {
      "epoch": 0.5208038494197565,
      "grad_norm": 0.5109191536903381,
      "learning_rate": 9.841093441351782e-05,
      "loss": 0.5996,
      "step": 690
    },
    {
      "epoch": 0.5215586376073215,
      "grad_norm": 0.3555682301521301,
      "learning_rate": 9.816648835687556e-05,
      "loss": 0.8665,
      "step": 691
    },
    {
      "epoch": 0.5223134257948863,
      "grad_norm": 0.24395717680454254,
      "learning_rate": 9.792205325938519e-05,
      "loss": 0.7586,
      "step": 692
    },
    {
      "epoch": 0.5230682139824512,
      "grad_norm": 0.48694056272506714,
      "learning_rate": 9.767763058206896e-05,
      "loss": 0.7215,
      "step": 693
    },
    {
      "epoch": 0.5238230021700161,
      "grad_norm": 0.26089370250701904,
      "learning_rate": 9.743322178587496e-05,
      "loss": 0.5908,
      "step": 694
    },
    {
      "epoch": 0.5245777903575809,
      "grad_norm": 0.4314495921134949,
      "learning_rate": 9.718882833166824e-05,
      "loss": 0.7366,
      "step": 695
    },
    {
      "epoch": 0.5253325785451458,
      "grad_norm": 0.545786440372467,
      "learning_rate": 9.694445168022213e-05,
      "loss": 0.5856,
      "step": 696
    },
    {
      "epoch": 0.5260873667327106,
      "grad_norm": 0.5742717385292053,
      "learning_rate": 9.670009329220963e-05,
      "loss": 0.8166,
      "step": 697
    },
    {
      "epoch": 0.5268421549202755,
      "grad_norm": 0.30425718426704407,
      "learning_rate": 9.64557546281945e-05,
      "loss": 0.7195,
      "step": 698
    },
    {
      "epoch": 0.5275969431078403,
      "grad_norm": 0.3523441255092621,
      "learning_rate": 9.62114371486226e-05,
      "loss": 0.7894,
      "step": 699
    },
    {
      "epoch": 0.5283517312954052,
      "grad_norm": 0.30096280574798584,
      "learning_rate": 9.596714231381322e-05,
      "loss": 0.5831,
      "step": 700
    },
    {
      "epoch": 0.52910651948297,
      "grad_norm": 0.3818857967853546,
      "learning_rate": 9.572287158395025e-05,
      "loss": 0.7196,
      "step": 701
    },
    {
      "epoch": 0.529861307670535,
      "grad_norm": 0.28144967555999756,
      "learning_rate": 9.547862641907351e-05,
      "loss": 0.5062,
      "step": 702
    },
    {
      "epoch": 0.5306160958580998,
      "grad_norm": 0.4330885112285614,
      "learning_rate": 9.523440827907006e-05,
      "loss": 0.5696,
      "step": 703
    },
    {
      "epoch": 0.5313708840456647,
      "grad_norm": 0.3000911474227905,
      "learning_rate": 9.499021862366532e-05,
      "loss": 0.7244,
      "step": 704
    },
    {
      "epoch": 0.5321256722332296,
      "grad_norm": 0.2175789624452591,
      "learning_rate": 9.474605891241464e-05,
      "loss": 0.7343,
      "step": 705
    },
    {
      "epoch": 0.5328804604207944,
      "grad_norm": 0.22964408993721008,
      "learning_rate": 9.45019306046942e-05,
      "loss": 0.6733,
      "step": 706
    },
    {
      "epoch": 0.5336352486083593,
      "grad_norm": 0.25151577591896057,
      "learning_rate": 9.425783515969258e-05,
      "loss": 0.6442,
      "step": 707
    },
    {
      "epoch": 0.5343900367959241,
      "grad_norm": 0.2162325084209442,
      "learning_rate": 9.401377403640192e-05,
      "loss": 0.5514,
      "step": 708
    },
    {
      "epoch": 0.535144824983489,
      "grad_norm": 0.38889867067337036,
      "learning_rate": 9.376974869360917e-05,
      "loss": 0.7138,
      "step": 709
    },
    {
      "epoch": 0.5358996131710538,
      "grad_norm": 0.2217796891927719,
      "learning_rate": 9.352576058988747e-05,
      "loss": 0.57,
      "step": 710
    },
    {
      "epoch": 0.5366544013586187,
      "grad_norm": 0.4775952100753784,
      "learning_rate": 9.328181118358735e-05,
      "loss": 0.7416,
      "step": 711
    },
    {
      "epoch": 0.5374091895461836,
      "grad_norm": 0.3231016993522644,
      "learning_rate": 9.303790193282804e-05,
      "loss": 0.7338,
      "step": 712
    },
    {
      "epoch": 0.5381639777337485,
      "grad_norm": 0.3518795073032379,
      "learning_rate": 9.279403429548876e-05,
      "loss": 0.7242,
      "step": 713
    },
    {
      "epoch": 0.5389187659213134,
      "grad_norm": 0.514863908290863,
      "learning_rate": 9.255020972919999e-05,
      "loss": 0.6143,
      "step": 714
    },
    {
      "epoch": 0.5396735541088782,
      "grad_norm": 0.22060760855674744,
      "learning_rate": 9.230642969133483e-05,
      "loss": 0.6557,
      "step": 715
    },
    {
      "epoch": 0.5404283422964431,
      "grad_norm": 0.3280368447303772,
      "learning_rate": 9.206269563900013e-05,
      "loss": 0.6115,
      "step": 716
    },
    {
      "epoch": 0.5411831304840079,
      "grad_norm": 0.4330524802207947,
      "learning_rate": 9.181900902902794e-05,
      "loss": 0.66,
      "step": 717
    },
    {
      "epoch": 0.5419379186715728,
      "grad_norm": 0.3006409704685211,
      "learning_rate": 9.157537131796673e-05,
      "loss": 0.6294,
      "step": 718
    },
    {
      "epoch": 0.5426927068591376,
      "grad_norm": 0.4032120704650879,
      "learning_rate": 9.133178396207269e-05,
      "loss": 0.6798,
      "step": 719
    },
    {
      "epoch": 0.5434474950467025,
      "grad_norm": 0.45081838965415955,
      "learning_rate": 9.108824841730103e-05,
      "loss": 0.8507,
      "step": 720
    },
    {
      "epoch": 0.5442022832342673,
      "grad_norm": 0.41532203555107117,
      "learning_rate": 9.084476613929725e-05,
      "loss": 0.6593,
      "step": 721
    },
    {
      "epoch": 0.5449570714218323,
      "grad_norm": 0.29806938767433167,
      "learning_rate": 9.060133858338853e-05,
      "loss": 0.7817,
      "step": 722
    },
    {
      "epoch": 0.5457118596093972,
      "grad_norm": 0.28623339533805847,
      "learning_rate": 9.035796720457495e-05,
      "loss": 0.6067,
      "step": 723
    },
    {
      "epoch": 0.546466647796962,
      "grad_norm": 0.29907873272895813,
      "learning_rate": 9.011465345752073e-05,
      "loss": 0.6196,
      "step": 724
    },
    {
      "epoch": 0.5472214359845269,
      "grad_norm": 0.3382197320461273,
      "learning_rate": 8.987139879654575e-05,
      "loss": 0.6923,
      "step": 725
    },
    {
      "epoch": 0.5479762241720917,
      "grad_norm": 0.8087363243103027,
      "learning_rate": 8.962820467561659e-05,
      "loss": 0.6824,
      "step": 726
    },
    {
      "epoch": 0.5487310123596566,
      "grad_norm": 0.4395870864391327,
      "learning_rate": 8.93850725483381e-05,
      "loss": 0.6196,
      "step": 727
    },
    {
      "epoch": 0.5494858005472214,
      "grad_norm": 0.42364901304244995,
      "learning_rate": 8.91420038679445e-05,
      "loss": 0.6953,
      "step": 728
    },
    {
      "epoch": 0.5502405887347863,
      "grad_norm": 0.44184979796409607,
      "learning_rate": 8.889900008729083e-05,
      "loss": 0.6937,
      "step": 729
    },
    {
      "epoch": 0.5509953769223511,
      "grad_norm": 0.26746973395347595,
      "learning_rate": 8.865606265884416e-05,
      "loss": 0.7786,
      "step": 730
    },
    {
      "epoch": 0.551750165109916,
      "grad_norm": 0.19981546700000763,
      "learning_rate": 8.841319303467502e-05,
      "loss": 0.5282,
      "step": 731
    },
    {
      "epoch": 0.552504953297481,
      "grad_norm": 0.23025472462177277,
      "learning_rate": 8.81703926664486e-05,
      "loss": 0.4967,
      "step": 732
    },
    {
      "epoch": 0.5532597414850458,
      "grad_norm": 0.2669488191604614,
      "learning_rate": 8.792766300541622e-05,
      "loss": 0.6677,
      "step": 733
    },
    {
      "epoch": 0.5540145296726107,
      "grad_norm": 0.3710789382457733,
      "learning_rate": 8.76850055024065e-05,
      "loss": 0.7714,
      "step": 734
    },
    {
      "epoch": 0.5547693178601755,
      "grad_norm": 0.26778608560562134,
      "learning_rate": 8.744242160781681e-05,
      "loss": 0.5738,
      "step": 735
    },
    {
      "epoch": 0.5555241060477404,
      "grad_norm": 0.31208527088165283,
      "learning_rate": 8.719991277160452e-05,
      "loss": 0.6392,
      "step": 736
    },
    {
      "epoch": 0.5562788942353052,
      "grad_norm": 0.38729196786880493,
      "learning_rate": 8.69574804432784e-05,
      "loss": 0.7297,
      "step": 737
    },
    {
      "epoch": 0.5570336824228701,
      "grad_norm": 0.23564033210277557,
      "learning_rate": 8.671512607188985e-05,
      "loss": 0.6391,
      "step": 738
    },
    {
      "epoch": 0.5577884706104349,
      "grad_norm": 0.28251415491104126,
      "learning_rate": 8.647285110602444e-05,
      "loss": 0.6074,
      "step": 739
    },
    {
      "epoch": 0.5585432587979998,
      "grad_norm": 0.3790563642978668,
      "learning_rate": 8.623065699379298e-05,
      "loss": 0.6597,
      "step": 740
    },
    {
      "epoch": 0.5592980469855646,
      "grad_norm": 0.2328951060771942,
      "learning_rate": 8.598854518282311e-05,
      "loss": 0.6182,
      "step": 741
    },
    {
      "epoch": 0.5600528351731295,
      "grad_norm": 0.32889077067375183,
      "learning_rate": 8.574651712025049e-05,
      "loss": 0.6426,
      "step": 742
    },
    {
      "epoch": 0.5608076233606945,
      "grad_norm": 0.2101319581270218,
      "learning_rate": 8.550457425271022e-05,
      "loss": 0.5118,
      "step": 743
    },
    {
      "epoch": 0.5615624115482593,
      "grad_norm": 0.23277752101421356,
      "learning_rate": 8.526271802632812e-05,
      "loss": 0.599,
      "step": 744
    },
    {
      "epoch": 0.5623171997358242,
      "grad_norm": 0.31592637300491333,
      "learning_rate": 8.50209498867123e-05,
      "loss": 0.7134,
      "step": 745
    },
    {
      "epoch": 0.563071987923389,
      "grad_norm": 0.37598279118537903,
      "learning_rate": 8.477927127894424e-05,
      "loss": 0.7182,
      "step": 746
    },
    {
      "epoch": 0.5638267761109539,
      "grad_norm": 0.4367167353630066,
      "learning_rate": 8.453768364757027e-05,
      "loss": 0.5897,
      "step": 747
    },
    {
      "epoch": 0.5645815642985187,
      "grad_norm": 0.27462416887283325,
      "learning_rate": 8.429618843659297e-05,
      "loss": 0.7378,
      "step": 748
    },
    {
      "epoch": 0.5653363524860836,
      "grad_norm": 0.4099229872226715,
      "learning_rate": 8.405478708946254e-05,
      "loss": 0.6395,
      "step": 749
    },
    {
      "epoch": 0.5660911406736484,
      "grad_norm": 0.2926162779331207,
      "learning_rate": 8.38134810490681e-05,
      "loss": 0.6923,
      "step": 750
    },
    {
      "epoch": 0.5668459288612133,
      "grad_norm": 0.329886794090271,
      "learning_rate": 8.35722717577291e-05,
      "loss": 0.5607,
      "step": 751
    },
    {
      "epoch": 0.5676007170487782,
      "grad_norm": 0.22328637540340424,
      "learning_rate": 8.333116065718676e-05,
      "loss": 0.7165,
      "step": 752
    },
    {
      "epoch": 0.568355505236343,
      "grad_norm": 0.3209885358810425,
      "learning_rate": 8.309014918859538e-05,
      "loss": 0.747,
      "step": 753
    },
    {
      "epoch": 0.569110293423908,
      "grad_norm": 0.3285164535045624,
      "learning_rate": 8.284923879251371e-05,
      "loss": 0.6528,
      "step": 754
    },
    {
      "epoch": 0.5698650816114728,
      "grad_norm": 2.711712598800659,
      "learning_rate": 8.260843090889641e-05,
      "loss": 0.7937,
      "step": 755
    },
    {
      "epoch": 0.5706198697990377,
      "grad_norm": 0.2926597595214844,
      "learning_rate": 8.236772697708537e-05,
      "loss": 0.6912,
      "step": 756
    },
    {
      "epoch": 0.5713746579866025,
      "grad_norm": 0.4050889313220978,
      "learning_rate": 8.212712843580124e-05,
      "loss": 0.7552,
      "step": 757
    },
    {
      "epoch": 0.5721294461741674,
      "grad_norm": 0.18140175938606262,
      "learning_rate": 8.188663672313465e-05,
      "loss": 0.5569,
      "step": 758
    },
    {
      "epoch": 0.5728842343617322,
      "grad_norm": 0.30796483159065247,
      "learning_rate": 8.16462532765377e-05,
      "loss": 0.6158,
      "step": 759
    },
    {
      "epoch": 0.5736390225492971,
      "grad_norm": 0.24730604887008667,
      "learning_rate": 8.140597953281544e-05,
      "loss": 0.6467,
      "step": 760
    },
    {
      "epoch": 0.574393810736862,
      "grad_norm": 0.19059793651103973,
      "learning_rate": 8.116581692811711e-05,
      "loss": 0.7204,
      "step": 761
    },
    {
      "epoch": 0.5751485989244268,
      "grad_norm": 0.2155088484287262,
      "learning_rate": 8.092576689792775e-05,
      "loss": 0.5619,
      "step": 762
    },
    {
      "epoch": 0.5759033871119917,
      "grad_norm": 0.2816886603832245,
      "learning_rate": 8.068583087705946e-05,
      "loss": 0.4959,
      "step": 763
    },
    {
      "epoch": 0.5766581752995565,
      "grad_norm": 0.3044871389865875,
      "learning_rate": 8.044601029964293e-05,
      "loss": 0.7732,
      "step": 764
    },
    {
      "epoch": 0.5774129634871215,
      "grad_norm": 0.2026183009147644,
      "learning_rate": 8.020630659911881e-05,
      "loss": 0.53,
      "step": 765
    },
    {
      "epoch": 0.5781677516746863,
      "grad_norm": 0.3302825689315796,
      "learning_rate": 7.996672120822918e-05,
      "loss": 0.6606,
      "step": 766
    },
    {
      "epoch": 0.5789225398622512,
      "grad_norm": 0.26263314485549927,
      "learning_rate": 7.972725555900895e-05,
      "loss": 0.6467,
      "step": 767
    },
    {
      "epoch": 0.579677328049816,
      "grad_norm": 0.34307944774627686,
      "learning_rate": 7.948791108277729e-05,
      "loss": 0.6942,
      "step": 768
    },
    {
      "epoch": 0.5804321162373809,
      "grad_norm": 0.5520722270011902,
      "learning_rate": 7.924868921012918e-05,
      "loss": 0.6283,
      "step": 769
    },
    {
      "epoch": 0.5811869044249458,
      "grad_norm": 0.24169306457042694,
      "learning_rate": 7.900959137092672e-05,
      "loss": 0.5758,
      "step": 770
    },
    {
      "epoch": 0.5819416926125106,
      "grad_norm": 0.21455155313014984,
      "learning_rate": 7.877061899429066e-05,
      "loss": 0.5896,
      "step": 771
    },
    {
      "epoch": 0.5826964808000755,
      "grad_norm": 0.3757900297641754,
      "learning_rate": 7.853177350859182e-05,
      "loss": 0.6962,
      "step": 772
    },
    {
      "epoch": 0.5834512689876403,
      "grad_norm": 0.3107989430427551,
      "learning_rate": 7.829305634144264e-05,
      "loss": 0.6619,
      "step": 773
    },
    {
      "epoch": 0.5842060571752052,
      "grad_norm": 0.29923322796821594,
      "learning_rate": 7.80544689196885e-05,
      "loss": 0.5959,
      "step": 774
    },
    {
      "epoch": 0.58496084536277,
      "grad_norm": 0.2398703396320343,
      "learning_rate": 7.781601266939936e-05,
      "loss": 0.6688,
      "step": 775
    },
    {
      "epoch": 0.585715633550335,
      "grad_norm": 0.39115238189697266,
      "learning_rate": 7.757768901586106e-05,
      "loss": 0.5717,
      "step": 776
    },
    {
      "epoch": 0.5864704217378998,
      "grad_norm": 0.2048291116952896,
      "learning_rate": 7.733949938356696e-05,
      "loss": 0.5621,
      "step": 777
    },
    {
      "epoch": 0.5872252099254647,
      "grad_norm": 0.29999491572380066,
      "learning_rate": 7.71014451962093e-05,
      "loss": 0.673,
      "step": 778
    },
    {
      "epoch": 0.5879799981130295,
      "grad_norm": 0.25957146286964417,
      "learning_rate": 7.686352787667083e-05,
      "loss": 0.6362,
      "step": 779
    },
    {
      "epoch": 0.5887347863005944,
      "grad_norm": 0.2675851583480835,
      "learning_rate": 7.662574884701613e-05,
      "loss": 0.6546,
      "step": 780
    },
    {
      "epoch": 0.5894895744881593,
      "grad_norm": 1.2925868034362793,
      "learning_rate": 7.638810952848326e-05,
      "loss": 0.6644,
      "step": 781
    },
    {
      "epoch": 0.5902443626757241,
      "grad_norm": 0.30115386843681335,
      "learning_rate": 7.615061134147521e-05,
      "loss": 0.6453,
      "step": 782
    },
    {
      "epoch": 0.590999150863289,
      "grad_norm": 0.28809088468551636,
      "learning_rate": 7.591325570555135e-05,
      "loss": 0.653,
      "step": 783
    },
    {
      "epoch": 0.5917539390508538,
      "grad_norm": 0.24987299740314484,
      "learning_rate": 7.567604403941906e-05,
      "loss": 0.6644,
      "step": 784
    },
    {
      "epoch": 0.5925087272384187,
      "grad_norm": 0.8539102077484131,
      "learning_rate": 7.54389777609252e-05,
      "loss": 0.7199,
      "step": 785
    },
    {
      "epoch": 0.5932635154259835,
      "grad_norm": 0.3040579557418823,
      "learning_rate": 7.520205828704753e-05,
      "loss": 0.5985,
      "step": 786
    },
    {
      "epoch": 0.5940183036135485,
      "grad_norm": 0.3526783883571625,
      "learning_rate": 7.496528703388647e-05,
      "loss": 0.6689,
      "step": 787
    },
    {
      "epoch": 0.5947730918011133,
      "grad_norm": 0.564152717590332,
      "learning_rate": 7.472866541665646e-05,
      "loss": 0.6453,
      "step": 788
    },
    {
      "epoch": 0.5955278799886782,
      "grad_norm": 0.4983164668083191,
      "learning_rate": 7.44921948496775e-05,
      "loss": 0.7555,
      "step": 789
    },
    {
      "epoch": 0.5962826681762431,
      "grad_norm": 0.38057827949523926,
      "learning_rate": 7.425587674636676e-05,
      "loss": 0.6752,
      "step": 790
    },
    {
      "epoch": 0.5970374563638079,
      "grad_norm": 0.5071314573287964,
      "learning_rate": 7.401971251923014e-05,
      "loss": 0.6902,
      "step": 791
    },
    {
      "epoch": 0.5977922445513728,
      "grad_norm": 0.39532041549682617,
      "learning_rate": 7.378370357985378e-05,
      "loss": 0.6323,
      "step": 792
    },
    {
      "epoch": 0.5985470327389376,
      "grad_norm": 0.17288599908351898,
      "learning_rate": 7.354785133889565e-05,
      "loss": 0.668,
      "step": 793
    },
    {
      "epoch": 0.5993018209265025,
      "grad_norm": 0.23105229437351227,
      "learning_rate": 7.331215720607714e-05,
      "loss": 0.7441,
      "step": 794
    },
    {
      "epoch": 0.6000566091140673,
      "grad_norm": 0.1719582974910736,
      "learning_rate": 7.307662259017454e-05,
      "loss": 0.625,
      "step": 795
    },
    {
      "epoch": 0.6008113973016322,
      "grad_norm": 0.2779467701911926,
      "learning_rate": 7.284124889901074e-05,
      "loss": 0.6413,
      "step": 796
    },
    {
      "epoch": 0.601566185489197,
      "grad_norm": 0.29810991883277893,
      "learning_rate": 7.260603753944673e-05,
      "loss": 0.7419,
      "step": 797
    },
    {
      "epoch": 0.602320973676762,
      "grad_norm": 0.2749348282814026,
      "learning_rate": 7.237098991737324e-05,
      "loss": 0.7505,
      "step": 798
    },
    {
      "epoch": 0.6030757618643269,
      "grad_norm": 0.4243059456348419,
      "learning_rate": 7.213610743770233e-05,
      "loss": 0.6802,
      "step": 799
    },
    {
      "epoch": 0.6038305500518917,
      "grad_norm": 0.4168979823589325,
      "learning_rate": 7.190139150435898e-05,
      "loss": 0.6927,
      "step": 800
    },
    {
      "epoch": 0.6038305500518917,
      "eval_loss": 0.6699516773223877,
      "eval_runtime": 3193.0239,
      "eval_samples_per_second": 2.951,
      "eval_steps_per_second": 0.738,
      "step": 800
    },
    {
      "epoch": 0.6045853382394566,
      "grad_norm": 0.3626990020275116,
      "learning_rate": 7.166684352027265e-05,
      "loss": 0.5491,
      "step": 801
    },
    {
      "epoch": 0.6053401264270214,
      "grad_norm": 0.36172422766685486,
      "learning_rate": 7.143246488736896e-05,
      "loss": 0.7958,
      "step": 802
    },
    {
      "epoch": 0.6060949146145863,
      "grad_norm": 0.3848172128200531,
      "learning_rate": 7.119825700656137e-05,
      "loss": 0.8483,
      "step": 803
    },
    {
      "epoch": 0.6068497028021511,
      "grad_norm": 0.29314130544662476,
      "learning_rate": 7.096422127774265e-05,
      "loss": 0.6975,
      "step": 804
    },
    {
      "epoch": 0.607604490989716,
      "grad_norm": 0.3318146765232086,
      "learning_rate": 7.073035909977662e-05,
      "loss": 0.8297,
      "step": 805
    },
    {
      "epoch": 0.6083592791772808,
      "grad_norm": 0.461789071559906,
      "learning_rate": 7.049667187048974e-05,
      "loss": 0.733,
      "step": 806
    },
    {
      "epoch": 0.6091140673648457,
      "grad_norm": 0.3976203203201294,
      "learning_rate": 7.026316098666282e-05,
      "loss": 0.692,
      "step": 807
    },
    {
      "epoch": 0.6098688555524107,
      "grad_norm": 0.3985314965248108,
      "learning_rate": 7.00298278440226e-05,
      "loss": 0.7941,
      "step": 808
    },
    {
      "epoch": 0.6106236437399755,
      "grad_norm": 0.43836700916290283,
      "learning_rate": 6.979667383723346e-05,
      "loss": 0.6001,
      "step": 809
    },
    {
      "epoch": 0.6113784319275404,
      "grad_norm": 0.3184882402420044,
      "learning_rate": 6.956370035988899e-05,
      "loss": 0.5899,
      "step": 810
    },
    {
      "epoch": 0.6121332201151052,
      "grad_norm": 0.4548240303993225,
      "learning_rate": 6.933090880450386e-05,
      "loss": 0.705,
      "step": 811
    },
    {
      "epoch": 0.6128880083026701,
      "grad_norm": 0.19337843358516693,
      "learning_rate": 6.909830056250527e-05,
      "loss": 0.6789,
      "step": 812
    },
    {
      "epoch": 0.6136427964902349,
      "grad_norm": 0.38472363352775574,
      "learning_rate": 6.886587702422474e-05,
      "loss": 0.6374,
      "step": 813
    },
    {
      "epoch": 0.6143975846777998,
      "grad_norm": 0.3091118633747101,
      "learning_rate": 6.863363957888979e-05,
      "loss": 0.7471,
      "step": 814
    },
    {
      "epoch": 0.6151523728653646,
      "grad_norm": 0.27329716086387634,
      "learning_rate": 6.840158961461567e-05,
      "loss": 0.6811,
      "step": 815
    },
    {
      "epoch": 0.6159071610529295,
      "grad_norm": 0.2906245291233063,
      "learning_rate": 6.816972851839701e-05,
      "loss": 0.6991,
      "step": 816
    },
    {
      "epoch": 0.6166619492404943,
      "grad_norm": 0.3744916021823883,
      "learning_rate": 6.793805767609953e-05,
      "loss": 0.6266,
      "step": 817
    },
    {
      "epoch": 0.6174167374280592,
      "grad_norm": 0.3884618282318115,
      "learning_rate": 6.770657847245183e-05,
      "loss": 0.6115,
      "step": 818
    },
    {
      "epoch": 0.6181715256156242,
      "grad_norm": 0.24910594522953033,
      "learning_rate": 6.7475292291037e-05,
      "loss": 0.5602,
      "step": 819
    },
    {
      "epoch": 0.618926313803189,
      "grad_norm": 0.30159464478492737,
      "learning_rate": 6.724420051428444e-05,
      "loss": 0.5393,
      "step": 820
    },
    {
      "epoch": 0.6196811019907539,
      "grad_norm": 0.3137858808040619,
      "learning_rate": 6.701330452346155e-05,
      "loss": 0.5916,
      "step": 821
    },
    {
      "epoch": 0.6204358901783187,
      "grad_norm": 0.2354368269443512,
      "learning_rate": 6.678260569866551e-05,
      "loss": 0.6481,
      "step": 822
    },
    {
      "epoch": 0.6211906783658836,
      "grad_norm": 0.21267837285995483,
      "learning_rate": 6.655210541881503e-05,
      "loss": 0.612,
      "step": 823
    },
    {
      "epoch": 0.6219454665534484,
      "grad_norm": 0.2983770966529846,
      "learning_rate": 6.632180506164202e-05,
      "loss": 0.5829,
      "step": 824
    },
    {
      "epoch": 0.6227002547410133,
      "grad_norm": 0.2267133891582489,
      "learning_rate": 6.609170600368347e-05,
      "loss": 0.582,
      "step": 825
    },
    {
      "epoch": 0.6234550429285781,
      "grad_norm": 0.18234440684318542,
      "learning_rate": 6.586180962027322e-05,
      "loss": 0.6406,
      "step": 826
    },
    {
      "epoch": 0.624209831116143,
      "grad_norm": 0.27222946286201477,
      "learning_rate": 6.56321172855336e-05,
      "loss": 0.6528,
      "step": 827
    },
    {
      "epoch": 0.624964619303708,
      "grad_norm": 0.34623605012893677,
      "learning_rate": 6.540263037236738e-05,
      "loss": 0.649,
      "step": 828
    },
    {
      "epoch": 0.6257194074912727,
      "grad_norm": 0.450777143239975,
      "learning_rate": 6.517335025244949e-05,
      "loss": 0.7347,
      "step": 829
    },
    {
      "epoch": 0.6264741956788377,
      "grad_norm": 0.24704444408416748,
      "learning_rate": 6.494427829621883e-05,
      "loss": 0.6485,
      "step": 830
    },
    {
      "epoch": 0.6272289838664025,
      "grad_norm": 0.4070065915584564,
      "learning_rate": 6.471541587287003e-05,
      "loss": 0.7719,
      "step": 831
    },
    {
      "epoch": 0.6279837720539674,
      "grad_norm": 0.4086577296257019,
      "learning_rate": 6.448676435034538e-05,
      "loss": 0.6129,
      "step": 832
    },
    {
      "epoch": 0.6287385602415322,
      "grad_norm": 0.41377270221710205,
      "learning_rate": 6.425832509532652e-05,
      "loss": 0.7621,
      "step": 833
    },
    {
      "epoch": 0.6294933484290971,
      "grad_norm": 0.26865315437316895,
      "learning_rate": 6.40300994732264e-05,
      "loss": 0.7646,
      "step": 834
    },
    {
      "epoch": 0.6302481366166619,
      "grad_norm": 0.4178389310836792,
      "learning_rate": 6.380208884818105e-05,
      "loss": 0.5964,
      "step": 835
    },
    {
      "epoch": 0.6310029248042268,
      "grad_norm": 0.4668399393558502,
      "learning_rate": 6.357429458304135e-05,
      "loss": 0.6103,
      "step": 836
    },
    {
      "epoch": 0.6317577129917917,
      "grad_norm": 0.2772664427757263,
      "learning_rate": 6.334671803936507e-05,
      "loss": 0.5984,
      "step": 837
    },
    {
      "epoch": 0.6325125011793565,
      "grad_norm": 0.2511301040649414,
      "learning_rate": 6.311936057740859e-05,
      "loss": 0.5385,
      "step": 838
    },
    {
      "epoch": 0.6332672893669214,
      "grad_norm": 0.30314648151397705,
      "learning_rate": 6.289222355611881e-05,
      "loss": 0.5808,
      "step": 839
    },
    {
      "epoch": 0.6340220775544863,
      "grad_norm": 0.26070189476013184,
      "learning_rate": 6.266530833312498e-05,
      "loss": 0.5168,
      "step": 840
    },
    {
      "epoch": 0.6347768657420512,
      "grad_norm": 0.37088435888290405,
      "learning_rate": 6.243861626473073e-05,
      "loss": 0.5925,
      "step": 841
    },
    {
      "epoch": 0.635531653929616,
      "grad_norm": 0.3667066693305969,
      "learning_rate": 6.221214870590578e-05,
      "loss": 0.7126,
      "step": 842
    },
    {
      "epoch": 0.6362864421171809,
      "grad_norm": 0.3355177342891693,
      "learning_rate": 6.198590701027796e-05,
      "loss": 0.6048,
      "step": 843
    },
    {
      "epoch": 0.6370412303047457,
      "grad_norm": 0.29189279675483704,
      "learning_rate": 6.175989253012507e-05,
      "loss": 0.7494,
      "step": 844
    },
    {
      "epoch": 0.6377960184923106,
      "grad_norm": 0.4016910195350647,
      "learning_rate": 6.153410661636683e-05,
      "loss": 0.8187,
      "step": 845
    },
    {
      "epoch": 0.6385508066798754,
      "grad_norm": 0.23765982687473297,
      "learning_rate": 6.130855061855676e-05,
      "loss": 0.6299,
      "step": 846
    },
    {
      "epoch": 0.6393055948674403,
      "grad_norm": 0.18742269277572632,
      "learning_rate": 6.108322588487418e-05,
      "loss": 0.6473,
      "step": 847
    },
    {
      "epoch": 0.6400603830550052,
      "grad_norm": 0.3133346736431122,
      "learning_rate": 6.085813376211607e-05,
      "loss": 0.7152,
      "step": 848
    },
    {
      "epoch": 0.64081517124257,
      "grad_norm": 0.23508110642433167,
      "learning_rate": 6.063327559568908e-05,
      "loss": 0.4884,
      "step": 849
    },
    {
      "epoch": 0.641569959430135,
      "grad_norm": 0.3065457344055176,
      "learning_rate": 6.040865272960148e-05,
      "loss": 0.5825,
      "step": 850
    },
    {
      "epoch": 0.6423247476176998,
      "grad_norm": 0.3392764627933502,
      "learning_rate": 6.018426650645512e-05,
      "loss": 0.5068,
      "step": 851
    },
    {
      "epoch": 0.6430795358052647,
      "grad_norm": 0.3480938971042633,
      "learning_rate": 5.996011826743737e-05,
      "loss": 0.7535,
      "step": 852
    },
    {
      "epoch": 0.6438343239928295,
      "grad_norm": 0.41247862577438354,
      "learning_rate": 5.973620935231318e-05,
      "loss": 0.641,
      "step": 853
    },
    {
      "epoch": 0.6445891121803944,
      "grad_norm": 0.2942664921283722,
      "learning_rate": 5.9512541099417e-05,
      "loss": 0.7474,
      "step": 854
    },
    {
      "epoch": 0.6453439003679592,
      "grad_norm": 0.20566880702972412,
      "learning_rate": 5.928911484564481e-05,
      "loss": 0.6521,
      "step": 855
    },
    {
      "epoch": 0.6460986885555241,
      "grad_norm": 0.2853389084339142,
      "learning_rate": 5.9065931926446146e-05,
      "loss": 0.7626,
      "step": 856
    },
    {
      "epoch": 0.646853476743089,
      "grad_norm": 0.2808678150177002,
      "learning_rate": 5.884299367581607e-05,
      "loss": 0.722,
      "step": 857
    },
    {
      "epoch": 0.6476082649306538,
      "grad_norm": 0.24462555348873138,
      "learning_rate": 5.862030142628727e-05,
      "loss": 0.6929,
      "step": 858
    },
    {
      "epoch": 0.6483630531182187,
      "grad_norm": 0.3061349391937256,
      "learning_rate": 5.8397856508922e-05,
      "loss": 0.6107,
      "step": 859
    },
    {
      "epoch": 0.6491178413057835,
      "grad_norm": 0.3085644841194153,
      "learning_rate": 5.8175660253304176e-05,
      "loss": 0.6973,
      "step": 860
    },
    {
      "epoch": 0.6498726294933485,
      "grad_norm": 0.2948172092437744,
      "learning_rate": 5.795371398753153e-05,
      "loss": 0.7082,
      "step": 861
    },
    {
      "epoch": 0.6506274176809133,
      "grad_norm": 0.22953428328037262,
      "learning_rate": 5.773201903820741e-05,
      "loss": 0.5477,
      "step": 862
    },
    {
      "epoch": 0.6513822058684782,
      "grad_norm": 0.23765362799167633,
      "learning_rate": 5.751057673043316e-05,
      "loss": 0.5859,
      "step": 863
    },
    {
      "epoch": 0.652136994056043,
      "grad_norm": 0.356414794921875,
      "learning_rate": 5.7289388387799935e-05,
      "loss": 0.7164,
      "step": 864
    },
    {
      "epoch": 0.6528917822436079,
      "grad_norm": 0.2542695105075836,
      "learning_rate": 5.706845533238097e-05,
      "loss": 0.7314,
      "step": 865
    },
    {
      "epoch": 0.6536465704311728,
      "grad_norm": 0.3375186324119568,
      "learning_rate": 5.684777888472359e-05,
      "loss": 0.6683,
      "step": 866
    },
    {
      "epoch": 0.6544013586187376,
      "grad_norm": 0.3015506863594055,
      "learning_rate": 5.662736036384142e-05,
      "loss": 0.4727,
      "step": 867
    },
    {
      "epoch": 0.6551561468063025,
      "grad_norm": 0.2300107777118683,
      "learning_rate": 5.640720108720623e-05,
      "loss": 0.6176,
      "step": 868
    },
    {
      "epoch": 0.6559109349938673,
      "grad_norm": 0.30038273334503174,
      "learning_rate": 5.618730237074048e-05,
      "loss": 0.6175,
      "step": 869
    },
    {
      "epoch": 0.6566657231814322,
      "grad_norm": 0.40476134419441223,
      "learning_rate": 5.596766552880903e-05,
      "loss": 0.5972,
      "step": 870
    },
    {
      "epoch": 0.657420511368997,
      "grad_norm": 0.35540077090263367,
      "learning_rate": 5.574829187421166e-05,
      "loss": 0.6895,
      "step": 871
    },
    {
      "epoch": 0.658175299556562,
      "grad_norm": 0.2853168547153473,
      "learning_rate": 5.5529182718174836e-05,
      "loss": 0.5132,
      "step": 872
    },
    {
      "epoch": 0.6589300877441268,
      "grad_norm": 0.3805483281612396,
      "learning_rate": 5.531033937034429e-05,
      "loss": 0.6651,
      "step": 873
    },
    {
      "epoch": 0.6596848759316917,
      "grad_norm": 0.19974863529205322,
      "learning_rate": 5.5091763138776806e-05,
      "loss": 0.5993,
      "step": 874
    },
    {
      "epoch": 0.6604396641192566,
      "grad_norm": 0.37461575865745544,
      "learning_rate": 5.4873455329932736e-05,
      "loss": 0.7451,
      "step": 875
    },
    {
      "epoch": 0.6611944523068214,
      "grad_norm": 0.3252417743206024,
      "learning_rate": 5.465541724866787e-05,
      "loss": 0.7236,
      "step": 876
    },
    {
      "epoch": 0.6619492404943863,
      "grad_norm": 0.3628082573413849,
      "learning_rate": 5.443765019822593e-05,
      "loss": 0.6268,
      "step": 877
    },
    {
      "epoch": 0.6627040286819511,
      "grad_norm": 0.20970752835273743,
      "learning_rate": 5.4220155480230625e-05,
      "loss": 0.6131,
      "step": 878
    },
    {
      "epoch": 0.663458816869516,
      "grad_norm": 0.27799662947654724,
      "learning_rate": 5.4002934394677804e-05,
      "loss": 0.7302,
      "step": 879
    },
    {
      "epoch": 0.6642136050570808,
      "grad_norm": 0.2660004794597626,
      "learning_rate": 5.3785988239927934e-05,
      "loss": 0.4926,
      "step": 880
    },
    {
      "epoch": 0.6649683932446457,
      "grad_norm": 0.4360455870628357,
      "learning_rate": 5.356931831269798e-05,
      "loss": 0.7214,
      "step": 881
    },
    {
      "epoch": 0.6657231814322105,
      "grad_norm": 0.3818310499191284,
      "learning_rate": 5.335292590805407e-05,
      "loss": 0.8076,
      "step": 882
    },
    {
      "epoch": 0.6664779696197755,
      "grad_norm": 0.3123783469200134,
      "learning_rate": 5.3136812319403374e-05,
      "loss": 0.7371,
      "step": 883
    },
    {
      "epoch": 0.6672327578073403,
      "grad_norm": 0.2487073838710785,
      "learning_rate": 5.292097883848666e-05,
      "loss": 0.6926,
      "step": 884
    },
    {
      "epoch": 0.6679875459949052,
      "grad_norm": 0.28560197353363037,
      "learning_rate": 5.270542675537034e-05,
      "loss": 0.7942,
      "step": 885
    },
    {
      "epoch": 0.6687423341824701,
      "grad_norm": 0.3530522584915161,
      "learning_rate": 5.249015735843901e-05,
      "loss": 0.7361,
      "step": 886
    },
    {
      "epoch": 0.6694971223700349,
      "grad_norm": 0.28512364625930786,
      "learning_rate": 5.227517193438746e-05,
      "loss": 0.6605,
      "step": 887
    },
    {
      "epoch": 0.6702519105575998,
      "grad_norm": 0.21226853132247925,
      "learning_rate": 5.206047176821324e-05,
      "loss": 0.6074,
      "step": 888
    },
    {
      "epoch": 0.6710066987451646,
      "grad_norm": 0.4543754458427429,
      "learning_rate": 5.184605814320889e-05,
      "loss": 0.6845,
      "step": 889
    },
    {
      "epoch": 0.6717614869327295,
      "grad_norm": 0.2025262862443924,
      "learning_rate": 5.163193234095412e-05,
      "loss": 0.6887,
      "step": 890
    },
    {
      "epoch": 0.6725162751202943,
      "grad_norm": 0.2224321961402893,
      "learning_rate": 5.141809564130847e-05,
      "loss": 0.7308,
      "step": 891
    },
    {
      "epoch": 0.6732710633078592,
      "grad_norm": 0.27533990144729614,
      "learning_rate": 5.1204549322403286e-05,
      "loss": 0.5851,
      "step": 892
    },
    {
      "epoch": 0.674025851495424,
      "grad_norm": 0.19357118010520935,
      "learning_rate": 5.0991294660634434e-05,
      "loss": 0.6463,
      "step": 893
    },
    {
      "epoch": 0.674780639682989,
      "grad_norm": 0.3172290623188019,
      "learning_rate": 5.077833293065436e-05,
      "loss": 0.6494,
      "step": 894
    },
    {
      "epoch": 0.6755354278705539,
      "grad_norm": 0.3163188695907593,
      "learning_rate": 5.056566540536476e-05,
      "loss": 0.608,
      "step": 895
    },
    {
      "epoch": 0.6762902160581187,
      "grad_norm": 0.19972337782382965,
      "learning_rate": 5.035329335590868e-05,
      "loss": 0.6624,
      "step": 896
    },
    {
      "epoch": 0.6770450042456836,
      "grad_norm": 0.23079103231430054,
      "learning_rate": 5.0141218051663205e-05,
      "loss": 0.688,
      "step": 897
    },
    {
      "epoch": 0.6777997924332484,
      "grad_norm": 0.2718868553638458,
      "learning_rate": 4.992944076023159e-05,
      "loss": 0.6199,
      "step": 898
    },
    {
      "epoch": 0.6785545806208133,
      "grad_norm": 0.27063536643981934,
      "learning_rate": 4.971796274743601e-05,
      "loss": 0.6703,
      "step": 899
    },
    {
      "epoch": 0.6793093688083781,
      "grad_norm": 0.26562461256980896,
      "learning_rate": 4.95067852773096e-05,
      "loss": 0.5825,
      "step": 900
    },
    {
      "epoch": 0.680064156995943,
      "grad_norm": 0.530328094959259,
      "learning_rate": 4.9295909612089265e-05,
      "loss": 0.7246,
      "step": 901
    },
    {
      "epoch": 0.6808189451835078,
      "grad_norm": 0.58443683385849,
      "learning_rate": 4.908533701220796e-05,
      "loss": 0.5489,
      "step": 902
    },
    {
      "epoch": 0.6815737333710727,
      "grad_norm": 0.21616952121257782,
      "learning_rate": 4.887506873628708e-05,
      "loss": 0.6992,
      "step": 903
    },
    {
      "epoch": 0.6823285215586377,
      "grad_norm": 0.38708266615867615,
      "learning_rate": 4.866510604112916e-05,
      "loss": 0.7323,
      "step": 904
    },
    {
      "epoch": 0.6830833097462025,
      "grad_norm": 0.25146982073783875,
      "learning_rate": 4.845545018171013e-05,
      "loss": 0.7484,
      "step": 905
    },
    {
      "epoch": 0.6838380979337674,
      "grad_norm": 0.2952474355697632,
      "learning_rate": 4.8246102411171976e-05,
      "loss": 0.8104,
      "step": 906
    },
    {
      "epoch": 0.6845928861213322,
      "grad_norm": 0.3132903575897217,
      "learning_rate": 4.80370639808152e-05,
      "loss": 0.5654,
      "step": 907
    },
    {
      "epoch": 0.6853476743088971,
      "grad_norm": 0.2956811487674713,
      "learning_rate": 4.782833614009136e-05,
      "loss": 0.6499,
      "step": 908
    },
    {
      "epoch": 0.6861024624964619,
      "grad_norm": 0.349758505821228,
      "learning_rate": 4.761992013659546e-05,
      "loss": 0.7256,
      "step": 909
    },
    {
      "epoch": 0.6868572506840268,
      "grad_norm": 0.24359586834907532,
      "learning_rate": 4.7411817216058764e-05,
      "loss": 0.5788,
      "step": 910
    },
    {
      "epoch": 0.6876120388715916,
      "grad_norm": 0.4185621440410614,
      "learning_rate": 4.7204028622341047e-05,
      "loss": 0.827,
      "step": 911
    },
    {
      "epoch": 0.6883668270591565,
      "grad_norm": 0.4091508984565735,
      "learning_rate": 4.699655559742345e-05,
      "loss": 0.6545,
      "step": 912
    },
    {
      "epoch": 0.6891216152467214,
      "grad_norm": 0.3457033932209015,
      "learning_rate": 4.678939938140079e-05,
      "loss": 0.7113,
      "step": 913
    },
    {
      "epoch": 0.6898764034342862,
      "grad_norm": 0.3788376748561859,
      "learning_rate": 4.6582561212474376e-05,
      "loss": 0.5612,
      "step": 914
    },
    {
      "epoch": 0.6906311916218512,
      "grad_norm": 0.24681244790554047,
      "learning_rate": 4.637604232694441e-05,
      "loss": 0.6715,
      "step": 915
    },
    {
      "epoch": 0.691385979809416,
      "grad_norm": 0.290060818195343,
      "learning_rate": 4.616984395920282e-05,
      "loss": 0.828,
      "step": 916
    },
    {
      "epoch": 0.6921407679969809,
      "grad_norm": 0.27356815338134766,
      "learning_rate": 4.596396734172559e-05,
      "loss": 0.5583,
      "step": 917
    },
    {
      "epoch": 0.6928955561845457,
      "grad_norm": 0.2680487036705017,
      "learning_rate": 4.575841370506571e-05,
      "loss": 0.7849,
      "step": 918
    },
    {
      "epoch": 0.6936503443721106,
      "grad_norm": 0.2018953412771225,
      "learning_rate": 4.55531842778456e-05,
      "loss": 0.692,
      "step": 919
    },
    {
      "epoch": 0.6944051325596754,
      "grad_norm": 0.3565406799316406,
      "learning_rate": 4.534828028674981e-05,
      "loss": 0.7029,
      "step": 920
    },
    {
      "epoch": 0.6951599207472403,
      "grad_norm": 0.3468970060348511,
      "learning_rate": 4.514370295651781e-05,
      "loss": 0.6032,
      "step": 921
    },
    {
      "epoch": 0.6959147089348051,
      "grad_norm": 0.2501802444458008,
      "learning_rate": 4.4939453509936417e-05,
      "loss": 0.5561,
      "step": 922
    },
    {
      "epoch": 0.69666949712237,
      "grad_norm": 0.40746474266052246,
      "learning_rate": 4.473553316783282e-05,
      "loss": 0.8406,
      "step": 923
    },
    {
      "epoch": 0.6974242853099349,
      "grad_norm": 0.4181479215621948,
      "learning_rate": 4.453194314906695e-05,
      "loss": 0.6762,
      "step": 924
    },
    {
      "epoch": 0.6981790734974997,
      "grad_norm": 0.321612685918808,
      "learning_rate": 4.432868467052449e-05,
      "loss": 0.6177,
      "step": 925
    },
    {
      "epoch": 0.6989338616850647,
      "grad_norm": 0.31760352849960327,
      "learning_rate": 4.412575894710931e-05,
      "loss": 0.7088,
      "step": 926
    },
    {
      "epoch": 0.6996886498726295,
      "grad_norm": 0.4605279564857483,
      "learning_rate": 4.39231671917365e-05,
      "loss": 0.6263,
      "step": 927
    },
    {
      "epoch": 0.7004434380601944,
      "grad_norm": 0.2747000753879547,
      "learning_rate": 4.372091061532487e-05,
      "loss": 0.7796,
      "step": 928
    },
    {
      "epoch": 0.7011982262477592,
      "grad_norm": 0.31585216522216797,
      "learning_rate": 4.351899042678993e-05,
      "loss": 0.6591,
      "step": 929
    },
    {
      "epoch": 0.7019530144353241,
      "grad_norm": 0.2538273334503174,
      "learning_rate": 4.3317407833036384e-05,
      "loss": 0.5384,
      "step": 930
    },
    {
      "epoch": 0.7027078026228889,
      "grad_norm": 0.29889559745788574,
      "learning_rate": 4.311616403895126e-05,
      "loss": 0.7252,
      "step": 931
    },
    {
      "epoch": 0.7034625908104538,
      "grad_norm": 0.551654577255249,
      "learning_rate": 4.291526024739647e-05,
      "loss": 0.5453,
      "step": 932
    },
    {
      "epoch": 0.7042173789980187,
      "grad_norm": 0.29829591512680054,
      "learning_rate": 4.271469765920163e-05,
      "loss": 0.7388,
      "step": 933
    },
    {
      "epoch": 0.7049721671855835,
      "grad_norm": 0.25721538066864014,
      "learning_rate": 4.2514477473157034e-05,
      "loss": 0.6442,
      "step": 934
    },
    {
      "epoch": 0.7057269553731484,
      "grad_norm": 0.2970423698425293,
      "learning_rate": 4.2314600886006264e-05,
      "loss": 0.7392,
      "step": 935
    },
    {
      "epoch": 0.7064817435607132,
      "grad_norm": 0.2867329716682434,
      "learning_rate": 4.2115069092439316e-05,
      "loss": 0.6673,
      "step": 936
    },
    {
      "epoch": 0.7072365317482782,
      "grad_norm": 0.4004952013492584,
      "learning_rate": 4.191588328508518e-05,
      "loss": 0.5917,
      "step": 937
    },
    {
      "epoch": 0.707991319935843,
      "grad_norm": 0.3005662262439728,
      "learning_rate": 4.1717044654504934e-05,
      "loss": 0.6526,
      "step": 938
    },
    {
      "epoch": 0.7087461081234079,
      "grad_norm": 0.3930591940879822,
      "learning_rate": 4.151855438918442e-05,
      "loss": 0.7878,
      "step": 939
    },
    {
      "epoch": 0.7095008963109727,
      "grad_norm": 0.24643871188163757,
      "learning_rate": 4.1320413675527404e-05,
      "loss": 0.4969,
      "step": 940
    },
    {
      "epoch": 0.7102556844985376,
      "grad_norm": 0.22425514459609985,
      "learning_rate": 4.112262369784816e-05,
      "loss": 0.6043,
      "step": 941
    },
    {
      "epoch": 0.7110104726861025,
      "grad_norm": 0.26094064116477966,
      "learning_rate": 4.092518563836475e-05,
      "loss": 0.5559,
      "step": 942
    },
    {
      "epoch": 0.7117652608736673,
      "grad_norm": 0.3560623526573181,
      "learning_rate": 4.072810067719158e-05,
      "loss": 0.663,
      "step": 943
    },
    {
      "epoch": 0.7125200490612322,
      "grad_norm": 0.24203293025493622,
      "learning_rate": 4.053136999233269e-05,
      "loss": 0.6205,
      "step": 944
    },
    {
      "epoch": 0.713274837248797,
      "grad_norm": 0.32811111211776733,
      "learning_rate": 4.0334994759674505e-05,
      "loss": 0.6293,
      "step": 945
    },
    {
      "epoch": 0.714029625436362,
      "grad_norm": 0.4660181701183319,
      "learning_rate": 4.013897615297889e-05,
      "loss": 0.7311,
      "step": 946
    },
    {
      "epoch": 0.7147844136239268,
      "grad_norm": 0.3177625834941864,
      "learning_rate": 3.994331534387602e-05,
      "loss": 0.6419,
      "step": 947
    },
    {
      "epoch": 0.7155392018114917,
      "grad_norm": 0.26848694682121277,
      "learning_rate": 3.9748013501857563e-05,
      "loss": 0.6807,
      "step": 948
    },
    {
      "epoch": 0.7162939899990565,
      "grad_norm": 0.2603176236152649,
      "learning_rate": 3.955307179426959e-05,
      "loss": 0.6066,
      "step": 949
    },
    {
      "epoch": 0.7170487781866214,
      "grad_norm": 0.36485087871551514,
      "learning_rate": 3.93584913863055e-05,
      "loss": 0.7883,
      "step": 950
    },
    {
      "epoch": 0.7178035663741863,
      "grad_norm": 0.2768622934818268,
      "learning_rate": 3.9164273440999274e-05,
      "loss": 0.683,
      "step": 951
    },
    {
      "epoch": 0.7185583545617511,
      "grad_norm": 0.21714940667152405,
      "learning_rate": 3.8970419119218296e-05,
      "loss": 0.6408,
      "step": 952
    },
    {
      "epoch": 0.719313142749316,
      "grad_norm": 0.4014114737510681,
      "learning_rate": 3.877692957965663e-05,
      "loss": 0.799,
      "step": 953
    },
    {
      "epoch": 0.7200679309368808,
      "grad_norm": 0.5028637051582336,
      "learning_rate": 3.858380597882787e-05,
      "loss": 0.6615,
      "step": 954
    },
    {
      "epoch": 0.7208227191244457,
      "grad_norm": 0.3330519497394562,
      "learning_rate": 3.8391049471058474e-05,
      "loss": 0.7173,
      "step": 955
    },
    {
      "epoch": 0.7215775073120105,
      "grad_norm": 0.19043613970279694,
      "learning_rate": 3.819866120848058e-05,
      "loss": 0.6246,
      "step": 956
    },
    {
      "epoch": 0.7223322954995754,
      "grad_norm": 0.27476853132247925,
      "learning_rate": 3.800664234102546e-05,
      "loss": 0.8586,
      "step": 957
    },
    {
      "epoch": 0.7230870836871403,
      "grad_norm": 0.25848913192749023,
      "learning_rate": 3.7814994016416264e-05,
      "loss": 0.7101,
      "step": 958
    },
    {
      "epoch": 0.7238418718747052,
      "grad_norm": 0.2391154021024704,
      "learning_rate": 3.762371738016153e-05,
      "loss": 0.7307,
      "step": 959
    },
    {
      "epoch": 0.72459666006227,
      "grad_norm": 0.31152597069740295,
      "learning_rate": 3.7432813575548e-05,
      "loss": 0.5047,
      "step": 960
    },
    {
      "epoch": 0.7253514482498349,
      "grad_norm": 0.32124173641204834,
      "learning_rate": 3.72422837436341e-05,
      "loss": 0.9057,
      "step": 961
    },
    {
      "epoch": 0.7261062364373998,
      "grad_norm": 0.3099352717399597,
      "learning_rate": 3.705212902324291e-05,
      "loss": 0.6279,
      "step": 962
    },
    {
      "epoch": 0.7268610246249646,
      "grad_norm": 0.5356445908546448,
      "learning_rate": 3.6862350550955363e-05,
      "loss": 0.7501,
      "step": 963
    },
    {
      "epoch": 0.7276158128125295,
      "grad_norm": 0.2630438208580017,
      "learning_rate": 3.6672949461103615e-05,
      "loss": 0.5742,
      "step": 964
    },
    {
      "epoch": 0.7283706010000943,
      "grad_norm": 0.18017493188381195,
      "learning_rate": 3.6483926885764006e-05,
      "loss": 0.6602,
      "step": 965
    },
    {
      "epoch": 0.7291253891876592,
      "grad_norm": 0.22176551818847656,
      "learning_rate": 3.629528395475063e-05,
      "loss": 0.6952,
      "step": 966
    },
    {
      "epoch": 0.729880177375224,
      "grad_norm": 0.33776283264160156,
      "learning_rate": 3.610702179560821e-05,
      "loss": 0.7519,
      "step": 967
    },
    {
      "epoch": 0.730634965562789,
      "grad_norm": 0.5461177229881287,
      "learning_rate": 3.59191415336057e-05,
      "loss": 0.6618,
      "step": 968
    },
    {
      "epoch": 0.7313897537503538,
      "grad_norm": 0.26690271496772766,
      "learning_rate": 3.5731644291729235e-05,
      "loss": 0.6343,
      "step": 969
    },
    {
      "epoch": 0.7321445419379187,
      "grad_norm": 0.33564168214797974,
      "learning_rate": 3.55445311906758e-05,
      "loss": 0.5569,
      "step": 970
    },
    {
      "epoch": 0.7328993301254836,
      "grad_norm": 0.3119487762451172,
      "learning_rate": 3.5357803348846085e-05,
      "loss": 0.656,
      "step": 971
    },
    {
      "epoch": 0.7336541183130484,
      "grad_norm": 0.23522299528121948,
      "learning_rate": 3.517146188233821e-05,
      "loss": 0.6325,
      "step": 972
    },
    {
      "epoch": 0.7344089065006133,
      "grad_norm": 0.3864399790763855,
      "learning_rate": 3.498550790494083e-05,
      "loss": 0.6701,
      "step": 973
    },
    {
      "epoch": 0.7351636946881781,
      "grad_norm": 0.25470176339149475,
      "learning_rate": 3.479994252812644e-05,
      "loss": 0.7304,
      "step": 974
    },
    {
      "epoch": 0.735918482875743,
      "grad_norm": 0.2372923344373703,
      "learning_rate": 3.4614766861044945e-05,
      "loss": 0.5777,
      "step": 975
    },
    {
      "epoch": 0.7366732710633078,
      "grad_norm": 0.2686150074005127,
      "learning_rate": 3.442998201051677e-05,
      "loss": 0.6492,
      "step": 976
    },
    {
      "epoch": 0.7374280592508727,
      "grad_norm": 0.22040143609046936,
      "learning_rate": 3.424558908102653e-05,
      "loss": 0.5926,
      "step": 977
    },
    {
      "epoch": 0.7381828474384375,
      "grad_norm": 0.26021116971969604,
      "learning_rate": 3.40615891747161e-05,
      "loss": 0.7419,
      "step": 978
    },
    {
      "epoch": 0.7389376356260025,
      "grad_norm": 0.25964319705963135,
      "learning_rate": 3.3877983391378367e-05,
      "loss": 0.5897,
      "step": 979
    },
    {
      "epoch": 0.7396924238135674,
      "grad_norm": 0.32742398977279663,
      "learning_rate": 3.369477282845034e-05,
      "loss": 0.6698,
      "step": 980
    },
    {
      "epoch": 0.7404472120011322,
      "grad_norm": 0.3012094497680664,
      "learning_rate": 3.3511958581006874e-05,
      "loss": 0.7241,
      "step": 981
    },
    {
      "epoch": 0.7412020001886971,
      "grad_norm": 0.24738456308841705,
      "learning_rate": 3.332954174175388e-05,
      "loss": 0.7194,
      "step": 982
    },
    {
      "epoch": 0.7419567883762619,
      "grad_norm": 0.4235929846763611,
      "learning_rate": 3.3147523401022006e-05,
      "loss": 0.6417,
      "step": 983
    },
    {
      "epoch": 0.7427115765638268,
      "grad_norm": 0.28368133306503296,
      "learning_rate": 3.2965904646759915e-05,
      "loss": 0.681,
      "step": 984
    },
    {
      "epoch": 0.7434663647513916,
      "grad_norm": 0.21085791289806366,
      "learning_rate": 3.278468656452798e-05,
      "loss": 0.5392,
      "step": 985
    },
    {
      "epoch": 0.7442211529389565,
      "grad_norm": 0.2423618584871292,
      "learning_rate": 3.260387023749165e-05,
      "loss": 0.6197,
      "step": 986
    },
    {
      "epoch": 0.7449759411265213,
      "grad_norm": 0.4339735507965088,
      "learning_rate": 3.242345674641508e-05,
      "loss": 0.7109,
      "step": 987
    },
    {
      "epoch": 0.7457307293140862,
      "grad_norm": 0.42240822315216064,
      "learning_rate": 3.224344716965452e-05,
      "loss": 0.6829,
      "step": 988
    },
    {
      "epoch": 0.7464855175016512,
      "grad_norm": 0.26920971274375916,
      "learning_rate": 3.2063842583152095e-05,
      "loss": 0.69,
      "step": 989
    },
    {
      "epoch": 0.747240305689216,
      "grad_norm": 0.3411796987056732,
      "learning_rate": 3.188464406042909e-05,
      "loss": 0.7636,
      "step": 990
    },
    {
      "epoch": 0.7479950938767809,
      "grad_norm": 0.43650564551353455,
      "learning_rate": 3.170585267257985e-05,
      "loss": 0.7007,
      "step": 991
    },
    {
      "epoch": 0.7487498820643457,
      "grad_norm": 0.26404619216918945,
      "learning_rate": 3.152746948826517e-05,
      "loss": 0.7552,
      "step": 992
    },
    {
      "epoch": 0.7495046702519106,
      "grad_norm": 0.30114683508872986,
      "learning_rate": 3.1349495573705866e-05,
      "loss": 0.7027,
      "step": 993
    },
    {
      "epoch": 0.7502594584394754,
      "grad_norm": 0.28327110409736633,
      "learning_rate": 3.117193199267663e-05,
      "loss": 0.6368,
      "step": 994
    },
    {
      "epoch": 0.7510142466270403,
      "grad_norm": 0.2849569022655487,
      "learning_rate": 3.0994779806499405e-05,
      "loss": 0.7802,
      "step": 995
    },
    {
      "epoch": 0.7517690348146051,
      "grad_norm": 0.21896407008171082,
      "learning_rate": 3.0818040074037305e-05,
      "loss": 0.6372,
      "step": 996
    },
    {
      "epoch": 0.75252382300217,
      "grad_norm": 0.3581289052963257,
      "learning_rate": 3.0641713851687994e-05,
      "loss": 0.5978,
      "step": 997
    },
    {
      "epoch": 0.7532786111897348,
      "grad_norm": 0.22261808812618256,
      "learning_rate": 3.046580219337769e-05,
      "loss": 0.5307,
      "step": 998
    },
    {
      "epoch": 0.7540333993772997,
      "grad_norm": 0.3566007614135742,
      "learning_rate": 3.029030615055457e-05,
      "loss": 0.6752,
      "step": 999
    },
    {
      "epoch": 0.7547881875648647,
      "grad_norm": 0.30415353178977966,
      "learning_rate": 3.0115226772182737e-05,
      "loss": 0.8319,
      "step": 1000
    },
    {
      "epoch": 0.7547881875648647,
      "eval_loss": 0.6675713062286377,
      "eval_runtime": 3179.3361,
      "eval_samples_per_second": 2.964,
      "eval_steps_per_second": 0.741,
      "step": 1000
    },
    {
      "epoch": 0.7555429757524295,
      "grad_norm": 0.40372657775878906,
      "learning_rate": 2.994056510473571e-05,
      "loss": 0.7349,
      "step": 1001
    },
    {
      "epoch": 0.7562977639399944,
      "grad_norm": 0.2283443659543991,
      "learning_rate": 2.9766322192190377e-05,
      "loss": 0.5768,
      "step": 1002
    },
    {
      "epoch": 0.7570525521275592,
      "grad_norm": 0.26109611988067627,
      "learning_rate": 2.9592499076020708e-05,
      "loss": 0.6506,
      "step": 1003
    },
    {
      "epoch": 0.7578073403151241,
      "grad_norm": 0.26998627185821533,
      "learning_rate": 2.9419096795191346e-05,
      "loss": 0.7382,
      "step": 1004
    },
    {
      "epoch": 0.7585621285026889,
      "grad_norm": 0.36248722672462463,
      "learning_rate": 2.92461163861517e-05,
      "loss": 0.7001,
      "step": 1005
    },
    {
      "epoch": 0.7593169166902538,
      "grad_norm": 0.26811161637306213,
      "learning_rate": 2.907355888282948e-05,
      "loss": 0.6898,
      "step": 1006
    },
    {
      "epoch": 0.7600717048778186,
      "grad_norm": 0.32453712821006775,
      "learning_rate": 2.890142531662471e-05,
      "loss": 0.7477,
      "step": 1007
    },
    {
      "epoch": 0.7608264930653835,
      "grad_norm": 0.4307006895542145,
      "learning_rate": 2.8729716716403398e-05,
      "loss": 0.4583,
      "step": 1008
    },
    {
      "epoch": 0.7615812812529484,
      "grad_norm": 0.3569681644439697,
      "learning_rate": 2.8558434108491582e-05,
      "loss": 0.6786,
      "step": 1009
    },
    {
      "epoch": 0.7623360694405132,
      "grad_norm": 0.3400719165802002,
      "learning_rate": 2.8387578516668968e-05,
      "loss": 0.6069,
      "step": 1010
    },
    {
      "epoch": 0.7630908576280782,
      "grad_norm": 0.3146662414073944,
      "learning_rate": 2.8217150962163042e-05,
      "loss": 0.6549,
      "step": 1011
    },
    {
      "epoch": 0.763845645815643,
      "grad_norm": 0.3857819736003876,
      "learning_rate": 2.804715246364272e-05,
      "loss": 0.6558,
      "step": 1012
    },
    {
      "epoch": 0.7646004340032079,
      "grad_norm": 0.2595381438732147,
      "learning_rate": 2.7877584037212558e-05,
      "loss": 0.6824,
      "step": 1013
    },
    {
      "epoch": 0.7653552221907727,
      "grad_norm": 0.3099771738052368,
      "learning_rate": 2.7708446696406342e-05,
      "loss": 0.7446,
      "step": 1014
    },
    {
      "epoch": 0.7661100103783376,
      "grad_norm": 0.24470241367816925,
      "learning_rate": 2.75397414521813e-05,
      "loss": 0.755,
      "step": 1015
    },
    {
      "epoch": 0.7668647985659024,
      "grad_norm": 0.213555246591568,
      "learning_rate": 2.7371469312911978e-05,
      "loss": 0.6201,
      "step": 1016
    },
    {
      "epoch": 0.7676195867534673,
      "grad_norm": 0.2478504776954651,
      "learning_rate": 2.7203631284384078e-05,
      "loss": 0.6431,
      "step": 1017
    },
    {
      "epoch": 0.7683743749410322,
      "grad_norm": 0.23081393539905548,
      "learning_rate": 2.703622836978872e-05,
      "loss": 0.7128,
      "step": 1018
    },
    {
      "epoch": 0.769129163128597,
      "grad_norm": 0.3518885374069214,
      "learning_rate": 2.686926156971613e-05,
      "loss": 0.6897,
      "step": 1019
    },
    {
      "epoch": 0.7698839513161619,
      "grad_norm": 0.3111558258533478,
      "learning_rate": 2.6702731882149957e-05,
      "loss": 0.6817,
      "step": 1020
    },
    {
      "epoch": 0.7706387395037267,
      "grad_norm": 0.28710827231407166,
      "learning_rate": 2.6536640302461034e-05,
      "loss": 0.538,
      "step": 1021
    },
    {
      "epoch": 0.7713935276912917,
      "grad_norm": 0.20897357165813446,
      "learning_rate": 2.6370987823401704e-05,
      "loss": 0.5687,
      "step": 1022
    },
    {
      "epoch": 0.7721483158788565,
      "grad_norm": 0.4245098829269409,
      "learning_rate": 2.6205775435099623e-05,
      "loss": 0.5981,
      "step": 1023
    },
    {
      "epoch": 0.7729031040664214,
      "grad_norm": 0.2362263947725296,
      "learning_rate": 2.604100412505204e-05,
      "loss": 0.6071,
      "step": 1024
    },
    {
      "epoch": 0.7736578922539862,
      "grad_norm": 0.26932957768440247,
      "learning_rate": 2.5876674878119732e-05,
      "loss": 0.7109,
      "step": 1025
    },
    {
      "epoch": 0.7744126804415511,
      "grad_norm": 0.3404290974140167,
      "learning_rate": 2.5712788676521382e-05,
      "loss": 0.6097,
      "step": 1026
    },
    {
      "epoch": 0.775167468629116,
      "grad_norm": 0.3470824062824249,
      "learning_rate": 2.554934649982731e-05,
      "loss": 0.7112,
      "step": 1027
    },
    {
      "epoch": 0.7759222568166808,
      "grad_norm": 0.37356066703796387,
      "learning_rate": 2.5386349324954007e-05,
      "loss": 0.7164,
      "step": 1028
    },
    {
      "epoch": 0.7766770450042457,
      "grad_norm": 0.2631151080131531,
      "learning_rate": 2.5223798126158004e-05,
      "loss": 0.6225,
      "step": 1029
    },
    {
      "epoch": 0.7774318331918105,
      "grad_norm": 0.45144668221473694,
      "learning_rate": 2.5061693875030278e-05,
      "loss": 0.8658,
      "step": 1030
    },
    {
      "epoch": 0.7781866213793754,
      "grad_norm": 0.29441845417022705,
      "learning_rate": 2.490003754049024e-05,
      "loss": 0.6268,
      "step": 1031
    },
    {
      "epoch": 0.7789414095669402,
      "grad_norm": 0.2112281173467636,
      "learning_rate": 2.4738830088780108e-05,
      "loss": 0.6888,
      "step": 1032
    },
    {
      "epoch": 0.7796961977545052,
      "grad_norm": 0.3030405640602112,
      "learning_rate": 2.457807248345908e-05,
      "loss": 0.741,
      "step": 1033
    },
    {
      "epoch": 0.78045098594207,
      "grad_norm": 0.19658897817134857,
      "learning_rate": 2.4417765685397475e-05,
      "loss": 0.6938,
      "step": 1034
    },
    {
      "epoch": 0.7812057741296349,
      "grad_norm": 0.28650525212287903,
      "learning_rate": 2.4257910652771187e-05,
      "loss": 0.7309,
      "step": 1035
    },
    {
      "epoch": 0.7819605623171997,
      "grad_norm": 0.4236191213130951,
      "learning_rate": 2.4098508341055714e-05,
      "loss": 0.6786,
      "step": 1036
    },
    {
      "epoch": 0.7827153505047646,
      "grad_norm": 0.6639095544815063,
      "learning_rate": 2.393955970302072e-05,
      "loss": 0.5306,
      "step": 1037
    },
    {
      "epoch": 0.7834701386923295,
      "grad_norm": 0.2817174792289734,
      "learning_rate": 2.3781065688724068e-05,
      "loss": 0.7003,
      "step": 1038
    },
    {
      "epoch": 0.7842249268798943,
      "grad_norm": 0.3736404776573181,
      "learning_rate": 2.3623027245506392e-05,
      "loss": 0.5947,
      "step": 1039
    },
    {
      "epoch": 0.7849797150674592,
      "grad_norm": 0.3049956262111664,
      "learning_rate": 2.346544531798519e-05,
      "loss": 0.9115,
      "step": 1040
    },
    {
      "epoch": 0.785734503255024,
      "grad_norm": 0.2101871371269226,
      "learning_rate": 2.3308320848049437e-05,
      "loss": 0.6543,
      "step": 1041
    },
    {
      "epoch": 0.786489291442589,
      "grad_norm": 0.4293341636657715,
      "learning_rate": 2.3151654774853693e-05,
      "loss": 0.7291,
      "step": 1042
    },
    {
      "epoch": 0.7872440796301537,
      "grad_norm": 0.39906564354896545,
      "learning_rate": 2.299544803481274e-05,
      "loss": 0.588,
      "step": 1043
    },
    {
      "epoch": 0.7879988678177187,
      "grad_norm": 0.3851369321346283,
      "learning_rate": 2.283970156159573e-05,
      "loss": 0.6584,
      "step": 1044
    },
    {
      "epoch": 0.7887536560052835,
      "grad_norm": 0.24893325567245483,
      "learning_rate": 2.2684416286120848e-05,
      "loss": 0.6541,
      "step": 1045
    },
    {
      "epoch": 0.7895084441928484,
      "grad_norm": 0.21785226464271545,
      "learning_rate": 2.2529593136549622e-05,
      "loss": 0.5748,
      "step": 1046
    },
    {
      "epoch": 0.7902632323804133,
      "grad_norm": 0.2737730145454407,
      "learning_rate": 2.2375233038281297e-05,
      "loss": 0.7337,
      "step": 1047
    },
    {
      "epoch": 0.7910180205679781,
      "grad_norm": 0.5488680601119995,
      "learning_rate": 2.2221336913947522e-05,
      "loss": 0.564,
      "step": 1048
    },
    {
      "epoch": 0.791772808755543,
      "grad_norm": 0.40173956751823425,
      "learning_rate": 2.20679056834066e-05,
      "loss": 0.6543,
      "step": 1049
    },
    {
      "epoch": 0.7925275969431078,
      "grad_norm": 0.25139087438583374,
      "learning_rate": 2.1914940263738205e-05,
      "loss": 0.6976,
      "step": 1050
    },
    {
      "epoch": 0.7932823851306727,
      "grad_norm": 0.38013461232185364,
      "learning_rate": 2.176244156923768e-05,
      "loss": 0.6232,
      "step": 1051
    },
    {
      "epoch": 0.7940371733182375,
      "grad_norm": 0.30530261993408203,
      "learning_rate": 2.1610410511410805e-05,
      "loss": 0.7275,
      "step": 1052
    },
    {
      "epoch": 0.7947919615058024,
      "grad_norm": 0.4873257279396057,
      "learning_rate": 2.1458847998968124e-05,
      "loss": 0.7648,
      "step": 1053
    },
    {
      "epoch": 0.7955467496933672,
      "grad_norm": 0.20648786425590515,
      "learning_rate": 2.1307754937819714e-05,
      "loss": 0.5643,
      "step": 1054
    },
    {
      "epoch": 0.7963015378809322,
      "grad_norm": 0.307393878698349,
      "learning_rate": 2.1157132231069587e-05,
      "loss": 0.6805,
      "step": 1055
    },
    {
      "epoch": 0.7970563260684971,
      "grad_norm": 0.23329633474349976,
      "learning_rate": 2.100698077901049e-05,
      "loss": 0.707,
      "step": 1056
    },
    {
      "epoch": 0.7978111142560619,
      "grad_norm": 0.2826187014579773,
      "learning_rate": 2.0857301479118275e-05,
      "loss": 0.7879,
      "step": 1057
    },
    {
      "epoch": 0.7985659024436268,
      "grad_norm": 0.2766802906990051,
      "learning_rate": 2.0708095226046807e-05,
      "loss": 0.6654,
      "step": 1058
    },
    {
      "epoch": 0.7993206906311916,
      "grad_norm": 0.2757978141307831,
      "learning_rate": 2.0559362911622436e-05,
      "loss": 0.6338,
      "step": 1059
    },
    {
      "epoch": 0.8000754788187565,
      "grad_norm": 0.279615581035614,
      "learning_rate": 2.0411105424838684e-05,
      "loss": 0.6501,
      "step": 1060
    },
    {
      "epoch": 0.8008302670063213,
      "grad_norm": 0.30165156722068787,
      "learning_rate": 2.026332365185102e-05,
      "loss": 0.6958,
      "step": 1061
    },
    {
      "epoch": 0.8015850551938862,
      "grad_norm": 0.37823107838630676,
      "learning_rate": 2.0116018475971443e-05,
      "loss": 0.9172,
      "step": 1062
    },
    {
      "epoch": 0.802339843381451,
      "grad_norm": 0.2728630602359772,
      "learning_rate": 1.996919077766334e-05,
      "loss": 0.7024,
      "step": 1063
    },
    {
      "epoch": 0.803094631569016,
      "grad_norm": 0.2793569564819336,
      "learning_rate": 1.9822841434536045e-05,
      "loss": 0.7276,
      "step": 1064
    },
    {
      "epoch": 0.8038494197565809,
      "grad_norm": 0.3501023054122925,
      "learning_rate": 1.967697132133981e-05,
      "loss": 0.6909,
      "step": 1065
    },
    {
      "epoch": 0.8046042079441457,
      "grad_norm": 0.23134638369083405,
      "learning_rate": 1.953158130996039e-05,
      "loss": 0.6796,
      "step": 1066
    },
    {
      "epoch": 0.8053589961317106,
      "grad_norm": 0.3390396535396576,
      "learning_rate": 1.9386672269413976e-05,
      "loss": 0.6362,
      "step": 1067
    },
    {
      "epoch": 0.8061137843192754,
      "grad_norm": 0.3614787757396698,
      "learning_rate": 1.9242245065841812e-05,
      "loss": 0.7014,
      "step": 1068
    },
    {
      "epoch": 0.8068685725068403,
      "grad_norm": 0.2866775095462799,
      "learning_rate": 1.9098300562505266e-05,
      "loss": 0.6809,
      "step": 1069
    },
    {
      "epoch": 0.8076233606944051,
      "grad_norm": 0.3078922927379608,
      "learning_rate": 1.8954839619780428e-05,
      "loss": 0.6434,
      "step": 1070
    },
    {
      "epoch": 0.80837814888197,
      "grad_norm": 0.23635929822921753,
      "learning_rate": 1.881186309515318e-05,
      "loss": 0.5863,
      "step": 1071
    },
    {
      "epoch": 0.8091329370695348,
      "grad_norm": 0.28842487931251526,
      "learning_rate": 1.866937184321389e-05,
      "loss": 0.7318,
      "step": 1072
    },
    {
      "epoch": 0.8098877252570997,
      "grad_norm": 0.4074484407901764,
      "learning_rate": 1.852736671565244e-05,
      "loss": 0.7541,
      "step": 1073
    },
    {
      "epoch": 0.8106425134446645,
      "grad_norm": 0.2808852195739746,
      "learning_rate": 1.8385848561253027e-05,
      "loss": 0.5878,
      "step": 1074
    },
    {
      "epoch": 0.8113973016322295,
      "grad_norm": 0.22965286672115326,
      "learning_rate": 1.8244818225889184e-05,
      "loss": 0.5502,
      "step": 1075
    },
    {
      "epoch": 0.8121520898197944,
      "grad_norm": 0.4521905183792114,
      "learning_rate": 1.8104276552518697e-05,
      "loss": 0.6269,
      "step": 1076
    },
    {
      "epoch": 0.8129068780073592,
      "grad_norm": 0.24387669563293457,
      "learning_rate": 1.7964224381178473e-05,
      "loss": 0.7058,
      "step": 1077
    },
    {
      "epoch": 0.8136616661949241,
      "grad_norm": 0.2727693021297455,
      "learning_rate": 1.782466254897971e-05,
      "loss": 0.5134,
      "step": 1078
    },
    {
      "epoch": 0.8144164543824889,
      "grad_norm": 0.35729336738586426,
      "learning_rate": 1.768559189010267e-05,
      "loss": 0.6457,
      "step": 1079
    },
    {
      "epoch": 0.8151712425700538,
      "grad_norm": 0.2639511227607727,
      "learning_rate": 1.7547013235791887e-05,
      "loss": 0.6514,
      "step": 1080
    },
    {
      "epoch": 0.8159260307576186,
      "grad_norm": 0.23334059119224548,
      "learning_rate": 1.740892741435105e-05,
      "loss": 0.5537,
      "step": 1081
    },
    {
      "epoch": 0.8166808189451835,
      "grad_norm": 0.2253081500530243,
      "learning_rate": 1.72713352511382e-05,
      "loss": 0.6471,
      "step": 1082
    },
    {
      "epoch": 0.8174356071327483,
      "grad_norm": 0.2675133943557739,
      "learning_rate": 1.7134237568560617e-05,
      "loss": 0.6146,
      "step": 1083
    },
    {
      "epoch": 0.8181903953203132,
      "grad_norm": 0.2810386121273041,
      "learning_rate": 1.6997635186070103e-05,
      "loss": 0.7039,
      "step": 1084
    },
    {
      "epoch": 0.8189451835078781,
      "grad_norm": 0.2173774391412735,
      "learning_rate": 1.6861528920157875e-05,
      "loss": 0.6703,
      "step": 1085
    },
    {
      "epoch": 0.819699971695443,
      "grad_norm": 0.2095620036125183,
      "learning_rate": 1.672591958434988e-05,
      "loss": 0.7662,
      "step": 1086
    },
    {
      "epoch": 0.8204547598830079,
      "grad_norm": 0.24614666402339935,
      "learning_rate": 1.659080798920184e-05,
      "loss": 0.6912,
      "step": 1087
    },
    {
      "epoch": 0.8212095480705727,
      "grad_norm": 0.3447144627571106,
      "learning_rate": 1.6456194942294335e-05,
      "loss": 0.6627,
      "step": 1088
    },
    {
      "epoch": 0.8219643362581376,
      "grad_norm": 0.43708714842796326,
      "learning_rate": 1.632208124822815e-05,
      "loss": 0.5929,
      "step": 1089
    },
    {
      "epoch": 0.8227191244457024,
      "grad_norm": 0.43550431728363037,
      "learning_rate": 1.6188467708619292e-05,
      "loss": 0.6846,
      "step": 1090
    },
    {
      "epoch": 0.8234739126332673,
      "grad_norm": 0.31294873356819153,
      "learning_rate": 1.605535512209435e-05,
      "loss": 0.668,
      "step": 1091
    },
    {
      "epoch": 0.8242287008208321,
      "grad_norm": 0.3090845048427582,
      "learning_rate": 1.592274428428555e-05,
      "loss": 0.5758,
      "step": 1092
    },
    {
      "epoch": 0.824983489008397,
      "grad_norm": 0.29303210973739624,
      "learning_rate": 1.5790635987826218e-05,
      "loss": 0.664,
      "step": 1093
    },
    {
      "epoch": 0.8257382771959619,
      "grad_norm": 0.4138537645339966,
      "learning_rate": 1.5659031022345795e-05,
      "loss": 0.6736,
      "step": 1094
    },
    {
      "epoch": 0.8264930653835267,
      "grad_norm": 0.19113922119140625,
      "learning_rate": 1.5527930174465354e-05,
      "loss": 0.6729,
      "step": 1095
    },
    {
      "epoch": 0.8272478535710917,
      "grad_norm": 0.23314012587070465,
      "learning_rate": 1.539733422779269e-05,
      "loss": 0.6514,
      "step": 1096
    },
    {
      "epoch": 0.8280026417586565,
      "grad_norm": 0.24648088216781616,
      "learning_rate": 1.526724396291783e-05,
      "loss": 0.6712,
      "step": 1097
    },
    {
      "epoch": 0.8287574299462214,
      "grad_norm": 0.20178617537021637,
      "learning_rate": 1.5137660157408184e-05,
      "loss": 0.535,
      "step": 1098
    },
    {
      "epoch": 0.8295122181337862,
      "grad_norm": 0.44638144969940186,
      "learning_rate": 1.5008583585804048e-05,
      "loss": 0.5299,
      "step": 1099
    },
    {
      "epoch": 0.8302670063213511,
      "grad_norm": 0.3377129137516022,
      "learning_rate": 1.488001501961389e-05,
      "loss": 0.7399,
      "step": 1100
    },
    {
      "epoch": 0.8310217945089159,
      "grad_norm": 0.3321734070777893,
      "learning_rate": 1.4751955227309722e-05,
      "loss": 0.6962,
      "step": 1101
    },
    {
      "epoch": 0.8317765826964808,
      "grad_norm": 0.22979624569416046,
      "learning_rate": 1.4624404974322637e-05,
      "loss": 0.6418,
      "step": 1102
    },
    {
      "epoch": 0.8325313708840457,
      "grad_norm": 0.34498700499534607,
      "learning_rate": 1.4497365023038012e-05,
      "loss": 0.5593,
      "step": 1103
    },
    {
      "epoch": 0.8332861590716105,
      "grad_norm": 0.2301950305700302,
      "learning_rate": 1.4370836132791222e-05,
      "loss": 0.6915,
      "step": 1104
    },
    {
      "epoch": 0.8340409472591754,
      "grad_norm": 0.29568207263946533,
      "learning_rate": 1.4244819059862824e-05,
      "loss": 0.6501,
      "step": 1105
    },
    {
      "epoch": 0.8347957354467402,
      "grad_norm": 0.3028510808944702,
      "learning_rate": 1.4119314557474272e-05,
      "loss": 0.6688,
      "step": 1106
    },
    {
      "epoch": 0.8355505236343052,
      "grad_norm": 0.2285766750574112,
      "learning_rate": 1.3994323375783269e-05,
      "loss": 0.5793,
      "step": 1107
    },
    {
      "epoch": 0.83630531182187,
      "grad_norm": 0.33780550956726074,
      "learning_rate": 1.3869846261879382e-05,
      "loss": 0.6802,
      "step": 1108
    },
    {
      "epoch": 0.8370601000094349,
      "grad_norm": 0.5522000789642334,
      "learning_rate": 1.3745883959779416e-05,
      "loss": 0.7042,
      "step": 1109
    },
    {
      "epoch": 0.8378148881969997,
      "grad_norm": 0.16996216773986816,
      "learning_rate": 1.3622437210423211e-05,
      "loss": 0.6772,
      "step": 1110
    },
    {
      "epoch": 0.8385696763845646,
      "grad_norm": 0.3642832636833191,
      "learning_rate": 1.3499506751668933e-05,
      "loss": 0.6566,
      "step": 1111
    },
    {
      "epoch": 0.8393244645721294,
      "grad_norm": 0.29272326827049255,
      "learning_rate": 1.3377093318288958e-05,
      "loss": 0.5933,
      "step": 1112
    },
    {
      "epoch": 0.8400792527596943,
      "grad_norm": 0.25821977853775024,
      "learning_rate": 1.325519764196519e-05,
      "loss": 0.7437,
      "step": 1113
    },
    {
      "epoch": 0.8408340409472592,
      "grad_norm": 0.2011156678199768,
      "learning_rate": 1.313382045128495e-05,
      "loss": 0.7353,
      "step": 1114
    },
    {
      "epoch": 0.841588829134824,
      "grad_norm": 0.2339785248041153,
      "learning_rate": 1.3012962471736379e-05,
      "loss": 0.7628,
      "step": 1115
    },
    {
      "epoch": 0.8423436173223889,
      "grad_norm": 0.3431377410888672,
      "learning_rate": 1.2892624425704326e-05,
      "loss": 0.7414,
      "step": 1116
    },
    {
      "epoch": 0.8430984055099537,
      "grad_norm": 0.36225780844688416,
      "learning_rate": 1.2772807032465894e-05,
      "loss": 0.6732,
      "step": 1117
    },
    {
      "epoch": 0.8438531936975187,
      "grad_norm": 0.32909274101257324,
      "learning_rate": 1.2653511008186125e-05,
      "loss": 0.6936,
      "step": 1118
    },
    {
      "epoch": 0.8446079818850835,
      "grad_norm": 0.3216010332107544,
      "learning_rate": 1.2534737065913837e-05,
      "loss": 0.6724,
      "step": 1119
    },
    {
      "epoch": 0.8453627700726484,
      "grad_norm": 0.287259578704834,
      "learning_rate": 1.2416485915577224e-05,
      "loss": 0.7003,
      "step": 1120
    },
    {
      "epoch": 0.8461175582602132,
      "grad_norm": 0.232753723859787,
      "learning_rate": 1.229875826397976e-05,
      "loss": 0.6364,
      "step": 1121
    },
    {
      "epoch": 0.8468723464477781,
      "grad_norm": 0.19635428488254547,
      "learning_rate": 1.2181554814795803e-05,
      "loss": 0.6093,
      "step": 1122
    },
    {
      "epoch": 0.847627134635343,
      "grad_norm": 0.40340039134025574,
      "learning_rate": 1.2064876268566571e-05,
      "loss": 0.7425,
      "step": 1123
    },
    {
      "epoch": 0.8483819228229078,
      "grad_norm": 0.2454165518283844,
      "learning_rate": 1.1948723322695776e-05,
      "loss": 0.6043,
      "step": 1124
    },
    {
      "epoch": 0.8491367110104727,
      "grad_norm": 0.28299739956855774,
      "learning_rate": 1.1833096671445642e-05,
      "loss": 0.644,
      "step": 1125
    },
    {
      "epoch": 0.8498914991980375,
      "grad_norm": 0.3048984706401825,
      "learning_rate": 1.1717997005932546e-05,
      "loss": 0.7435,
      "step": 1126
    },
    {
      "epoch": 0.8506462873856024,
      "grad_norm": 0.30882203578948975,
      "learning_rate": 1.1603425014123126e-05,
      "loss": 0.8651,
      "step": 1127
    },
    {
      "epoch": 0.8514010755731672,
      "grad_norm": 0.32371675968170166,
      "learning_rate": 1.14893813808299e-05,
      "loss": 0.747,
      "step": 1128
    },
    {
      "epoch": 0.8521558637607322,
      "grad_norm": 0.30939337611198425,
      "learning_rate": 1.1375866787707435e-05,
      "loss": 0.8642,
      "step": 1129
    },
    {
      "epoch": 0.852910651948297,
      "grad_norm": 0.1939748376607895,
      "learning_rate": 1.1262881913248113e-05,
      "loss": 0.5561,
      "step": 1130
    },
    {
      "epoch": 0.8536654401358619,
      "grad_norm": 0.2665603458881378,
      "learning_rate": 1.1150427432778077e-05,
      "loss": 0.6179,
      "step": 1131
    },
    {
      "epoch": 0.8544202283234268,
      "grad_norm": 0.27036339044570923,
      "learning_rate": 1.1038504018453277e-05,
      "loss": 0.6466,
      "step": 1132
    },
    {
      "epoch": 0.8551750165109916,
      "grad_norm": 0.725617527961731,
      "learning_rate": 1.0927112339255374e-05,
      "loss": 0.6395,
      "step": 1133
    },
    {
      "epoch": 0.8559298046985565,
      "grad_norm": 0.22550490498542786,
      "learning_rate": 1.0816253060987836e-05,
      "loss": 0.599,
      "step": 1134
    },
    {
      "epoch": 0.8566845928861213,
      "grad_norm": 0.24095633625984192,
      "learning_rate": 1.0705926846271786e-05,
      "loss": 0.6943,
      "step": 1135
    },
    {
      "epoch": 0.8574393810736862,
      "grad_norm": 0.3741377890110016,
      "learning_rate": 1.0596134354542286e-05,
      "loss": 0.6153,
      "step": 1136
    },
    {
      "epoch": 0.858194169261251,
      "grad_norm": 0.29209962487220764,
      "learning_rate": 1.0486876242044153e-05,
      "loss": 0.7012,
      "step": 1137
    },
    {
      "epoch": 0.8589489574488159,
      "grad_norm": 0.46752333641052246,
      "learning_rate": 1.037815316182822e-05,
      "loss": 0.7219,
      "step": 1138
    },
    {
      "epoch": 0.8597037456363807,
      "grad_norm": 0.22345347702503204,
      "learning_rate": 1.0269965763747292e-05,
      "loss": 0.6348,
      "step": 1139
    },
    {
      "epoch": 0.8604585338239457,
      "grad_norm": 0.2682698667049408,
      "learning_rate": 1.0162314694452402e-05,
      "loss": 0.6425,
      "step": 1140
    },
    {
      "epoch": 0.8612133220115106,
      "grad_norm": 0.3834696412086487,
      "learning_rate": 1.0055200597388792e-05,
      "loss": 0.5978,
      "step": 1141
    },
    {
      "epoch": 0.8619681101990754,
      "grad_norm": 0.29305750131607056,
      "learning_rate": 9.948624112792226e-06,
      "loss": 0.6724,
      "step": 1142
    },
    {
      "epoch": 0.8627228983866403,
      "grad_norm": 0.35857293009757996,
      "learning_rate": 9.84258587768504e-06,
      "loss": 0.7294,
      "step": 1143
    },
    {
      "epoch": 0.8634776865742051,
      "grad_norm": 0.29631003737449646,
      "learning_rate": 9.737086525872352e-06,
      "loss": 0.709,
      "step": 1144
    },
    {
      "epoch": 0.86423247476177,
      "grad_norm": 0.36566638946533203,
      "learning_rate": 9.632126687938393e-06,
      "loss": 0.6941,
      "step": 1145
    },
    {
      "epoch": 0.8649872629493348,
      "grad_norm": 0.2900606095790863,
      "learning_rate": 9.527706991242502e-06,
      "loss": 0.6708,
      "step": 1146
    },
    {
      "epoch": 0.8657420511368997,
      "grad_norm": 0.3712988793849945,
      "learning_rate": 9.423828059915685e-06,
      "loss": 0.6341,
      "step": 1147
    },
    {
      "epoch": 0.8664968393244645,
      "grad_norm": 0.3868010938167572,
      "learning_rate": 9.320490514856594e-06,
      "loss": 0.7355,
      "step": 1148
    },
    {
      "epoch": 0.8672516275120294,
      "grad_norm": 0.31675928831100464,
      "learning_rate": 9.217694973728009e-06,
      "loss": 0.6879,
      "step": 1149
    },
    {
      "epoch": 0.8680064156995942,
      "grad_norm": 0.28847429156303406,
      "learning_rate": 9.115442050953015e-06,
      "loss": 0.6087,
      "step": 1150
    },
    {
      "epoch": 0.8687612038871592,
      "grad_norm": 0.19612976908683777,
      "learning_rate": 9.013732357711469e-06,
      "loss": 0.7554,
      "step": 1151
    },
    {
      "epoch": 0.8695159920747241,
      "grad_norm": 0.3313797414302826,
      "learning_rate": 8.912566501936192e-06,
      "loss": 0.6035,
      "step": 1152
    },
    {
      "epoch": 0.8702707802622889,
      "grad_norm": 0.2832201421260834,
      "learning_rate": 8.811945088309493e-06,
      "loss": 0.6769,
      "step": 1153
    },
    {
      "epoch": 0.8710255684498538,
      "grad_norm": 0.3306996822357178,
      "learning_rate": 8.711868718259397e-06,
      "loss": 0.7358,
      "step": 1154
    },
    {
      "epoch": 0.8717803566374186,
      "grad_norm": 0.2510899007320404,
      "learning_rate": 8.612337989956198e-06,
      "loss": 0.6922,
      "step": 1155
    },
    {
      "epoch": 0.8725351448249835,
      "grad_norm": 0.27462103962898254,
      "learning_rate": 8.513353498308741e-06,
      "loss": 0.694,
      "step": 1156
    },
    {
      "epoch": 0.8732899330125483,
      "grad_norm": 0.4708176851272583,
      "learning_rate": 8.414915834961034e-06,
      "loss": 0.6851,
      "step": 1157
    },
    {
      "epoch": 0.8740447212001132,
      "grad_norm": 0.3389897644519806,
      "learning_rate": 8.317025588288507e-06,
      "loss": 0.5798,
      "step": 1158
    },
    {
      "epoch": 0.874799509387678,
      "grad_norm": 0.2645065188407898,
      "learning_rate": 8.21968334339469e-06,
      "loss": 0.6927,
      "step": 1159
    },
    {
      "epoch": 0.875554297575243,
      "grad_norm": 0.30674704909324646,
      "learning_rate": 8.122889682107616e-06,
      "loss": 0.6561,
      "step": 1160
    },
    {
      "epoch": 0.8763090857628079,
      "grad_norm": 0.18437445163726807,
      "learning_rate": 8.0266451829763e-06,
      "loss": 0.6422,
      "step": 1161
    },
    {
      "epoch": 0.8770638739503727,
      "grad_norm": 0.22055676579475403,
      "learning_rate": 7.930950421267436e-06,
      "loss": 0.646,
      "step": 1162
    },
    {
      "epoch": 0.8778186621379376,
      "grad_norm": 0.3075316250324249,
      "learning_rate": 7.835805968961762e-06,
      "loss": 0.6921,
      "step": 1163
    },
    {
      "epoch": 0.8785734503255024,
      "grad_norm": 0.4014686644077301,
      "learning_rate": 7.741212394750829e-06,
      "loss": 0.6402,
      "step": 1164
    },
    {
      "epoch": 0.8793282385130673,
      "grad_norm": 0.34816738963127136,
      "learning_rate": 7.64717026403342e-06,
      "loss": 0.7505,
      "step": 1165
    },
    {
      "epoch": 0.8800830267006321,
      "grad_norm": 0.27654585242271423,
      "learning_rate": 7.553680138912378e-06,
      "loss": 0.5346,
      "step": 1166
    },
    {
      "epoch": 0.880837814888197,
      "grad_norm": 0.3094046115875244,
      "learning_rate": 7.460742578191016e-06,
      "loss": 0.597,
      "step": 1167
    },
    {
      "epoch": 0.8815926030757618,
      "grad_norm": 0.2103927731513977,
      "learning_rate": 7.3683581373699795e-06,
      "loss": 0.6711,
      "step": 1168
    },
    {
      "epoch": 0.8823473912633267,
      "grad_norm": 0.6250953078269958,
      "learning_rate": 7.276527368643793e-06,
      "loss": 0.6822,
      "step": 1169
    },
    {
      "epoch": 0.8831021794508916,
      "grad_norm": 0.2805517613887787,
      "learning_rate": 7.185250820897627e-06,
      "loss": 0.6162,
      "step": 1170
    },
    {
      "epoch": 0.8838569676384564,
      "grad_norm": 0.40660279989242554,
      "learning_rate": 7.094529039704012e-06,
      "loss": 0.7829,
      "step": 1171
    },
    {
      "epoch": 0.8846117558260214,
      "grad_norm": 0.25449874997138977,
      "learning_rate": 7.0043625673195115e-06,
      "loss": 0.5495,
      "step": 1172
    },
    {
      "epoch": 0.8853665440135862,
      "grad_norm": 0.30480724573135376,
      "learning_rate": 6.914751942681585e-06,
      "loss": 0.623,
      "step": 1173
    },
    {
      "epoch": 0.8861213322011511,
      "grad_norm": 0.3594135642051697,
      "learning_rate": 6.825697701405264e-06,
      "loss": 0.6999,
      "step": 1174
    },
    {
      "epoch": 0.8868761203887159,
      "grad_norm": 0.31923824548721313,
      "learning_rate": 6.737200375780073e-06,
      "loss": 0.7157,
      "step": 1175
    },
    {
      "epoch": 0.8876309085762808,
      "grad_norm": 0.2830456793308258,
      "learning_rate": 6.649260494766673e-06,
      "loss": 0.6056,
      "step": 1176
    },
    {
      "epoch": 0.8883856967638456,
      "grad_norm": 0.35883384943008423,
      "learning_rate": 6.561878583993897e-06,
      "loss": 0.7595,
      "step": 1177
    },
    {
      "epoch": 0.8891404849514105,
      "grad_norm": 0.3043292164802551,
      "learning_rate": 6.47505516575545e-06,
      "loss": 0.7345,
      "step": 1178
    },
    {
      "epoch": 0.8898952731389754,
      "grad_norm": 0.44164255261421204,
      "learning_rate": 6.388790759006902e-06,
      "loss": 0.7195,
      "step": 1179
    },
    {
      "epoch": 0.8906500613265402,
      "grad_norm": 0.4390563666820526,
      "learning_rate": 6.303085879362469e-06,
      "loss": 0.6756,
      "step": 1180
    },
    {
      "epoch": 0.8914048495141051,
      "grad_norm": 0.27045488357543945,
      "learning_rate": 6.217941039092068e-06,
      "loss": 0.6778,
      "step": 1181
    },
    {
      "epoch": 0.89215963770167,
      "grad_norm": 0.2651049494743347,
      "learning_rate": 6.133356747118124e-06,
      "loss": 0.7732,
      "step": 1182
    },
    {
      "epoch": 0.8929144258892349,
      "grad_norm": 0.22594691812992096,
      "learning_rate": 6.049333509012612e-06,
      "loss": 0.6012,
      "step": 1183
    },
    {
      "epoch": 0.8936692140767997,
      "grad_norm": 0.25640159845352173,
      "learning_rate": 5.965871826994041e-06,
      "loss": 0.7128,
      "step": 1184
    },
    {
      "epoch": 0.8944240022643646,
      "grad_norm": 0.42548584938049316,
      "learning_rate": 5.8829721999243525e-06,
      "loss": 0.7253,
      "step": 1185
    },
    {
      "epoch": 0.8951787904519294,
      "grad_norm": 0.29081884026527405,
      "learning_rate": 5.800635123306053e-06,
      "loss": 0.6068,
      "step": 1186
    },
    {
      "epoch": 0.8959335786394943,
      "grad_norm": 0.4406934380531311,
      "learning_rate": 5.718861089279248e-06,
      "loss": 0.6826,
      "step": 1187
    },
    {
      "epoch": 0.8966883668270591,
      "grad_norm": 0.3140653669834137,
      "learning_rate": 5.637650586618548e-06,
      "loss": 0.7092,
      "step": 1188
    },
    {
      "epoch": 0.897443155014624,
      "grad_norm": 0.31925928592681885,
      "learning_rate": 5.557004100730356e-06,
      "loss": 0.6808,
      "step": 1189
    },
    {
      "epoch": 0.8981979432021889,
      "grad_norm": 0.3106412887573242,
      "learning_rate": 5.476922113649862e-06,
      "loss": 0.7284,
      "step": 1190
    },
    {
      "epoch": 0.8989527313897537,
      "grad_norm": 0.1861330270767212,
      "learning_rate": 5.3974051040380895e-06,
      "loss": 0.6038,
      "step": 1191
    },
    {
      "epoch": 0.8997075195773186,
      "grad_norm": 0.3841107487678528,
      "learning_rate": 5.318453547179214e-06,
      "loss": 0.6702,
      "step": 1192
    },
    {
      "epoch": 0.9004623077648835,
      "grad_norm": 0.19354572892189026,
      "learning_rate": 5.240067914977553e-06,
      "loss": 0.6223,
      "step": 1193
    },
    {
      "epoch": 0.9012170959524484,
      "grad_norm": 0.4096985161304474,
      "learning_rate": 5.162248675954851e-06,
      "loss": 0.6543,
      "step": 1194
    },
    {
      "epoch": 0.9019718841400132,
      "grad_norm": 0.4800698161125183,
      "learning_rate": 5.084996295247402e-06,
      "loss": 0.607,
      "step": 1195
    },
    {
      "epoch": 0.9027266723275781,
      "grad_norm": 0.30872127413749695,
      "learning_rate": 5.00831123460338e-06,
      "loss": 0.6916,
      "step": 1196
    },
    {
      "epoch": 0.9034814605151429,
      "grad_norm": 0.31906503438949585,
      "learning_rate": 4.9321939523799155e-06,
      "loss": 0.7592,
      "step": 1197
    },
    {
      "epoch": 0.9042362487027078,
      "grad_norm": 0.32402703166007996,
      "learning_rate": 4.856644903540553e-06,
      "loss": 0.7486,
      "step": 1198
    },
    {
      "epoch": 0.9049910368902727,
      "grad_norm": 0.3295327126979828,
      "learning_rate": 4.781664539652319e-06,
      "loss": 0.5449,
      "step": 1199
    },
    {
      "epoch": 0.9057458250778375,
      "grad_norm": 0.35620346665382385,
      "learning_rate": 4.707253308883208e-06,
      "loss": 0.7119,
      "step": 1200
    },
    {
      "epoch": 0.9057458250778375,
      "eval_loss": 0.6664915084838867,
      "eval_runtime": 3186.4787,
      "eval_samples_per_second": 2.957,
      "eval_steps_per_second": 0.739,
      "step": 1200
    },
    {
      "epoch": 0.9065006132654024,
      "grad_norm": 0.25337091088294983,
      "learning_rate": 4.633411655999431e-06,
      "loss": 0.5851,
      "step": 1201
    },
    {
      "epoch": 0.9072554014529672,
      "grad_norm": 0.23264528810977936,
      "learning_rate": 4.560140022362691e-06,
      "loss": 0.6261,
      "step": 1202
    },
    {
      "epoch": 0.9080101896405322,
      "grad_norm": 0.4486328363418579,
      "learning_rate": 4.487438845927683e-06,
      "loss": 0.7084,
      "step": 1203
    },
    {
      "epoch": 0.908764977828097,
      "grad_norm": 0.3234231770038605,
      "learning_rate": 4.415308561239317e-06,
      "loss": 0.7062,
      "step": 1204
    },
    {
      "epoch": 0.9095197660156619,
      "grad_norm": 0.19419077038764954,
      "learning_rate": 4.3437495994302805e-06,
      "loss": 0.6335,
      "step": 1205
    },
    {
      "epoch": 0.9102745542032267,
      "grad_norm": 0.339739590883255,
      "learning_rate": 4.272762388218332e-06,
      "loss": 0.6896,
      "step": 1206
    },
    {
      "epoch": 0.9110293423907916,
      "grad_norm": 0.32657676935195923,
      "learning_rate": 4.202347351903857e-06,
      "loss": 0.8317,
      "step": 1207
    },
    {
      "epoch": 0.9117841305783565,
      "grad_norm": 0.4150348901748657,
      "learning_rate": 4.132504911367197e-06,
      "loss": 0.7065,
      "step": 1208
    },
    {
      "epoch": 0.9125389187659213,
      "grad_norm": 0.27996453642845154,
      "learning_rate": 4.063235484066274e-06,
      "loss": 0.6095,
      "step": 1209
    },
    {
      "epoch": 0.9132937069534862,
      "grad_norm": 0.30567991733551025,
      "learning_rate": 3.994539484033988e-06,
      "loss": 0.6198,
      "step": 1210
    },
    {
      "epoch": 0.914048495141051,
      "grad_norm": 0.2505205571651459,
      "learning_rate": 3.926417321875808e-06,
      "loss": 0.6769,
      "step": 1211
    },
    {
      "epoch": 0.9148032833286159,
      "grad_norm": 0.28362420201301575,
      "learning_rate": 3.858869404767296e-06,
      "loss": 0.7898,
      "step": 1212
    },
    {
      "epoch": 0.9155580715161807,
      "grad_norm": 0.2840142250061035,
      "learning_rate": 3.7918961364516558e-06,
      "loss": 0.6677,
      "step": 1213
    },
    {
      "epoch": 0.9163128597037457,
      "grad_norm": 0.37104517221450806,
      "learning_rate": 3.7254979172373637e-06,
      "loss": 0.7399,
      "step": 1214
    },
    {
      "epoch": 0.9170676478913105,
      "grad_norm": 0.26200050115585327,
      "learning_rate": 3.6596751439957e-06,
      "loss": 0.5544,
      "step": 1215
    },
    {
      "epoch": 0.9178224360788754,
      "grad_norm": 0.3147200644016266,
      "learning_rate": 3.594428210158496e-06,
      "loss": 0.6493,
      "step": 1216
    },
    {
      "epoch": 0.9185772242664403,
      "grad_norm": 0.4882217347621918,
      "learning_rate": 3.529757505715625e-06,
      "loss": 0.6901,
      "step": 1217
    },
    {
      "epoch": 0.9193320124540051,
      "grad_norm": 0.3254289925098419,
      "learning_rate": 3.46566341721285e-06,
      "loss": 0.7083,
      "step": 1218
    },
    {
      "epoch": 0.92008680064157,
      "grad_norm": 0.17309632897377014,
      "learning_rate": 3.4021463277493337e-06,
      "loss": 0.6657,
      "step": 1219
    },
    {
      "epoch": 0.9208415888291348,
      "grad_norm": 0.2898695468902588,
      "learning_rate": 3.3392066169755096e-06,
      "loss": 0.7001,
      "step": 1220
    },
    {
      "epoch": 0.9215963770166997,
      "grad_norm": 0.40120911598205566,
      "learning_rate": 3.276844661090683e-06,
      "loss": 0.6782,
      "step": 1221
    },
    {
      "epoch": 0.9223511652042645,
      "grad_norm": 0.15463151037693024,
      "learning_rate": 3.2150608328408773e-06,
      "loss": 0.5922,
      "step": 1222
    },
    {
      "epoch": 0.9231059533918294,
      "grad_norm": 0.2284420132637024,
      "learning_rate": 3.153855501516545e-06,
      "loss": 0.7532,
      "step": 1223
    },
    {
      "epoch": 0.9238607415793942,
      "grad_norm": 0.44857242703437805,
      "learning_rate": 3.0932290329503953e-06,
      "loss": 0.8108,
      "step": 1224
    },
    {
      "epoch": 0.9246155297669592,
      "grad_norm": 0.3811584413051605,
      "learning_rate": 3.0331817895151827e-06,
      "loss": 0.6776,
      "step": 1225
    },
    {
      "epoch": 0.925370317954524,
      "grad_norm": 0.21499626338481903,
      "learning_rate": 2.973714130121563e-06,
      "loss": 0.5965,
      "step": 1226
    },
    {
      "epoch": 0.9261251061420889,
      "grad_norm": 0.2696818709373474,
      "learning_rate": 2.914826410215932e-06,
      "loss": 0.6757,
      "step": 1227
    },
    {
      "epoch": 0.9268798943296538,
      "grad_norm": 0.3862650692462921,
      "learning_rate": 2.8565189817783333e-06,
      "loss": 0.6943,
      "step": 1228
    },
    {
      "epoch": 0.9276346825172186,
      "grad_norm": 0.3611625134944916,
      "learning_rate": 2.7987921933202654e-06,
      "loss": 0.5264,
      "step": 1229
    },
    {
      "epoch": 0.9283894707047835,
      "grad_norm": 0.28084781765937805,
      "learning_rate": 2.7416463898827347e-06,
      "loss": 0.672,
      "step": 1230
    },
    {
      "epoch": 0.9291442588923483,
      "grad_norm": 0.2262965589761734,
      "learning_rate": 2.685081913034082e-06,
      "loss": 0.7525,
      "step": 1231
    },
    {
      "epoch": 0.9298990470799132,
      "grad_norm": 0.34654903411865234,
      "learning_rate": 2.6290991008679713e-06,
      "loss": 0.5775,
      "step": 1232
    },
    {
      "epoch": 0.930653835267478,
      "grad_norm": 0.3025602102279663,
      "learning_rate": 2.573698288001403e-06,
      "loss": 0.5689,
      "step": 1233
    },
    {
      "epoch": 0.9314086234550429,
      "grad_norm": 0.3765603303909302,
      "learning_rate": 2.5188798055726604e-06,
      "loss": 0.7201,
      "step": 1234
    },
    {
      "epoch": 0.9321634116426077,
      "grad_norm": 0.2423149198293686,
      "learning_rate": 2.4646439812393896e-06,
      "loss": 0.7107,
      "step": 1235
    },
    {
      "epoch": 0.9329181998301727,
      "grad_norm": 0.31502580642700195,
      "learning_rate": 2.410991139176566e-06,
      "loss": 0.7116,
      "step": 1236
    },
    {
      "epoch": 0.9336729880177376,
      "grad_norm": 0.2508748769760132,
      "learning_rate": 2.3579216000746417e-06,
      "loss": 0.6963,
      "step": 1237
    },
    {
      "epoch": 0.9344277762053024,
      "grad_norm": 0.369667649269104,
      "learning_rate": 2.305435681137569e-06,
      "loss": 0.7333,
      "step": 1238
    },
    {
      "epoch": 0.9351825643928673,
      "grad_norm": 0.2863911986351013,
      "learning_rate": 2.2535336960809115e-06,
      "loss": 0.7263,
      "step": 1239
    },
    {
      "epoch": 0.9359373525804321,
      "grad_norm": 0.30449995398521423,
      "learning_rate": 2.2022159551299935e-06,
      "loss": 0.4842,
      "step": 1240
    },
    {
      "epoch": 0.936692140767997,
      "grad_norm": 0.30526790022850037,
      "learning_rate": 2.1514827650180423e-06,
      "loss": 0.5208,
      "step": 1241
    },
    {
      "epoch": 0.9374469289555618,
      "grad_norm": 0.33760398626327515,
      "learning_rate": 2.1013344289843027e-06,
      "loss": 0.7889,
      "step": 1242
    },
    {
      "epoch": 0.9382017171431267,
      "grad_norm": 0.28149843215942383,
      "learning_rate": 2.051771246772305e-06,
      "loss": 0.6638,
      "step": 1243
    },
    {
      "epoch": 0.9389565053306915,
      "grad_norm": 0.42221418023109436,
      "learning_rate": 2.002793514628043e-06,
      "loss": 0.719,
      "step": 1244
    },
    {
      "epoch": 0.9397112935182564,
      "grad_norm": 0.2669680416584015,
      "learning_rate": 1.954401525298144e-06,
      "loss": 0.7041,
      "step": 1245
    },
    {
      "epoch": 0.9404660817058214,
      "grad_norm": 0.4400778114795685,
      "learning_rate": 1.9065955680282243e-06,
      "loss": 0.6481,
      "step": 1246
    },
    {
      "epoch": 0.9412208698933862,
      "grad_norm": 0.27773797512054443,
      "learning_rate": 1.859375928561058e-06,
      "loss": 0.5495,
      "step": 1247
    },
    {
      "epoch": 0.9419756580809511,
      "grad_norm": 0.31160178780555725,
      "learning_rate": 1.812742889134955e-06,
      "loss": 0.7343,
      "step": 1248
    },
    {
      "epoch": 0.9427304462685159,
      "grad_norm": 0.40180081129074097,
      "learning_rate": 1.7666967284820201e-06,
      "loss": 0.8481,
      "step": 1249
    },
    {
      "epoch": 0.9434852344560808,
      "grad_norm": 0.2771635353565216,
      "learning_rate": 1.7212377218265185e-06,
      "loss": 0.6973,
      "step": 1250
    },
    {
      "epoch": 0.9442400226436456,
      "grad_norm": 0.35563862323760986,
      "learning_rate": 1.6763661408831677e-06,
      "loss": 0.7669,
      "step": 1251
    },
    {
      "epoch": 0.9449948108312105,
      "grad_norm": 0.27651524543762207,
      "learning_rate": 1.6320822538556158e-06,
      "loss": 0.631,
      "step": 1252
    },
    {
      "epoch": 0.9457495990187753,
      "grad_norm": 0.30178752541542053,
      "learning_rate": 1.5883863254347652e-06,
      "loss": 0.6908,
      "step": 1253
    },
    {
      "epoch": 0.9465043872063402,
      "grad_norm": 0.7116816639900208,
      "learning_rate": 1.5452786167971855e-06,
      "loss": 0.6091,
      "step": 1254
    },
    {
      "epoch": 0.9472591753939051,
      "grad_norm": 0.3324531018733978,
      "learning_rate": 1.5027593856036137e-06,
      "loss": 0.6066,
      "step": 1255
    },
    {
      "epoch": 0.9480139635814699,
      "grad_norm": 0.30517956614494324,
      "learning_rate": 1.460828885997334e-06,
      "loss": 0.6987,
      "step": 1256
    },
    {
      "epoch": 0.9487687517690349,
      "grad_norm": 0.28668174147605896,
      "learning_rate": 1.4194873686027565e-06,
      "loss": 0.7046,
      "step": 1257
    },
    {
      "epoch": 0.9495235399565997,
      "grad_norm": 0.3100184500217438,
      "learning_rate": 1.3787350805237965e-06,
      "loss": 0.6967,
      "step": 1258
    },
    {
      "epoch": 0.9502783281441646,
      "grad_norm": 0.38347792625427246,
      "learning_rate": 1.3385722653425302e-06,
      "loss": 0.6678,
      "step": 1259
    },
    {
      "epoch": 0.9510331163317294,
      "grad_norm": 0.2357538342475891,
      "learning_rate": 1.29899916311762e-06,
      "loss": 0.7163,
      "step": 1260
    },
    {
      "epoch": 0.9517879045192943,
      "grad_norm": 0.4566214680671692,
      "learning_rate": 1.2600160103829583e-06,
      "loss": 0.6019,
      "step": 1261
    },
    {
      "epoch": 0.9525426927068591,
      "grad_norm": 0.22979892790317535,
      "learning_rate": 1.2216230401462136e-06,
      "loss": 0.6393,
      "step": 1262
    },
    {
      "epoch": 0.953297480894424,
      "grad_norm": 0.26405149698257446,
      "learning_rate": 1.1838204818874875e-06,
      "loss": 0.7019,
      "step": 1263
    },
    {
      "epoch": 0.9540522690819888,
      "grad_norm": 0.442302405834198,
      "learning_rate": 1.1466085615578713e-06,
      "loss": 0.688,
      "step": 1264
    },
    {
      "epoch": 0.9548070572695537,
      "grad_norm": 0.2578158974647522,
      "learning_rate": 1.1099875015781358e-06,
      "loss": 0.5867,
      "step": 1265
    },
    {
      "epoch": 0.9555618454571186,
      "grad_norm": 0.3267114460468292,
      "learning_rate": 1.0739575208373987e-06,
      "loss": 0.7252,
      "step": 1266
    },
    {
      "epoch": 0.9563166336446834,
      "grad_norm": 0.20136021077632904,
      "learning_rate": 1.0385188346918485e-06,
      "loss": 0.6654,
      "step": 1267
    },
    {
      "epoch": 0.9570714218322484,
      "grad_norm": 0.4038151502609253,
      "learning_rate": 1.0036716549633785e-06,
      "loss": 0.5639,
      "step": 1268
    },
    {
      "epoch": 0.9578262100198132,
      "grad_norm": 0.3023516833782196,
      "learning_rate": 9.694161899383992e-07,
      "loss": 0.6545,
      "step": 1269
    },
    {
      "epoch": 0.9585809982073781,
      "grad_norm": 0.18134094774723053,
      "learning_rate": 9.357526443665276e-07,
      "loss": 0.5359,
      "step": 1270
    },
    {
      "epoch": 0.9593357863949429,
      "grad_norm": 0.286056250333786,
      "learning_rate": 9.026812194594448e-07,
      "loss": 0.5935,
      "step": 1271
    },
    {
      "epoch": 0.9600905745825078,
      "grad_norm": 0.3880782127380371,
      "learning_rate": 8.702021128895954e-07,
      "loss": 0.7077,
      "step": 1272
    },
    {
      "epoch": 0.9608453627700726,
      "grad_norm": 0.21869447827339172,
      "learning_rate": 8.383155187890901e-07,
      "loss": 0.5886,
      "step": 1273
    },
    {
      "epoch": 0.9616001509576375,
      "grad_norm": 0.25043755769729614,
      "learning_rate": 8.070216277484833e-07,
      "loss": 0.7006,
      "step": 1274
    },
    {
      "epoch": 0.9623549391452024,
      "grad_norm": 0.28366613388061523,
      "learning_rate": 7.763206268156964e-07,
      "loss": 0.5309,
      "step": 1275
    },
    {
      "epoch": 0.9631097273327672,
      "grad_norm": 0.27038007974624634,
      "learning_rate": 7.462126994948305e-07,
      "loss": 0.574,
      "step": 1276
    },
    {
      "epoch": 0.9638645155203321,
      "grad_norm": 0.2823457419872284,
      "learning_rate": 7.166980257451106e-07,
      "loss": 0.64,
      "step": 1277
    },
    {
      "epoch": 0.964619303707897,
      "grad_norm": 0.341775119304657,
      "learning_rate": 6.877767819798431e-07,
      "loss": 0.6202,
      "step": 1278
    },
    {
      "epoch": 0.9653740918954619,
      "grad_norm": 0.24776554107666016,
      "learning_rate": 6.594491410652492e-07,
      "loss": 0.7037,
      "step": 1279
    },
    {
      "epoch": 0.9661288800830267,
      "grad_norm": 0.34573617577552795,
      "learning_rate": 6.317152723195774e-07,
      "loss": 0.8089,
      "step": 1280
    },
    {
      "epoch": 0.9668836682705916,
      "grad_norm": 0.30832386016845703,
      "learning_rate": 6.045753415119592e-07,
      "loss": 0.5286,
      "step": 1281
    },
    {
      "epoch": 0.9676384564581564,
      "grad_norm": 0.3545877933502197,
      "learning_rate": 5.78029510861533e-07,
      "loss": 0.8156,
      "step": 1282
    },
    {
      "epoch": 0.9683932446457213,
      "grad_norm": 0.37466609477996826,
      "learning_rate": 5.520779390363551e-07,
      "loss": 0.7411,
      "step": 1283
    },
    {
      "epoch": 0.9691480328332862,
      "grad_norm": 0.3224070966243744,
      "learning_rate": 5.267207811525676e-07,
      "loss": 0.8011,
      "step": 1284
    },
    {
      "epoch": 0.969902821020851,
      "grad_norm": 0.3103907108306885,
      "learning_rate": 5.019581887733993e-07,
      "loss": 0.626,
      "step": 1285
    },
    {
      "epoch": 0.9706576092084159,
      "grad_norm": 0.3680419623851776,
      "learning_rate": 4.777903099082659e-07,
      "loss": 0.6062,
      "step": 1286
    },
    {
      "epoch": 0.9714123973959807,
      "grad_norm": 0.7337579727172852,
      "learning_rate": 4.542172890119267e-07,
      "loss": 0.7063,
      "step": 1287
    },
    {
      "epoch": 0.9721671855835456,
      "grad_norm": 0.2809107303619385,
      "learning_rate": 4.312392669835741e-07,
      "loss": 0.6499,
      "step": 1288
    },
    {
      "epoch": 0.9729219737711104,
      "grad_norm": 0.2075280100107193,
      "learning_rate": 4.0885638116601176e-07,
      "loss": 0.8105,
      "step": 1289
    },
    {
      "epoch": 0.9736767619586754,
      "grad_norm": 0.3280186355113983,
      "learning_rate": 3.870687653448224e-07,
      "loss": 0.639,
      "step": 1290
    },
    {
      "epoch": 0.9744315501462402,
      "grad_norm": 0.30522623658180237,
      "learning_rate": 3.658765497476124e-07,
      "loss": 0.6925,
      "step": 1291
    },
    {
      "epoch": 0.9751863383338051,
      "grad_norm": 0.28368693590164185,
      "learning_rate": 3.4527986104315734e-07,
      "loss": 0.5429,
      "step": 1292
    },
    {
      "epoch": 0.97594112652137,
      "grad_norm": 0.37401750683784485,
      "learning_rate": 3.252788223407244e-07,
      "loss": 0.6808,
      "step": 1293
    },
    {
      "epoch": 0.9766959147089348,
      "grad_norm": 0.9590082168579102,
      "learning_rate": 3.0587355318926204e-07,
      "loss": 0.6401,
      "step": 1294
    },
    {
      "epoch": 0.9774507028964997,
      "grad_norm": 0.3866276443004608,
      "learning_rate": 2.8706416957674506e-07,
      "loss": 0.671,
      "step": 1295
    },
    {
      "epoch": 0.9782054910840645,
      "grad_norm": 0.335938036441803,
      "learning_rate": 2.6885078392945297e-07,
      "loss": 0.6941,
      "step": 1296
    },
    {
      "epoch": 0.9789602792716294,
      "grad_norm": 0.7816243767738342,
      "learning_rate": 2.512335051112924e-07,
      "loss": 0.4992,
      "step": 1297
    },
    {
      "epoch": 0.9797150674591942,
      "grad_norm": 0.6208944916725159,
      "learning_rate": 2.3421243842319808e-07,
      "loss": 0.6027,
      "step": 1298
    },
    {
      "epoch": 0.9804698556467591,
      "grad_norm": 0.39317765831947327,
      "learning_rate": 2.177876856023997e-07,
      "loss": 0.7404,
      "step": 1299
    },
    {
      "epoch": 0.981224643834324,
      "grad_norm": 0.3370835483074188,
      "learning_rate": 2.0195934482192257e-07,
      "loss": 0.6503,
      "step": 1300
    },
    {
      "epoch": 0.9819794320218889,
      "grad_norm": 0.4402979612350464,
      "learning_rate": 1.8672751068995466e-07,
      "loss": 0.561,
      "step": 1301
    },
    {
      "epoch": 0.9827342202094537,
      "grad_norm": 0.4967517554759979,
      "learning_rate": 1.72092274249247e-07,
      "loss": 0.671,
      "step": 1302
    },
    {
      "epoch": 0.9834890083970186,
      "grad_norm": 0.2904543876647949,
      "learning_rate": 1.5805372297662546e-07,
      "loss": 0.6059,
      "step": 1303
    },
    {
      "epoch": 0.9842437965845835,
      "grad_norm": 0.26871904730796814,
      "learning_rate": 1.4461194078246865e-07,
      "loss": 0.6841,
      "step": 1304
    },
    {
      "epoch": 0.9849985847721483,
      "grad_norm": 0.2599211633205414,
      "learning_rate": 1.3176700801014186e-07,
      "loss": 0.6057,
      "step": 1305
    },
    {
      "epoch": 0.9857533729597132,
      "grad_norm": 0.2917141318321228,
      "learning_rate": 1.1951900143558625e-07,
      "loss": 0.5732,
      "step": 1306
    },
    {
      "epoch": 0.986508161147278,
      "grad_norm": 0.19617444276809692,
      "learning_rate": 1.0786799426683036e-07,
      "loss": 0.6048,
      "step": 1307
    },
    {
      "epoch": 0.9872629493348429,
      "grad_norm": 0.24769584834575653,
      "learning_rate": 9.6814056143546e-08,
      "loss": 0.6181,
      "step": 1308
    },
    {
      "epoch": 0.9880177375224077,
      "grad_norm": 0.22971074283123016,
      "learning_rate": 8.635725313663745e-08,
      "loss": 0.603,
      "step": 1309
    },
    {
      "epoch": 0.9887725257099726,
      "grad_norm": 0.22540183365345,
      "learning_rate": 7.649764774786405e-08,
      "loss": 0.6171,
      "step": 1310
    },
    {
      "epoch": 0.9895273138975375,
      "grad_norm": 0.3515390157699585,
      "learning_rate": 6.723529890946268e-08,
      "loss": 0.685,
      "step": 1311
    },
    {
      "epoch": 0.9902821020851024,
      "grad_norm": 0.4023972153663635,
      "learning_rate": 5.8570261983748044e-08,
      "loss": 0.676,
      "step": 1312
    },
    {
      "epoch": 0.9910368902726673,
      "grad_norm": 0.21682335436344147,
      "learning_rate": 5.05025887628352e-08,
      "loss": 0.6903,
      "step": 1313
    },
    {
      "epoch": 0.9917916784602321,
      "grad_norm": 0.21393842995166779,
      "learning_rate": 4.303232746835084e-08,
      "loss": 0.6702,
      "step": 1314
    },
    {
      "epoch": 0.992546466647797,
      "grad_norm": 0.2596125304698944,
      "learning_rate": 3.6159522751044726e-08,
      "loss": 0.6402,
      "step": 1315
    },
    {
      "epoch": 0.9933012548353618,
      "grad_norm": 0.5330621600151062,
      "learning_rate": 2.9884215690634266e-08,
      "loss": 0.6491,
      "step": 1316
    },
    {
      "epoch": 0.9940560430229267,
      "grad_norm": 0.3470461964607239,
      "learning_rate": 2.420644379549364e-08,
      "loss": 0.6172,
      "step": 1317
    },
    {
      "epoch": 0.9948108312104915,
      "grad_norm": 0.3586242198944092,
      "learning_rate": 1.912624100244287e-08,
      "loss": 0.6621,
      "step": 1318
    },
    {
      "epoch": 0.9955656193980564,
      "grad_norm": 0.26861831545829773,
      "learning_rate": 1.4643637676559075e-08,
      "loss": 0.6027,
      "step": 1319
    },
    {
      "epoch": 0.9963204075856212,
      "grad_norm": 0.31134340167045593,
      "learning_rate": 1.0758660610976634e-08,
      "loss": 0.649,
      "step": 1320
    },
    {
      "epoch": 0.9970751957731862,
      "grad_norm": 0.18917763233184814,
      "learning_rate": 7.471333026742855e-09,
      "loss": 0.7046,
      "step": 1321
    },
    {
      "epoch": 0.9978299839607511,
      "grad_norm": 0.26809489727020264,
      "learning_rate": 4.781674572673644e-09,
      "loss": 0.6625,
      "step": 1322
    },
    {
      "epoch": 0.9985847721483159,
      "grad_norm": 0.443121999502182,
      "learning_rate": 2.689701325209182e-09,
      "loss": 0.7564,
      "step": 1323
    },
    {
      "epoch": 0.9993395603358808,
      "grad_norm": 0.27882784605026245,
      "learning_rate": 1.1954257883806109e-09,
      "loss": 0.5474,
      "step": 1324
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.24614186584949493,
      "learning_rate": 2.9885689367681326e-10,
      "loss": 0.7371,
      "step": 1325
    }
  ],
  "logging_steps": 1.0,
  "max_steps": 1325,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 273405276635136.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
